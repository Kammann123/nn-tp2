{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo List!\n",
    "* ¿Qué suposición hace el MAE? ¿Qué está minimizando? ¿Por qué conviene usarlo como función de costo en este caso?\n",
    "* Usar otros optimizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuentes\n",
    "\n",
    "### Link: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "En esta fuente se puede encontrar una breve explicación del MAE y del MSE, una comparación entre ambos respecto de su comportamiento en entrenamiento frente a conjuntos de datos con y sin outliers, y luego una comparación de su comportamiento durante entrenamiento a razón de cómo son sus gradientes, lo cual provoca en el caso del MAE que la convergencia sea más lenta y sea necesario utilizar un **learning rate dinámico**. Explica que, si nos importa que la presencia de outliers tenga un impacto directo sobre el modelo, deberíamos utilizar MSE, mientras que si deseamos que no afecte demasiado podemos emplear MAE.\n",
    "\n",
    "### Link: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "En esta fuente se puede encontrar una explicación de los tres métodos para learning rate dinámico utilizados, el **time-based decay**, el **step decay** y el **exponential decay**, empleando para algunos de ellos la clase de Keras llamada Learning Rate Scheduler, que permite modificar a gusto del usuario el valor del learning rate a través del proceso.\n",
    "\n",
    "### Link: https://stackoverflow.com/questions/46308374/what-is-validation-data-used-for-in-a-keras-sequential-model\n",
    "Esta disución de StackOverflow es interesante sobre la separación de los datasets en entrenamiento, validación y evaluación del modelo, la use para verificar algunas cuestiones sobre cómo usaba la información de validación Keras, entre otras cosas.\n",
    "\n",
    "### Link: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "Explicación sobre el uso de **early stopping**, donde básicamente buscamos parar el entrenamiento aunque no se hayan terminado de correr todos los epochs predefinidos, porque se detecta que no hay mejoría en los resultados obtenidos, para ello se emplea la métrica evaluada sobre el conjunto de validación.\n",
    "\n",
    "### Link: https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/\n",
    "Explicación sobre el uso de **features polinomiales**, que básicamente consiste en agregar nuevas variables de entrada al modelo a partir de potencias obtenidas entre las variables de entrada originales. De esta forma, el espacio que conforman las variables es de mayor dimensión y por ello la solución es más flexible, aunque hay que tener cuidado de que no se ajuste demasiado provocando **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the database from the .csv file into a pandas dataframe\n",
    "df = pd.read_csv('../../databases/insurance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Codificación de variables no numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the sex variable or feature and create a new column in the dataframe \n",
    "# with the encoded version of the gender\n",
    "sex_encoder = preprocessing.LabelEncoder()\n",
    "sex_encoder.fit(df['sex'])\n",
    "df['sex-encoded'] = sex_encoder.transform(df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the smoker variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the smoker\n",
    "smoker_encoder = preprocessing.LabelEncoder()\n",
    "smoker_encoder.fit(df['smoker'])\n",
    "df['smoker-encoded'] = smoker_encoder.transform(df['smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one hot encoder and fit the available types of regions in the dataset\n",
    "region_encoder = preprocessing.OneHotEncoder()\n",
    "region_encoder.fit(df['region'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Transform all entries into the one hot encoded representation\n",
    "encoded_regions = region_encoder.transform(df['region'].to_numpy().reshape(-1, 1)).toarray()\n",
    "\n",
    "# Add each new encoded variable or feature to the dataset\n",
    "for i, category in enumerate(region_encoder.categories_[0]):\n",
    "    df[f'{category}-encoded'] = encoded_regions.transpose()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Filtrado de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering or removing of non desired variables\n",
    "df_x = df[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded', 'northwest-encoded', 'northeast-encoded', 'southwest-encoded', 'southeast-encoded']]\n",
    "df_y = df['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Separación de los conjuntos\n",
    "Es importante notar que, se realiza la separación del conjunto de datos original en **train**, **valid** y **test**, por fuera del framework de Keras para garantizar un adecuado tratamiento de los conjuntos acorde a la metodología empleada. En otras palabras, de esta forma nos aseguramos que cualquier preprocesamiento o normalización sobre validación (valid) y evaluación (test) se realiza a partir de la información obtenida en entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regresión Lineal\n",
    "\n",
    "\n",
    "#### Comentarios\n",
    "1. Al principio, sucedió que el MAE era muy lento para convergencia, lo cual tiene sentido por el tipo de función de costo que representa. Particularmente, comparado con MSE, es mucho más lentro. Empecé probando modificar de forma estática y a mano el **learning rate**.\n",
    "2. Luego, con un learning rate cada vez mayor, pude observar que el entrenamiento era más rápido, pero sucedían dos cuestiones. En primer lugar, que se producía una especie oscilación en torno a un valor que asumo que es el mínimo al cual se acerca el entrenamiento, con lo cual sería necesario disminuir cerca de ahí el valor del learning rate. Por otro lado, este mínimo no era el mismo mínimo que obtuve con el MSE, debe ser un plateau, un mínimo local pero no el absoluto. Me propuse usar **learning rate dinámico** y **comenzar de diferentes puntos**.\n",
    "3. Cuando probe utilizar MSE, si no normalizaba con z-score todas las variables, rápidamente divergía la función de costo y se rompía el entrenamiento. Por otro lado, la misma normalización afectaba mucho al entrenamiento del MAE. *¿Por qué?* Lo pude corregir un poco al aumentar el learning rate por un factor, lo cual debe tener sentido si se considera que ahora las variables estando normalizadas tienen una menor magnitud lo cual puede producir que los pasos sean menores que antes, y por eso se ralentizó.\n",
    "4. Interesante, llegué a esta discusión https://datascience.stackexchange.com/questions/9020/do-i-have-to-standardize-my-new-polynomial-features a raiz de una pregunta bastante sencilla, **¿por qué no está mejorando la métrica con mayor orden de polynomial features?**. Resulta ser que normalizando las variables y luego aplicando polynomial features, obtengo nuevas variables que siguen encontrándose en el intervalo [0,1] pero que su orden de magnitud es mucho menor. *Conclusión, siempre normalizar las variables que entran al modelo, y por ende si aplicas polynomial features tenés que normalizar luego de crear las nuevas variables.*\n",
    "5. Con la corrección mencionada anteriormente con respecto a la normalización, mejoró el resultado de ordenes grandes de polinomios.\n",
    "6. Me llama la atención que por lo general los resultados de validación son mejores que en entrenamiento, y además, esta diferencia se achica más a medida que aumenta el orden de los polinomios. Me hace pensar que por alguna razón estoy en underfitting, o estimando incorrectamente las métricas (por ejemplo por tamaño del dataset). Este artículo menciona algo que puede ser útil https://keras.io/getting_started/faq/#why-is-my-training-loss-much-higher-than-my-testing-loss, una posibilidad sería que la validación sobre un epoch siempre tienda a ser mejor que el promedio del train en los batch, porque fue entrenándose mejor. Aunque no me convence después de muchos epochs que suceda esto. **¿Debería estar usando k-folding?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import rl_helper\n",
    "importlib.reload(rl_helper);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-144650\n",
      "Model checkpoints at checkpoints/rl/20210525-144650\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3644.799072265625 Valid: 2969.898193359375 Test: 3130.251953125\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          scheduler='time-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          epochs=500,\n",
    "                          batch_size=64\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-144909\n",
      "Model checkpoints at checkpoints/rl/20210525-144909\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 11415.05859375 Valid: 10254.7333984375 Test: 9868.3759765625\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1.0,\n",
    "                          scheduler='time-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          epochs=500,\n",
    "                          batch_size=64\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-145644\n",
      "Model checkpoints at checkpoints/rl/20210525-145644\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3654.8017578125 Valid: 2994.168212890625 Test: 3098.084228515625\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          scheduler='time-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=500,\n",
    "                          batch_size=64\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-145948\n",
      "Model checkpoints at checkpoints/rl/20210525-145948\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3632.73046875 Valid: 2955.25537109375 Test: 3081.125732421875\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          scheduler='step-decay',\n",
    "                          drop_rate=0.5,\n",
    "                          epochs_drop=10,\n",
    "                          epochs=500,\n",
    "                          batch_size=32,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-150229\n",
      "Model checkpoints at checkpoints/rl/20210525-150229\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3635.908203125 Valid: 2956.652099609375 Test: 3075.880615234375\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          scheduler='step-decay',\n",
    "                          drop_rate=0.5,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs_drop=10,\n",
    "                          epochs=500,\n",
    "                          batch_size=32,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-150501\n",
      "Model checkpoints at checkpoints/rl/20210525-150501\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3642.355712890625 Valid: 2952.64306640625 Test: 3103.237060546875\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.07,\n",
    "                          epochs=500,\n",
    "                          batch_size=32,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-150759\n",
      "Model checkpoints at checkpoints/rl/20210525-150759\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3730.654541015625 Valid: 3021.2919921875 Test: 3236.439453125\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=500,\n",
    "                          batch_size=32,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-151058\n",
      "Model checkpoints at checkpoints/rl/20210525-151058\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1)                 55        \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2326.44189453125 Valid: 1785.8372802734375 Test: 1824.603515625\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          degree=2,\n",
    "                          epochs=500,\n",
    "                          batch_size=32,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.09\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-151354\n",
      "Model checkpoints at checkpoints/rl/20210525-151354\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1)                 220       \n",
      "=================================================================\n",
      "Total params: 220\n",
      "Trainable params: 220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2193.197998046875 Valid: 1703.93359375 Test: 1724.305419921875\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          degree=3,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.1,\n",
    "                          epochs=500,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-151709\n",
      "Model checkpoints at checkpoints/rl/20210525-151709\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 1)                 220       \n",
      "=================================================================\n",
      "Total params: 220\n",
      "Trainable params: 220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2204.8984375 Valid: 1693.052001953125 Test: 1734.9503173828125\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          degree=3,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.1,\n",
    "                          epochs=500,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-152033\n",
      "Model checkpoints at checkpoints/rl/20210525-152033\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1)                 220       \n",
      "=================================================================\n",
      "Total params: 220\n",
      "Trainable params: 220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2198.080810546875 Valid: 1717.03271484375 Test: 1758.10498046875\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          degree=3,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.1,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=500,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-152339\n",
      "Model checkpoints at checkpoints/rl/20210525-152339\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 1)                 715       \n",
      "=================================================================\n",
      "Total params: 715\n",
      "Trainable params: 715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2206.0283203125 Valid: 1877.9254150390625 Test: 1834.62109375\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          degree=4,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.1,\n",
    "                          epochs=500,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-152642\n",
      "Model checkpoints at checkpoints/rl/20210525-152642\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 1)                 715       \n",
      "=================================================================\n",
      "Total params: 715\n",
      "Trainable params: 715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2194.46337890625 Valid: 1981.76806640625 Test: 2041.5289306640625\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=1000,\n",
    "                          degree=4,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.1,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=500,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-152951\n",
      "Model checkpoints at checkpoints/rl/20210525-152951\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1)                 2002      \n",
      "=================================================================\n",
      "Total params: 2,002\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2193.317138671875 Valid: 1799.4388427734375 Test: 1841.3829345703125\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=2.0,\n",
    "                          degree=5,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.001,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=1000,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-153530\n",
      "Model checkpoints at checkpoints/rl/20210525-153530\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 1)                 5005      \n",
      "=================================================================\n",
      "Total params: 5,005\n",
      "Trainable params: 5,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 2105.380615234375 Valid: 1966.4605712890625 Test: 1889.2669677734375\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=5.0,\n",
    "                          degree=6,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=1000,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-154102\n",
      "Model checkpoints at checkpoints/rl/20210525-154102\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1)                 11440     \n",
      "=================================================================\n",
      "Total params: 11,440\n",
      "Trainable params: 11,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1980.1461181640625 Valid: 1836.9769287109375 Test: 1943.13427734375\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=5.0,\n",
    "                          degree=7,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=1000,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-154723\n",
      "Model checkpoints at checkpoints/rl/20210525-154723\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 1)                 24310     \n",
      "=================================================================\n",
      "Total params: 24,310\n",
      "Trainable params: 24,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1895.37890625 Valid: 1869.8349609375 Test: 2096.034912109375\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=5.0,\n",
    "                          degree=8,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=1000,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-163054\n",
      "Model checkpoints at checkpoints/rl/20210525-163054\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 1)                 48620     \n",
      "=================================================================\n",
      "Total params: 48,620\n",
      "Trainable params: 48,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1994.5565185546875 Valid: 2052.66357421875 Test: 2201.368408203125\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=5.0,\n",
    "                          degree=9,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=1000,\n",
    "                          batch_size=32\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/20210525-163547\n",
      "Model checkpoints at checkpoints/rl/20210525-163547\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 1)                 48620     \n",
      "=================================================================\n",
      "Total params: 48,620\n",
      "Trainable params: 48,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=5.0,\n",
    "                          degree=9,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=1000,\n",
    "                          batch_size=32,\n",
    "                          regularizer='l1',\n",
    "                          regularizer_lambda=1e-3\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=5.0,\n",
    "                          degree=9,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.01,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          epochs=1000,\n",
    "                          batch_size=32,\n",
    "                          regularizer='l2',\n",
    "                          regularizer_lambda=1e-3\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                          learning_rate=5.0,\n",
    "                          degree=15,\n",
    "                          scheduler='exponential-decay',\n",
    "                          decay_rate=0.001,\n",
    "                          optimizer='adam',\n",
    "                          beta_1=0.9,\n",
    "                          beta_2=0.99,\n",
    "                          patience=200,\n",
    "                          delta=1,\n",
    "                          epochs=10000,\n",
    "                          batch_size=64\n",
    "                         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
