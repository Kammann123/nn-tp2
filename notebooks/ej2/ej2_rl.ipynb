{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 2 - Ejercicio 2\n",
    "# Notebook #2: Regresión Lineal + Features Polinomiales\n",
    "\n",
    "## Integrantes del grupo\n",
    "* Gaytan, Joaquín Oscar\n",
    "* Kammann, Lucas Agustín"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuentes\n",
    "\n",
    "### Link: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\n",
    "En esta fuente se puede encontrar una breve explicación del MAE y del MSE, una comparación entre ambos respecto de su comportamiento en entrenamiento frente a conjuntos de datos con y sin outliers en el error de los resultados, y luego una comparación de su comportamiento durante entrenamiento a razón de cómo son sus gradientes, lo cual provoca en el caso del MAE que la convergencia sea más lenta y sea necesario utilizar un **learning rate dinámico**. Explica que, si nos importa que la presencia de outliers tenga un impacto directo sobre el modelo, deberíamos utilizar MSE, mientras que si deseamos que no afecte demasiado podemos emplear MAE.\n",
    "\n",
    "### Link: https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "En esta fuente se puede encontrar una explicación de los tres métodos para learning rate dinámico utilizados, el **time-based decay**, el **step decay** y el **exponential decay**, empleando para algunos de ellos la clase de Keras llamada Learning Rate Scheduler, que permite modificar a gusto del usuario el valor del learning rate a través del proceso.\n",
    "\n",
    "### Link: https://stackoverflow.com/questions/46308374/what-is-validation-data-used-for-in-a-keras-sequential-model\n",
    "Esta discusión de StackOverflow es interesante sobre la separación de los datasets en entrenamiento, validación y evaluación del modelo, la use para verificar algunas cuestiones sobre cómo usaba la información de validación Keras, entre otras cosas.\n",
    "\n",
    "### Link: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
    "Explicación sobre el uso de **early stopping**, donde básicamente buscamos parar el entrenamiento aunque no se hayan terminado de correr todos los epochs predefinidos, porque se detecta que no hay mejoría en los resultados obtenidos, para ello se emplea la métrica evaluada sobre el conjunto de validación.\n",
    "\n",
    "### Link: https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/\n",
    "Explicación sobre el uso de **features polinomiales**, que básicamente consiste en agregar nuevas variables de entrada al modelo a partir de potencias obtenidas entre las variables de entrada originales. De esta forma, el espacio que conforman las variables es de mayor dimensión y por ello la solución es más flexible, aunque hay que tener cuidado de que no se ajuste demasiado provocando **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the database from the .csv file into a pandas dataframe\n",
    "df = pd.read_csv('../../databases/insurance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import helper\n",
    "importlib.reload(helper);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Codificación de variables no numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the sex variable or feature and create a new column in the dataframe \n",
    "# with the encoded version of the gender\n",
    "sex_encoder = preprocessing.LabelEncoder()\n",
    "sex_encoder.fit(df['sex'])\n",
    "df['sex-encoded'] = sex_encoder.transform(df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the smoker variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the smoker\n",
    "smoker_encoder = preprocessing.LabelEncoder()\n",
    "smoker_encoder.fit(df['smoker'])\n",
    "df['smoker-encoded'] = smoker_encoder.transform(df['smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one hot encoder and fit the available types of regions in the dataset\n",
    "region_encoder = preprocessing.OneHotEncoder()\n",
    "region_encoder.fit(df['region'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# Transform all entries into the one hot encoded representation\n",
    "encoded_regions = region_encoder.transform(df['region'].to_numpy().reshape(-1, 1)).toarray()\n",
    "\n",
    "# Add each new encoded variable or feature to the dataset\n",
    "for i, category in enumerate(region_encoder.categories_[0]):\n",
    "    df[f'{category}-encoded'] = encoded_regions.transpose()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Eliminando outliers\n",
    "A partir del análisis realizado sobre la base de datos sobre la cual se entrenan los modelos, se detectaron outliers en la variable del índice de masa corporal, se decide remover estos datos por completo ya que no fue necesario tener en cuenta una estrategia para corregir datos incompletos o incorrectos y el impacto que tiene sobre la totalidad de datos es menos del 1%. Entonces, se eliminan los casos con outliers del BMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers by setting NaN on those rows at the column of BMI\n",
    "helper.remove_outliers(df, 'bmi')\n",
    "\n",
    "# Remove NaN values from the dataframe\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Filtrado de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering or removing of non desired variables\n",
    "df_x = df[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded', 'northwest-encoded', 'northeast-encoded', 'southwest-encoded', 'southeast-encoded']]\n",
    "df_y = df['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Separación de los conjuntos\n",
    "Se realiza la separación del conjunto de datos original en **train**, **valid** y **test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.2, random_state=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regresión Lineal\n",
    "En este punto, se comienzan las pruebas y experimentos de los modelos utilizados, con el objetivo de ganar intuición en los beneficios, mejores o desventajas en cómo se puedan modificar y agregar hiperparámetros. Las *observaciones y conclusiones* son comentarios de situaciones particulares que sucedieron o suceden en la última versión de los experimentos, a modo de reflexión y relevar las particularlidades más notables con las que nos fuimos encontrando.\n",
    "\n",
    "#### Observaciones y conclusiones\n",
    "1. Al principio, sucedió que el MAE era muy lento para convergencia, lo cual tiene sentido por cómo es su gradiente. Particularmente, comparado con MSE, es mucho más lento. Empezamos probando modificar de forma estática y a mano el **learning rate**.\n",
    "2. Luego, con un learning rate cada vez mayor, pudimos observar que el entrenamiento era más rápido, pero sucedían dos cuestiones. En primer lugar, que se producía una especie oscilación en torno a un valor que asumo que es el mínimo al cual se acerca el entrenamiento, con lo cual sería necesario disminuir cerca de ahí el valor del learning rate. Por otro lado, este mínimo no era el mismo mínimo que obtuve con el MSE, debe ser un plateau, un mínimo local pero no el absoluto. Me propuse usar **learning rate dinámico** y **comenzar de diferentes puntos**.\n",
    "3. Cuando probamos utilizar MSE, si no normalizaba con z-score todas las variables, rápidamente divergía la función de costo y se rompía el entrenamiento. Por otro lado, la misma normalización afectaba mucho al entrenamiento del MAE. *¿Por qué?* Lo pude corregir un poco al aumentar el learning rate por un factor, lo cual debe tener sentido si se considera que ahora las variables estando normalizadas tienen una menor magnitud lo cual puede producir que los pasos sean menores que antes, y por eso se ralentizó.\n",
    "4. Interesante, encontramos esta discusión https://datascience.stackexchange.com/questions/9020/do-i-have-to-standardize-my-new-polynomial-features a raiz de una pregunta bastante sencilla, **¿por qué no está mejorando la métrica con mayor orden de polynomial features?**. Resulta ser que normalizando las variables y luego aplicando polynomial features, obtengo nuevas variables que siguen encontrándose en el intervalo [0,1] pero que su orden de magnitud es mucho menor. *Conclusión, siempre normalizar las variables que entran al modelo, y por ende si aplicas polynomial features tenés que normalizar luego de crear las nuevas variables.*\n",
    "5. Con la corrección mencionada anteriormente con respecto a la normalización, mejoró el resultado de ordenes grandes de polinomios.\n",
    "6. Nos llamó la atención que por lo general los resultados de validación eran mejores que en entrenamiento, y además, esta diferencia se achica más a medida que aumenta el orden de los polinomios. En este escenario, pensamos en dos posibilidades, un posible **underfitting** o bien un problema con los conjuntos de train, valid y test. No obstante, encontramos este artículo como una tercera opción que creemos que no aplica a nuestro problema. Este artículo menciona algo que puede ser útil https://keras.io/getting_started/faq/#why-is-my-training-loss-much-higher-than-my-testing-loss, una posibilidad sería que la validación sobre un epoch siempre tienda a ser mejor que el promedio del train en los batch, porque fue entrenándose mejor. Aunque no me convence después de muchos epochs que suceda esto. **¿Deberíamos estar usando k-folding?** **¿Deberíamos mejorar el modelo para no tener underfitting?**\n",
    "7. Finalmente, se determinó que la baja cantidad de muestras en los datos provoca que los estimadores tengan una mayor varianza lo cual afecta drásticamente la estimación en la evaluación del modelo. En principio, se dejó utilizando el **método hold-out**, conscientes de que deberíamos aplicar **k-folding** para poder dar una estimación de menor varianza en el test.\n",
    "8. En el conjunto de datos utilizados para **train**, **valid** y **test**, existen nueve **outliers** en el **BMI**, es menos de 1% del conjunto de datos pero antes no los estabamos removiendo, al quitarlos disminuyeron notoriamente las métricas de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ej2 import rl_helper\n",
    "importlib.reload(rl_helper);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Experimentos, análisis y observaciones\n",
    "En los experimentos a continuación se realizan diversas pruebas, y se van presentando comentarios y observaciones o conclusiones sobre los resultados. Vale mencionar, que en todos los escenarios se aplicó **early stop**, monitoreando el resultado de la función de costo sobre el conjunto de validación se busca parar el proceso de entrenamiento cuando no se detectan mejorías, para evitar tiempos de entrenamiento sin un resultado efectivo. Esto se acompaña con el uso de **model checkpoint**, así se registra el estado del modelo en el mejor resultado registrado, el cual se restaura al final del entrenamiento para evaluar los resultados efectivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold the degree of the polynomial feature used and the performance in train and valid sets,\n",
    "# elements should be of the format (degree, mae_train, mae_valid)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1. Regresión lineal\n",
    "En estas primeras pruebas, se emplea una **regresión lineal** sin ninguna modificación, agregado, técnica o método especial. Es decir, se utiliza un modelo con una **capa densa de una única neurona o unidad**, que será la salida del modelo, con una función de activación **lineal** y una función de costo **MAE**. Por defecto, inicialmente se entrena utilizando como optimizador **SGD**.\n",
    "\n",
    "#### Observaciones y conclusiones\n",
    "Durante el entrenamiento del modelo, inicialmente la función de costo es muy elevada y además resulta un proceso muy lento, con lo cual la convergencia a una solución óptima requiere mucho tiempo. Esto se debe particularmente al uso del valor medio del error absoluto (**MAE**) como función de costo, cuya gradiente es menor que para otras funciones de costo como puede ser el caso del **MSE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-1/20210531-004409\n",
      "Model checkpoints at checkpoints/rl/experiment-1/20210531-004409\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 12513.67 Valid: 11605.37 Test: 13234.44\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=0.1,\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-1'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([1, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-2/20210531-004937\n",
      "Model checkpoints at checkpoints/rl/experiment-2/20210531-004937\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 7637.68 Valid: 6874.74 Test: 8443.88\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1.0,\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-2'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([1, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2. Learning Rate Scheduling\n",
    "Se busca solucionar la lenta convergencia que se tiene con una función de costo **MAE** al utilizar valores más grandes de **learning rate**, no obstante, es sabido que esto puede producir divergencia u oscilaciones indeseadas del modelo, entonces se propone probar diferentes esquemas dinámicos en donde el valor empleado puede evolucionar el tiempo, particularmente disminuyendo. Así, se empiezan con valores altos de learning rate que va decayendo con el tiempo, permitiendo alcanzar mínimos de forma más rapida y luego converger sin demasaidas oscilaciones.\n",
    "\n",
    "#### Observaciones y conclusiones\n",
    "Se puede notar una diferencia fuerte en la cantidad de tiempo requerido para entrenar el modelo, gracias al uso del **learning rate scheduling**. Además, eso permitió encontrar resultados mucho mejores. Particularmente, no se detectaron diferencias grandes en el resultado respecto del tipo de variación dinámica que se emplea. Se probó utilizar además el optimizador **adam** y el optimizador **RMSprop** y se vió que la curva de aprendizaje presentaba más variación en forma de picos y valles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-3/20210531-005500\n",
      "Model checkpoints at checkpoints/rl/experiment-3/20210531-005500\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3668.72 Valid: 3322.72 Test: 4478.52\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     scheduler='time-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-3'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([1, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-4/20210531-005701\n",
      "Model checkpoints at checkpoints/rl/experiment-4/20210531-005701\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 10865.36 Valid: 9963.14 Test: 11573.5\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1.0,\n",
    "                                                     scheduler='time-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-4'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-5/20210531-010356\n",
      "Model checkpoints at checkpoints/rl/experiment-5/20210531-010356\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3251.96 Valid: 3503.46 Test: 4244.9\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     scheduler='time-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-5'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-6/20210531-010718\n",
      "Model checkpoints at checkpoints/rl/experiment-6/20210531-010718\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3432.96 Valid: 3402.63 Test: 4363.57\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     scheduler='step-decay',\n",
    "                                                     drop_rate=0.5,\n",
    "                                                     epochs_drop=10,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-6'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-7/20210531-010925\n",
      "Model checkpoints at checkpoints/rl/experiment-7/20210531-010925\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3219.83 Valid: 3516.67 Test: 4205.48\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     scheduler='step-decay',\n",
    "                                                     drop_rate=0.5,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     epochs_drop=10,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-7'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-8/20210531-011105\n",
      "Model checkpoints at checkpoints/rl/experiment-8/20210531-011105\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3737.59 Valid: 3352.02 Test: 4571.47\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.07,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-8'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-9/20210531-011246\n",
      "Model checkpoints at checkpoints/rl/experiment-9/20210531-011246\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3247.38 Valid: 3519.07 Test: 4276.44\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-9'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-9/20210531-011433\n",
      "Model checkpoints at checkpoints/rl/experiment-9/20210531-011433\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 3265.79 Valid: 3502.68 Test: 4280.26\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     optimizer='rmsprop',\n",
    "                                                     momentum=0.9,\n",
    "                                                     rho=0.99,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=500,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-9'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Polynomial Features\n",
    "Es probable que la complejidad del modelo no sea lo suficientemente alta para poder representar correctamente la solución al problema, una posible solución es utilizar **polynomial features** que permite crear un espacio de soluciones más grande. A continuación, se prueba con diferentes órdenes y se observan resultados.\n",
    "\n",
    "#### Observaciones y conclusiones\n",
    "1. En general, los resultados son mucho mejores que con el escenario anterior de primer orden.\n",
    "2. A medida que aumentamos el orden del feature polinomial, se observan pocas variaciones en la métrica de entrenamiento, pero se ve un incremento en la métrica de validación.\n",
    "3. A medida que elevamos más el orden del feature polinomial, se puede observar que empiezan a suceder situaciones de **overfitting**, el modelo nuevamente pierde capacidad de generalización, pero esta vez porque su performance en entrenamiento da mucho mejor que en el resto de los casos. Esto evidencia que aprendió demasiado sobre entrenamiento. En este punto, deberíamos conseguir más datos para permitirle generalizar mejor, o limitar el gran espacio de soluciones que posee frente a la gran complejidad por ser de alto orden.\n",
    "4. Observar que para ordenes muy grandes fue necesario reducir el batch size para poder encontrar una buena solución.\n",
    "5. Observar que el learning rate empleado ahora es menor.\n",
    "6. Observar que para el mayor órden la métrica se deterioró, puede ser que no haya un buen ajuste de hiperparámetros para alcanzar el mínimo, o también podría pasar que la información para obtener un buen resultado no es suficiente y faltan datos para entrenar un orden tan elevado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-10/20210531-065653\n",
      "Model checkpoints at checkpoints/rl/experiment-10/20210531-065653\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1)                 55        \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1784.37 Valid: 1959.21 Test: 2681.13\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     degree=2,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.09,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-10'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([2, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-11/20210531-070104\n",
      "Model checkpoints at checkpoints/rl/experiment-11/20210531-070104\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 1)                 220       \n",
      "=================================================================\n",
      "Total params: 220\n",
      "Trainable params: 220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1769.21 Valid: 2054.24 Test: 2831.25\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     degree=3,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.1,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-11'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([3, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-12/20210531-070520\n",
      "Model checkpoints at checkpoints/rl/experiment-12/20210531-070520\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 1)                 715       \n",
      "=================================================================\n",
      "Total params: 715\n",
      "Trainable params: 715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1772.05 Valid: 2225.83 Test: 3007.08\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     degree=4,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.1,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-12'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([4, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-13/20210531-070923\n",
      "Model checkpoints at checkpoints/rl/experiment-13/20210531-070923\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1)                 2002      \n",
      "=================================================================\n",
      "Total params: 2,002\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1727.61 Valid: 2005.75 Test: 2868.35\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=2.0,\n",
    "                                                     degree=5,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.001,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-13'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([5, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-14/20210531-071855\n",
      "Model checkpoints at checkpoints/rl/experiment-14/20210531-071855\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 1)                 5005      \n",
      "=================================================================\n",
      "Total params: 5,005\n",
      "Trainable params: 5,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1689.06 Valid: 2113.95 Test: 2956.51\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=5.0,\n",
    "                                                     degree=6,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-14'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([6, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-15/20210531-072459\n",
      "Model checkpoints at checkpoints/rl/experiment-15/20210531-072459\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 1)                 11440     \n",
      "=================================================================\n",
      "Total params: 11,440\n",
      "Trainable params: 11,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1621.28 Valid: 2152.73 Test: 3035.9\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=5.0,\n",
    "                                                     degree=7,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-15'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([7, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-16/20210531-073103\n",
      "Model checkpoints at checkpoints/rl/experiment-16/20210531-073103\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 1)                 24310     \n",
      "=================================================================\n",
      "Total params: 24,310\n",
      "Trainable params: 24,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1578.54 Valid: 2131.66 Test: 3295.88\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=5.0,\n",
    "                                                     degree=8,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-16'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([8, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-17/20210531-073736\n",
      "Model checkpoints at checkpoints/rl/experiment-17/20210531-073736\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 1)                 48620     \n",
      "=================================================================\n",
      "Total params: 48,620\n",
      "Trainable params: 48,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 1535.92 Valid: 2131.7 Test: 3511.49\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=5.0,\n",
    "                                                     degree=9,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.01,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-17'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([9, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-18/20210531-141804\n",
      "Model checkpoints at checkpoints/rl/experiment-18/20210531-141804\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 1)                 92378     \n",
      "=================================================================\n",
      "Total params: 92,378\n",
      "Trainable params: 92,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 5420.86 Valid: 5363.93 Test: 6841.81\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=5.0,\n",
    "                                                     degree=10,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.001,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-18'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([10, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de la métrica en función del orden de feature polinomial\n",
    "Utilizando el registro de los resultados para cada orden de feature polinomial,se grafica la evolución para observar el comportamiento y sacar conclusiones a partir de ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decoding results into its components\n",
    "results_orders = [order for order, mae_train, mae_valid in results]\n",
    "results_mae_train = [mae_train for order, mae_train, mae_valid in results]\n",
    "results_mae_valid = [mae_valid for order, mae_train, mae_valid in results]\n",
    "\n",
    "# Plot each of them with a color and a label\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(results_orders, results_mae_train, marker='o', color='blue', label='Train')\n",
    "ax.plot(results_orders, results_mae_valid, marker='o', color='orange', label='Valid')\n",
    "ax.set_xlabel('Orden', fontsize=14)\n",
    "ax.set_ylabel('MAE', fontsize=14)\n",
    "ax.set_title('Performance vs Orden', fontsize=15)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Regularización\n",
    "A partir de los resultados anteriores, se observa que a medida que aumentamos el orden del feature polinomial el modelo es capaz de resolver mejor el problema, no sólo mejorando la métrica, sino generalizando mejor. No obstante, cuando se eleva demasiado el orden del polinomio se empieza a enfrentar un problema de overfitting. Esto es, que el modelo presenta un espacio de soluciones de muy alta dimensionalidad que le permite adaptarse flexiblemente al comportamiento del conjunto de entrenamiento, entonces aunque esto no implique que la métrica de entrenamiento mejore, sí implica que la solución obtenida está muy arraigada a la naturaleza del conjunto de entrenamiento.\n",
    "El objetivo ahora es emplear **regularización** para poder restringir el espacio de soluciones, utilizando los métodos **L1** y **L2**, para ver si se puede reducir el overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-19/20210531-142359\n",
      "Model checkpoints at checkpoints/rl/experiment-19/20210531-142359\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 1)                 2002      \n",
      "=================================================================\n",
      "Total params: 2,002\n",
      "Trainable params: 2,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 8334.6 Valid: 7604.0 Test: 8873.72\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000.0,\n",
    "                                                     degree=5,\n",
    "                                                     regularizer='l2',\n",
    "                                                     regularizer_lambda=6-4,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.09,\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=32,\n",
    "                                                     tag='experiment-19'\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4. MSE (Mean Square Error) como función de costo\n",
    "Por lo general, otra función de costo y métrica ampliamente utilizada en problemas de regresión es el **MSE**. Es sabido que la diferencia esencial entre las funciones de costo **MAE** y **MSE**, está en que cada una de ellas maximiza la verosimilitud del modelo con el conjunto de datos de entrenamiento, asumiendo que dada una colección de variables o features de entrada, la salida del modelo se comportará con una distribución de Laplace o Gaussiana respectivamente. Es decir, en **MAE** asumimos que la salida tendrá una distribución de Laplace y en **MSE** una distribución Gaussiana. La solución para cada caso será el punto de mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logs at tb-logs/rl/experiment-20/20210531-142655\n",
      "Model checkpoints at checkpoints/rl/experiment-20/20210531-142655\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 1)                 55        \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[MAE] Train: 18820992.0 Valid: 23315550.0 Test: 35659096.0\n"
     ]
    }
   ],
   "source": [
    "# Run model experiment\n",
    "mae_train, mae_valid, mae_test = rl_helper.run_model(x_train, y_train, x_valid, y_valid, x_test, y_test,\n",
    "                                                     learning_rate=1000,\n",
    "                                                     degree=2,\n",
    "                                                     scheduler='exponential-decay',\n",
    "                                                     decay_rate=0.09,\n",
    "                                                     loss='mse',\n",
    "                                                     optimizer='adam',\n",
    "                                                     beta_1=0.9,\n",
    "                                                     beta_2=0.99,\n",
    "                                                     patience=50,\n",
    "                                                     min_delta=5,\n",
    "                                                     epochs=1000,\n",
    "                                                     batch_size=64,\n",
    "                                                     tag='experiment-20'\n",
    "                                                    )\n",
    "\n",
    "# Register the results for the degree used\n",
    "results.append([2, mae_train, mae_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Conclusiones\n",
    "* La diferencia entre **MAE** y **MSE** desde la interpretación de cómo afectan al modelo, es que la penalización de un error es distinta a medida que es mayor. Esto provoca que en el MAE los errores sean tratados de igual forma, con lo cual el modelo tiende a solucionar el problema minimizando ese error que es más sencillo para los errores pequeños, dado que en los valores grandes se pueden encontrar outliers. Por otro lado, MSE da mayor peso o penalización a los pesos altos, esto fuerza al modelo a adaptarse para obtener una solución en donde se tengan en cuenta ampliamente los outliers. En conclusión, si se observan las predicciones de un modelo entrenado minimizando MAE y otro minimizando MSE, se verá esta clara distinción en cómo se distribuyen los errores. El MAE tenderá a tener error en los apartamientos más granes, el MSE tenderá a buscar un compromiso para incluir a los outliers, por lo cual su error tendrá una mayor distribución entre pequeños y grandes errores. **¡Importante! No ingorar**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
