{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuentes\n",
    "\n",
    "### Link: https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "Explicación sobre las redes neuronales con múltiples capas, y lo que más interesante me pareció fue la enumeración de diferentes enfoques para saber cómo proponer la cantidad de capas y las neurones por cada capa a utilizar en el modelo de la red neuronal.\n",
    "\n",
    "### Link: https://medium.com/fintechexplained/what-are-hidden-layers-4f54f7328263\n",
    "En este artículo hace una explicación de las capas de una red neuronal de múltiples capas, útil para diferencias capa de entrada, salida y capas ocultas o intermedias.\n",
    "\n",
    "### Link: https://mmuratarat.github.io/2019-06-12/embeddings-with-numeric-variables-Keras\n",
    "Este artículo es importante porque te explica cómo usar la API funcional de Keras, que fue útil a la hora de utilizar embeddings porque necesitabamos que las nuevas entradas del modelo generadas por la capa de embedding se juntaran con las demás variables del problema, para lo cual tuvimos que emplear una capa *concatenate*, que funciona al utilizar la API funciona de Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import helper\n",
    "importlib.reload(helper);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the database from the .csv file into a pandas dataframe\n",
    "df = pd.read_csv('../../databases/insurance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Codificación de variables no numéricas o categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the sex variable or feature and create a new column in the dataframe \n",
    "# with the encoded version of the gender\n",
    "sex_encoder = preprocessing.LabelEncoder()\n",
    "sex_encoder.fit(df['sex'])\n",
    "df['sex-encoded'] = sex_encoder.transform(df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the smoker variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the smoker\n",
    "smoker_encoder = preprocessing.LabelEncoder()\n",
    "smoker_encoder.fit(df['smoker'])\n",
    "df['smoker-encoded'] = smoker_encoder.transform(df['smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the region variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the region\n",
    "region_encoder = preprocessing.LabelEncoder()\n",
    "region_encoder.fit(df['region'])\n",
    "df['region-encoded'] = region_encoder.transform(df['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>sex-encoded</th>\n",
       "      <th>smoker-encoded</th>\n",
       "      <th>region-encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges  sex-encoded  \\\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400            0   \n",
       "1   18    male  33.770         1     no  southeast   1725.55230            1   \n",
       "2   28    male  33.000         3     no  southeast   4449.46200            1   \n",
       "3   33    male  22.705         0     no  northwest  21984.47061            1   \n",
       "4   32    male  28.880         0     no  northwest   3866.85520            1   \n",
       "\n",
       "   smoker-encoded  region-encoded  \n",
       "0               1               3  \n",
       "1               0               2  \n",
       "2               0               2  \n",
       "3               0               1  \n",
       "4               0               1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Filtrado o eliminación de variables no necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering or removing of non desired variables\n",
    "df_x = df[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded', 'region-encoded']]\n",
    "df_y = df['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Separación de los conjuntos\n",
    "Es importante notar que, se realiza la separación del conjunto de datos original en **train**, **valid** y **test**, por fuera del framework de Keras para garantizar un adecuado tratamiento de los conjuntos acorde a la metodología empleada. En otras palabras, de esta forma nos aseguramos que cualquier preprocesamiento o normalización sobre validación (valid) y evaluación (test) se realiza a partir de la información obtenida en entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Normalización de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables where the z-score will be applied\n",
    "scalable_variables = ['bmi', 'age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Conjuntos de entrenamiento, validación y evaluación\n",
    "Particularmente, para las redes neuronales de múltiples capas, en este problema vamos a estar utilizando una capa de *embedding* para poder reducir la dimensionalidad necesaria para desacoplar las categorías de la variable *region*. Es decir, normalmente podríamos utilizar un *one hot encoding* para independizar en diferentes variables, pero produce una mayor dimensionalidad y poca interpretabilidad de lo que la red neuronal aprende durante el entrenamiento. En conclusión, debemos separar los grupos de entradas por el formato de entrada de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x_train[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_train['region-encoded']]\n",
    "x_valid = [x_valid[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_valid['region-encoded']]\n",
    "x_test = [x_test[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_test['region-encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir='tb-logs/mlp/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "    histogram_freq=1,\n",
    "    embeddings_freq=1, \n",
    "    update_freq='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers_neurons=[], layers_activation=[]):\n",
    "    \"\"\" Creates a neural network model using the given hyperparameters, where the model is developed\n",
    "        using the Keras framework. \n",
    "        The neural network is used to solve a regression problem where the input variables are:\n",
    "            * bmi\n",
    "            * sex\n",
    "            * region\n",
    "            * children\n",
    "            * age \n",
    "            * smokes\n",
    "        And the output variable is the amount of money charged for medical issues.\n",
    "        \n",
    "        @param layers_neurons List containing amount of neurons for each individual hidden layer\n",
    "        @param layers_activation List containing the activation function for each individual hidden layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate the amount of layers described\n",
    "    if len(layers_neurons) != len(layers_activation):\n",
    "        raise ValueError('Invalid amount of layers in both the neurons and the activation function description')\n",
    "\n",
    "    # Internal class parameters\n",
    "    layer_count = len(layers_neurons)\n",
    "\n",
    "    # Create two input layers for the categorical and the numerical variables\n",
    "    x1 = keras.layers.Input(shape=(5, ))\n",
    "    x2 = keras.layers.Input(shape=(1, ))\n",
    "\n",
    "    # Create an embedding layer with the categorical variable and create\n",
    "    # a new input layer for the neural network, combining both the embedding and the \n",
    "    # numerical variable input layer\n",
    "    embedding = keras.layers.Embedding(4, 2, input_length=1, embeddings_initializer='normal')(x2)\n",
    "    flatten = keras.layers.Flatten()(embedding)\n",
    "    x = keras.layers.Concatenate()([x1, flatten])\n",
    "\n",
    "    # Add the hidden layers to the neural network\n",
    "    layers = []\n",
    "    for layer_index in range(layer_count):\n",
    "        layer_neurons = layers_neurons[layer_index]\n",
    "        layer_activation = layers_activation[layer_index]\n",
    "        previous_layer = layers[layer_index - 1] if layer_index else x\n",
    "        current_layer = keras.layers.Dense(units=layer_neurons, activation=layer_activation)(previous_layer)\n",
    "        layers.append(current_layer)\n",
    "\n",
    "    # Add the output layer\n",
    "    y = keras.layers.Dense(units=1, activation='linear')(current_layer)\n",
    "\n",
    "    # Create the neural network model\n",
    "    return keras.Model(inputs=[x1, x2], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, x_valid, y_valid, layers_neurons=[], layers_activation=[], lr=0.1, batch_size=64, epochs=100):\n",
    "    \"\"\" Uses the create_model() function to create the neural network with the given hyperparameters,\n",
    "        compiles the model using the corresponding optimizer, loss function, metrics and other hyperparameters,\n",
    "        and runs the trainment process.\n",
    "        \n",
    "        @param x_train, y_train Train set\n",
    "        @param x_valid, y_valid Valid set\n",
    "        @param layers_neurons List containing amount of neurons for each individual hidden layer\n",
    "        @param layers_activation List containing the activation function for each individual hidden layer\n",
    "        @param lr Learning rate\n",
    "        @param batch_size Batch size\n",
    "        @param epochs Total amount of epochs for the training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the neural network\n",
    "    model = create_model(layers_neurons, layers_activation)\n",
    "    model.summary()\n",
    "    \n",
    "    # Compile the neural network\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=lr),\n",
    "        loss=keras.losses.MAE\n",
    "    )\n",
    "    \n",
    "    # Train the neural network\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        epochs=epochs, verbose=1, shuffle=True, batch_size=batch_size,\n",
    "        callbacks=[ tb_callback ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 2)         8           input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 7)            0           input_15[0][0]                   \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 3)            24          concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 3)            12          dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 3)            12          dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            4           dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 60\n",
      "Trainable params: 60\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 2s 138ms/step - loss: 14284.4329 - val_loss: 13100.4736\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14344.2906 - val_loss: 13099.2725\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14204.0163 - val_loss: 13098.0732\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13008.7438 - val_loss: 13096.8740\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13645.9583 - val_loss: 13095.6748\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13634.9976 - val_loss: 13094.4736\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13604.2998 - val_loss: 13093.2725\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13654.4820 - val_loss: 13092.0732\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14118.9914 - val_loss: 13090.8740\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13967.3033 - val_loss: 13089.6748\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13642.5056 - val_loss: 13088.4736\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 13694.4494 - val_loss: 13087.2725\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 13733.9270 - val_loss: 13086.0732\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13468.3366 - val_loss: 13084.8740\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13218.8356 - val_loss: 13083.6748\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13185.7399 - val_loss: 13082.4736\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13341.4407 - val_loss: 13081.2725\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14238.2547 - val_loss: 13080.0732\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13951.4775 - val_loss: 13078.8740\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 14223.5061 - val_loss: 13077.6748\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13573.0881 - val_loss: 13076.4736\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13730.5057 - val_loss: 13075.2725\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13645.6075 - val_loss: 13074.0732\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13543.7529 - val_loss: 13072.8740\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13015.0875 - val_loss: 13071.6748\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13826.3673 - val_loss: 13070.4736\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 14108.8631 - val_loss: 13069.2725\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13512.9437 - val_loss: 13068.0732\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13544.9792 - val_loss: 13066.8740\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13609.5799 - val_loss: 13065.6738\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13709.7971 - val_loss: 13064.4746\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13900.3554 - val_loss: 13063.2725\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13477.6915 - val_loss: 13062.0732\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13966.3543 - val_loss: 13060.8730\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13131.5821 - val_loss: 13059.6738\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 13606.7267 - val_loss: 13058.4746\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13521.4177 - val_loss: 13057.2734\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13718.4503 - val_loss: 13056.0732\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13664.3341 - val_loss: 13054.8730\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13682.0305 - val_loss: 13053.6738\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13990.0989 - val_loss: 13052.4746\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13927.6004 - val_loss: 13051.2734\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13331.3137 - val_loss: 13050.0732\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13604.3827 - val_loss: 13048.8740\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14049.5423 - val_loss: 13047.6738\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13563.0917 - val_loss: 13046.4746\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13460.2348 - val_loss: 13045.2734\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13397.8045 - val_loss: 13044.0732\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 13615.0281 - val_loss: 13042.8740\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 14161.7329 - val_loss: 13041.6738\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 13883.8779 - val_loss: 13040.4746\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 13556.1864 - val_loss: 13039.2734\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13733.4855 - val_loss: 13038.0732\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13792.2683 - val_loss: 13036.8740\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 14102.6864 - val_loss: 13035.6738\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13956.4980 - val_loss: 13034.4746\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13608.3513 - val_loss: 13033.2734\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13842.1449 - val_loss: 13032.0732\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13897.4641 - val_loss: 13030.8740\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13368.3422 - val_loss: 13029.6738\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13705.7199 - val_loss: 13028.4746\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13847.0657 - val_loss: 13027.2734\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 13549.0192 - val_loss: 13026.0742\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13951.8149 - val_loss: 13024.8740\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 14270.8905 - val_loss: 13023.6738\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 14083.5477 - val_loss: 13022.4746\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13491.2832 - val_loss: 13021.2754\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13384.8079 - val_loss: 13020.0742\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13447.5973 - val_loss: 13018.8740\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13462.3386 - val_loss: 13017.6738\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13492.0551 - val_loss: 13016.4746\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13390.0967 - val_loss: 13015.2754\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13615.8823 - val_loss: 13014.0742\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13622.9962 - val_loss: 13012.8740\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13732.1144 - val_loss: 13011.6738\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13636.4338 - val_loss: 13010.4746\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13242.9300 - val_loss: 13009.2754\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13134.3461 - val_loss: 13008.0742\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13539.0068 - val_loss: 13006.8740\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 13394.4066 - val_loss: 13005.6738\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13508.3707 - val_loss: 13004.4746\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13392.9837 - val_loss: 13003.2754\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 13422.9371 - val_loss: 13002.0752\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13086.3735 - val_loss: 13000.8740\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 13135.7852 - val_loss: 12999.6738\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13843.8223 - val_loss: 12998.4746\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13992.9061 - val_loss: 12997.2754\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 14320.2897 - val_loss: 12996.0752\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14062.7364 - val_loss: 12994.8740\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13547.8088 - val_loss: 12993.6738\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13517.5998 - val_loss: 12992.4746\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 13903.1410 - val_loss: 12991.2754\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13730.0104 - val_loss: 12990.0752\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 14496.1363 - val_loss: 12988.8740\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13401.3821 - val_loss: 12987.6738\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14301.2471 - val_loss: 12986.4746\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13737.3133 - val_loss: 12985.2754\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13627.5438 - val_loss: 12984.0752\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 13790.5871 - val_loss: 12982.8740\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 14193.4447 - val_loss: 12981.6738\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-77a5ec7a78ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_neurons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_activation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "model = train_model(x_train, y_train, x_valid, y_valid, layers_neurons=[3, 3, 3], layers_activation=['relu', 'relu', 'relu'])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_linear_regression_result(\n",
    "    y_test.to_numpy(), \n",
    "    model.predict([x_test[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_test[['region-encoded']]]), \n",
    "    result_label='Costo atención médica'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
