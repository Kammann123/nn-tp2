{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuentes\n",
    "\n",
    "### Link: https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\n",
    "Explicación sobre las redes neuronales con múltiples capas, y lo que más interesante me pareció fue la enumeración de diferentes enfoques para saber cómo proponer la cantidad de capas y las neurones por cada capa a utilizar en el modelo de la red neuronal.\n",
    "\n",
    "### Link: https://medium.com/fintechexplained/what-are-hidden-layers-4f54f7328263\n",
    "En este artículo hace una explicación de las capas de una red neuronal de múltiples capas, útil para diferencias capa de entrada, salida y capas ocultas o intermedias.\n",
    "\n",
    "### Link: https://mmuratarat.github.io/2019-06-12/embeddings-with-numeric-variables-Keras\n",
    "Este artículo es importante porque te explica cómo usar la API funcional de Keras, que fue útil a la hora de utilizar embeddings porque necesitabamos que las nuevas entradas del modelo generadas por la capa de embedding se juntaran con las demás variables del problema, para lo cual tuvimos que emplear una capa *concatenate*, que funciona al utilizar la API funciona de Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import helper\n",
    "importlib.reload(helper);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the database from the .csv file into a pandas dataframe\n",
    "df = pd.read_csv('../../databases/insurance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Codificación de variables no numéricas o categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the sex variable or feature and create a new column in the dataframe \n",
    "# with the encoded version of the gender\n",
    "sex_encoder = preprocessing.LabelEncoder()\n",
    "sex_encoder.fit(df['sex'])\n",
    "df['sex-encoded'] = sex_encoder.transform(df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the smoker variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the smoker\n",
    "smoker_encoder = preprocessing.LabelEncoder()\n",
    "smoker_encoder.fit(df['smoker'])\n",
    "df['smoker-encoded'] = smoker_encoder.transform(df['smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the region variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the region\n",
    "region_encoder = preprocessing.LabelEncoder()\n",
    "region_encoder.fit(df['region'])\n",
    "df['region-encoded'] = region_encoder.transform(df['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "      <th>sex-encoded</th>\n",
       "      <th>smoker-encoded</th>\n",
       "      <th>region-encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges  sex-encoded  \\\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400            0   \n",
       "1   18    male  33.770         1     no  southeast   1725.55230            1   \n",
       "2   28    male  33.000         3     no  southeast   4449.46200            1   \n",
       "3   33    male  22.705         0     no  northwest  21984.47061            1   \n",
       "4   32    male  28.880         0     no  northwest   3866.85520            1   \n",
       "\n",
       "   smoker-encoded  region-encoded  \n",
       "0               1               3  \n",
       "1               0               2  \n",
       "2               0               2  \n",
       "3               0               1  \n",
       "4               0               1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Filtrado o eliminación de variables no necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering or removing of non desired variables\n",
    "df_x = df[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded', 'region-encoded']]\n",
    "df_y = df['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Separación de los conjuntos\n",
    "Es importante notar que, se realiza la separación del conjunto de datos original en **train**, **valid** y **test**, por fuera del framework de Keras para garantizar un adecuado tratamiento de los conjuntos acorde a la metodología empleada. En otras palabras, de esta forma nos aseguramos que cualquier preprocesamiento o normalización sobre validación (valid) y evaluación (test) se realiza a partir de la información obtenida en entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Normalización de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables where the z-score will be applied\n",
    "scalable_variables = ['bmi', 'age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Conjuntos de entrenamiento, validación y evaluación\n",
    "Particularmente, para las redes neuronales de múltiples capas, en este problema vamos a estar utilizando una capa de *embedding* para poder reducir la dimensionalidad necesaria para desacoplar las categorías de la variable *region*. Es decir, normalmente podríamos utilizar un *one hot encoding* para independizar en diferentes variables, pero produce una mayor dimensionalidad y poca interpretabilidad de lo que la red neuronal aprende durante el entrenamiento. En conclusión, debemos separar los grupos de entradas por el formato de entrada de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x_train[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_train['region-encoded']]\n",
    "x_valid = [x_valid[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_valid['region-encoded']]\n",
    "x_test = [x_test[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_test['region-encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir='tb-logs/mlp/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"), \n",
    "    histogram_freq=1,\n",
    "    embeddings_freq=1, \n",
    "    update_freq='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers_neurons=[], layers_activation=[]):\n",
    "    \"\"\" Creates a neural network model using the given hyperparameters, where the model is developed\n",
    "        using the Keras framework. \n",
    "        The neural network is used to solve a regression problem where the input variables are:\n",
    "            * bmi\n",
    "            * sex\n",
    "            * region\n",
    "            * children\n",
    "            * age \n",
    "            * smokes\n",
    "        And the output variable is the amount of money charged for medical issues.\n",
    "        \n",
    "        @param layers_neurons List containing amount of neurons for each individual hidden layer\n",
    "        @param layers_activation List containing the activation function for each individual hidden layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate the amount of layers described\n",
    "    if len(layers_neurons) != len(layers_activation):\n",
    "        raise ValueError('Invalid amount of layers in both the neurons and the activation function description')\n",
    "\n",
    "    # Internal class parameters\n",
    "    layer_count = len(layers_neurons)\n",
    "\n",
    "    # Create two input layers for the categorical and the numerical variables\n",
    "    x1 = keras.layers.Input(shape=(5, ))\n",
    "    x2 = keras.layers.Input(shape=(1, ))\n",
    "\n",
    "    # Create an embedding layer with the categorical variable and create\n",
    "    # a new input layer for the neural network, combining both the embedding and the \n",
    "    # numerical variable input layer\n",
    "    embedding = keras.layers.Embedding(4, 2, input_length=1, embeddings_initializer='normal')(x2)\n",
    "    flatten = keras.layers.Flatten()(embedding)\n",
    "    x = keras.layers.Concatenate()([x1, flatten])\n",
    "\n",
    "    # Add the hidden layers to the neural network\n",
    "    layers = []\n",
    "    for layer_index in range(layer_count):\n",
    "        layer_neurons = layers_neurons[layer_index]\n",
    "        layer_activation = layers_activation[layer_index]\n",
    "        previous_layer = layers[layer_index - 1] if layer_index else x\n",
    "        current_layer = keras.layers.Dense(units=layer_neurons, activation=layer_activation)(previous_layer)\n",
    "        layers.append(current_layer)\n",
    "\n",
    "    # Add the output layer\n",
    "    y = keras.layers.Dense(units=1, activation='linear')(current_layer)\n",
    "\n",
    "    # Create the neural network model\n",
    "    return keras.Model(inputs=[x1, x2], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train, y_train, x_valid, y_valid, layers_neurons=[], layers_activation=[], lr=0.1, batch_size=64, epochs=100):\n",
    "    \"\"\" Uses the create_model() function to create the neural network with the given hyperparameters,\n",
    "        compiles the model using the corresponding optimizer, loss function, metrics and other hyperparameters,\n",
    "        and runs the trainment process.\n",
    "        \n",
    "        @param x_train, y_train Train set\n",
    "        @param x_valid, y_valid Valid set\n",
    "        @param layers_neurons List containing amount of neurons for each individual hidden layer\n",
    "        @param layers_activation List containing the activation function for each individual hidden layer\n",
    "        @param lr Learning rate\n",
    "        @param batch_size Batch size\n",
    "        @param epochs Total amount of epochs for the training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the neural network\n",
    "    model = create_model(layers_neurons, layers_activation)\n",
    "    model.summary()\n",
    "    \n",
    "    # Compile the neural network\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=lr),\n",
    "        loss=keras.losses.MAE\n",
    "    )\n",
    "    \n",
    "    # Train the neural network\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_valid, y_valid),\n",
    "        epochs=epochs, verbose=1, shuffle=True, batch_size=batch_size,\n",
    "        callbacks=[ tb_callback ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 1, 2)         8           input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2)            0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 7)            0           input_13[0][0]                   \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 3)            24          concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 3)            12          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 3)            12          dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            4           dense_18[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 60\n",
      "Trainable params: 60\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 2s 145ms/step - loss: 13675.5249 - val_loss: 13097.7285\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 13830.6586 - val_loss: 13079.2197\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 12952.2908 - val_loss: 13002.4365\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 9238.9866 - val_loss: 9526.9424\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 8573.0731 - val_loss: 15381.1826\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7840.2269 - val_loss: 39035.2930\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7878.6601 - val_loss: 62294.2539\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7508.2357 - val_loss: 86863.2188\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7397.3485 - val_loss: 120862.9141\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6396.9172 - val_loss: 159295.5938\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5970.1643 - val_loss: 156166.4688\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5816.5391 - val_loss: 145314.8594\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4675.6617 - val_loss: 152024.0469\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3982.3997 - val_loss: 164982.6094\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3783.2978 - val_loss: 145086.9062\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3878.8732 - val_loss: 160651.2344\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3720.8299 - val_loss: 145625.1562\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4078.6976 - val_loss: 155669.2031\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4099.8323 - val_loss: 157923.9062\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3791.9133 - val_loss: 145467.0469\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4109.5839 - val_loss: 161802.2188\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3574.9301 - val_loss: 148852.0156\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3841.9484 - val_loss: 160489.9688\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3638.7540 - val_loss: 156792.8594\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3370.5015 - val_loss: 161893.5625\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3919.5810 - val_loss: 153872.8281\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3964.5225 - val_loss: 160326.1562\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3506.5351 - val_loss: 154273.7500\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3713.4404 - val_loss: 171386.5469\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3602.5117 - val_loss: 146845.8750\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3842.5321 - val_loss: 154573.8906\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3699.9904 - val_loss: 165147.8281\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3983.6285 - val_loss: 157099.3438\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3621.2762 - val_loss: 162044.2969\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3566.0988 - val_loss: 162048.2969\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3625.2274 - val_loss: 156669.2188\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3826.4950 - val_loss: 158062.8281\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3793.2480 - val_loss: 155428.9219\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3653.5918 - val_loss: 157576.6562\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3758.1395 - val_loss: 165353.8438\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3750.8269 - val_loss: 156279.4062\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3742.9973 - val_loss: 168055.3906\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4001.2850 - val_loss: 158707.6094\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3499.7997 - val_loss: 161165.7812\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3549.9766 - val_loss: 163813.8750\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3683.4473 - val_loss: 164182.0938\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3483.6433 - val_loss: 162197.8125\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3592.7884 - val_loss: 163744.2188\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3726.1994 - val_loss: 163497.0156\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3613.3807 - val_loss: 166078.8125\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3971.4660 - val_loss: 166321.2656\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3749.5684 - val_loss: 166720.0938\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3641.3294 - val_loss: 165138.7344\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3777.0254 - val_loss: 173541.7500\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3436.2014 - val_loss: 167676.2969\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3313.7503 - val_loss: 170122.4219\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-5c3c66cbdc47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_neurons\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_activation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-99-c148bf52dc8f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(x_train, y_train, x_valid, y_valid, layers_neurons, layers_activation, lr, batch_size, epochs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Train the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     model.fit(\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1145\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m   2340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2341\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2342\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2344\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_start_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_log_embeddings\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m   2428\u001b[0m     embeddings_ckpt = os.path.join(self._log_write_dir, 'train',\n\u001b[0;32m   2429\u001b[0m                                    'keras_embedding.ckpt-{}'.format(epoch))\n\u001b[1;32m-> 2430\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_ckpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[0;32m   2124\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2125\u001b[0m       \u001b[1;31m# Record this checkpoint so it's visible from tf.train.latest_checkpoint.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2126\u001b[1;33m       checkpoint_management.update_checkpoint_state_internal(\n\u001b[0m\u001b[0;32m   2127\u001b[0m           \u001b[0msave_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2128\u001b[0m           \u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\u001b[0m in \u001b[0;36mupdate_checkpoint_state_internal\u001b[1;34m(save_dir, model_checkpoint_path, all_model_checkpoint_paths, latest_filename, save_relative_paths, all_model_checkpoint_timestamps, last_preserved_timestamp)\u001b[0m\n\u001b[0;32m    245\u001b[0m   \u001b[1;31m# Preventing potential read/write race condition by *atomically* writing to a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m   \u001b[1;31m# file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m   file_io.atomic_write_string_to_file(coord_checkpoint_filename,\n\u001b[0m\u001b[0;32m    248\u001b[0m                                       text_format.MessageToString(ckpt))\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[1;34m(filename, contents, overwrite)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0mtemp_pathname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".tmp\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0muuid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muuid4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m     \u001b[0mwrite_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m       \u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mwrite_string_to_file\u001b[1;34m(filename, file_content)\u001b[0m\n\u001b[0;32m    333\u001b[0m   \"\"\"\n\u001b[0;32m    334\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    198\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munused_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Make usable with \"with\" statement.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_buf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writable_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(x_train, y_train, x_valid, y_valid, layers_neurons=[3, 3, 3], layers_activation=['relu', 'relu', 'relu'])\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_linear_regression_result(\n",
    "    y_test.to_numpy(), \n",
    "    model.predict([x_test[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_test[['region-encoded']]]), \n",
    "    result_label='Costo atención médica'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
