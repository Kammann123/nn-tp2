{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuentes\n",
    "\n",
    "### Link: https://www.dlology.com/blog/how-to-do-hyperparameter-search-with-baysian-optimization-for-keras-model/\n",
    "Explicación de cómo utilizar el método de optimización bayesiana para buscar los hiper parámetros del modelo para resolver el problema.\n",
    "\n",
    "### Link: https://www.kaggle.com/prashant111/bayesian-optimization-using-hyperopt\n",
    "Ahora viendo cómo resolverlo utilizando la librería de **hyperopt**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the database from the .csv file into a pandas dataframe\n",
    "df = pd.read_csv('../../databases/insurance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Codificación de variables no numéricas o categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the sex variable or feature and create a new column in the dataframe \n",
    "# with the encoded version of the gender\n",
    "sex_encoder = preprocessing.LabelEncoder()\n",
    "sex_encoder.fit(df['sex'])\n",
    "df['sex-encoded'] = sex_encoder.transform(df['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the smoker variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the smoker\n",
    "smoker_encoder = preprocessing.LabelEncoder()\n",
    "smoker_encoder.fit(df['smoker'])\n",
    "df['smoker-encoded'] = smoker_encoder.transform(df['smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for the region variable or feature and create a new column in the dataframe\n",
    "# with the encoded version of the region\n",
    "region_encoder = preprocessing.LabelEncoder()\n",
    "region_encoder.fit(df['region'])\n",
    "df['region-encoded'] = region_encoder.transform(df['region'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Filtrado de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering or removing of non desired variables\n",
    "df_x = df[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded', 'region-encoded']]\n",
    "df_y = df['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Separación de los conjuntos\n",
    "Es importante notar que, se realiza la separación del conjunto de datos original en **train**, **valid** y **test**, por fuera del framework de Keras para garantizar un adecuado tratamiento de los conjuntos acorde a la metodología empleada. En otras palabras, de esta forma nos aseguramos que cualquier preprocesamiento o normalización sobre validación (valid) y evaluación (test) se realiza a partir de la información obtenida en entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.2, random_state=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Normalización de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables where the z-score will be applied\n",
    "scalable_variables = ['bmi', 'age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_valid.loc[:, scalable_variables] = scaler.transform(x_valid.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Conjuntos de entrenamiento, validación y evaluación\n",
    "Particularmente, para las redes neuronales de múltiples capas, en este problema vamos a estar utilizando una capa de *embedding* para poder reducir la dimensionalidad necesaria para desacoplar las categorías de la variable *region*. Es decir, normalmente podríamos utilizar un *one hot encoding* para independizar en diferentes variables, pero produce una mayor dimensionalidad y poca interpretabilidad de lo que la red neuronal aprende durante el entrenamiento. En conclusión, debemos separar los grupos de entradas por el formato de entrada de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [x_train[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_train['region-encoded']]\n",
    "x_valid = [x_valid[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_valid['region-encoded']]\n",
    "x_test = [x_test[['age', 'bmi', 'smoker-encoded', 'children', 'sex-encoded']], x_test['region-encoded']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import mlp_helper\n",
    "importlib.reload(mlp_helper);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Creación de la función **objetivo**\n",
    "Creamos la función **objetivo** la cual recibe los parámetros a partir de los cuales se crea el modelo, se lo entrena y se evalúa su performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(kwargs):\n",
    "    \"\"\" Objective function for the hyperopt algorithm. \"\"\"\n",
    "    \n",
    "    # Setting some fixed values\n",
    "    kwargs['tensorboard_on'] = False\n",
    "    kwargs['summary_on'] = False\n",
    "    kwargs['epochs'] = 500\n",
    "    kwargs['batch_size'] = 64\n",
    "    kwargs['optimizer'] = 'adam'\n",
    "    kwargs['beta_1'] = 0.9\n",
    "    kwargs['beta_2'] = 0.999\n",
    "    \n",
    "    # Running the model with the given hyperparameters and retrieving the test set performance\n",
    "    return mlp_helper.run_model(x_train, y_train, x_valid, y_valid, x_valid, y_valid, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Creación del **espacio** de hiperparámetros\n",
    "Creamos el **espacio** en el cual se encuentran cada uno de los hiperparáemtros que queremos probar, es decir, cuáles son los hiperparámetros y qué valores pueden tomar en cada prueba a realizar por el optimizador de hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'use_batch_normalization': hyperopt.hp.choice('use_batch_normalization', [False, True]),\n",
    "    'learning_rate': hyperopt.hp.choice('learning_rate', [1.0, 0.5, 0.1, 0.01]),\n",
    "    'hidden_layers': hyperopt.hp.quniform('hidden_layers', 0, 10, 1),\n",
    "    'units_per_layer': hyperopt.hp.quniform('units_per_layer', 2, 100, 10),\n",
    "    'hidden_layer_activation': hyperopt.hp.choice('hidden_layer_activation', ['relu', 'elu'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Selección del **algoritmo** de optimización\n",
    "Se puede elegir un **algoritmo** de optimización para el proceso de búsqueda de los mejores hiperparámetros dentro del espacio consignado acorde a los resultados que se van obteniendo de la función objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = hyperopt.tpe.suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Ejecutando la búsqueda\n",
    "Se ejecuta la búsqueda de los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = hyperopt.fmin(objective, space, algo=algorithm, max_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
