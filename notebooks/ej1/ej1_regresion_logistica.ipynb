{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "typical-athens",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 2 - Ejercicio 1 - Regresión Logística\n",
    "# Notebook #2: Implementación de una Regresión Lineal\n",
    "En esta notebook se busca implementar una regresión logística para poder estimar la condición de diabético de un paciente, perteneciente al Pima Indians Dataset analizado en la notebook anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-repeat",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "* Chequear correcto reemplazo de NaN por mean.\n",
    "* Meter el z-score en scripts comunes a ambos ejercicios. Chequear StandardScaler **correctamente inicializado**. **¿Errores de discretización?**\n",
    "    * ¿Dónde meto el área bajo la curva ROC y el F2? -> Respondido por Luqui y Karina.\n",
    "* Añadir **tensorboard** para log entre epochs. Migrar **TODOS LOS GRÁFICOS** a TensorBoard.\n",
    "    * Agregar evolución de f2-score sobre train en selección del umbral.\n",
    "* Graficar **learning rate**.\n",
    "* Sacar los evaluate con **test**, para evitar malas interpretaciones.\n",
    "* PRIMERA PRUEBA DE POLY (2) ESTÁ MAL! **Falta normalizar despues del poly**\n",
    "* Informar métricas secundarias\n",
    "* ¿Kernel/Activity regulariizer? -> **kernel regularizer** afecta a los pesos, **activity regularizer** a las salidas.\n",
    "* Chequear **Dropout**. Creo que no tendría sentido graficarlo en una sola capa.\n",
    "\n",
    "# ¿Qué cosas puedo variar?\n",
    "* Función de activación:\n",
    "    * Sigmoid\n",
    "    * RELU\n",
    "    * ELU\n",
    "    * tanh\n",
    "    * Leaky RELU\n",
    "    \n",
    "* Optimizador:\n",
    "    * SGD\n",
    "    * Adam\n",
    "    \n",
    "* Early Stopping: Para el entrenamiento cuando la **loss** deja de mejorar. Se pasa a través de un **callback**. (https://keras.io/api/callbacks/early_stopping/) (https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "* Kernel Initializer: Esto es, como son los pesos y bias iniciales. (https://keras.io/api/layers/initializers/)\n",
    "* Model Checkpoint: Guarda un checkpoint del modelo. Puede configurarse para elegir el mejor. Se pasa por **callback**. (https://keras.io/api/callbacks/model_checkpoint/)\n",
    "* Scheduling Learning Rate: Se hace variar el **learning rate** con una función. Es un **callback**. (https://keras.io/api/callbacks/learning_rate_scheduler/)\n",
    "* Reg. dropout: Para evitar overfitting, la capa de dropout \"borra\" una entrada de forma aleatoria y escala el resto. Es una **capa**. (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "* Regularización L1 y L2: Limita el espacio de soluciones agregando un término a la **función de costo**. (https://keras.io/api/layers/regularizers/)\n",
    "* Data Augmentation\n",
    "* Batch Normalization: Normaliza las entradas (media=0, dev=1). Es una **capa**. (https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-ranking",
   "metadata": {},
   "source": [
    "# Dudas\n",
    "* Al generar la métrica F2, ¿me devuelve por batch o por epoch? -> Esto finalmente se explica más adelante.\n",
    "* Al evaluar el predict en threshold selection ¿batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-blocking",
   "metadata": {},
   "source": [
    "# ¿Cuáles son los requerimientos para el **clasificador**?\n",
    "* Métrica principal: **Área bajo la curva ROC**\n",
    "* Buscar el **umbral de decisión** para maximizar el **f2 score** \n",
    "* Informar métricas secundarias:\n",
    "    * Especificidad - Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "    * Sensibilidad\n",
    "    * Valor predictivo positivo\n",
    "    * Valor predictivo negativo\n",
    "    \n",
    "* **Pregunta adicional**:\n",
    "Dada la situación en la cual cambia la prevalencia de la enfermedad en la población a ser del 20%. Se desea reutilizar el modelo sin volver a entrenar, ¿Cómo lo harían? ¿Qué métricas se mantienen igual y cuáles cambiarian?. **¿clases desbalanceadas -> class weight?**. Las f-score son buenas para casos no balanceados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-frost",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subject-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baking-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "through-means",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read database from .csv\n",
    "df = pd.read_csv('../../databases/diabetes.csv', delimiter=',')\n",
    "\n",
    "# Show first rows of data\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-action",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-apparatus",
   "metadata": {},
   "source": [
    "## 2.1 Filtrado de valores inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "future-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>733.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.153420</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>12.382158</td>\n",
       "      <td>10.476982</td>\n",
       "      <td>118.775855</td>\n",
       "      <td>6.924988</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  763.000000     733.000000     541.000000  394.000000   \n",
       "mean      3.845052  121.686763      72.405184      29.153420  155.548223   \n",
       "std       3.369578   30.535641      12.382158      10.476982  118.775855   \n",
       "min       0.000000   44.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   76.250000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  125.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  190.000000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  757.000000                768.000000  768.000000  768.000000  \n",
       "mean    32.457464                  0.471876   33.240885    0.348958  \n",
       "std      6.924988                  0.331329   11.760232    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.500000                  0.243750   24.000000    0.000000  \n",
       "50%     32.300000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering Glucose values\n",
    "df['Glucose'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Blood Pressure values\n",
    "df['BloodPressure'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Skin Thickness values\n",
    "df['SkinThickness'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Insulin values\n",
    "df['Insulin'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Body Mass Index values\n",
    "df['BMI'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-muscle",
   "metadata": {},
   "source": [
    "## 2.2 Remoción de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attempted-camping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from src.helper import remove_outliers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aerial-runner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>764.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>759.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.786649</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.115438</td>\n",
       "      <td>28.903346</td>\n",
       "      <td>132.610811</td>\n",
       "      <td>32.204005</td>\n",
       "      <td>0.429832</td>\n",
       "      <td>32.805007</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.278714</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>11.239072</td>\n",
       "      <td>9.865480</td>\n",
       "      <td>74.285393</td>\n",
       "      <td>6.491385</td>\n",
       "      <td>0.249684</td>\n",
       "      <td>11.113182</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   764.000000  763.000000     719.000000     538.000000  370.000000   \n",
       "mean      3.786649  121.686763      72.115438      28.903346  132.610811   \n",
       "std       3.278714   30.535641      11.239072       9.865480   74.285393   \n",
       "min       0.000000   44.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   75.000000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  120.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  177.500000   \n",
       "max      13.000000  199.000000     104.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  749.000000                739.000000  759.000000  768.000000  \n",
       "mean    32.204005                  0.429832   32.805007    0.348958  \n",
       "std      6.491385                  0.249684   11.113182    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.400000                  0.238000   24.000000    0.000000  \n",
       "50%     32.000000                  0.356000   29.000000    0.000000  \n",
       "75%     36.500000                  0.587000   40.000000    1.000000  \n",
       "max     50.000000                  1.191000   66.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "y_labels = ['Outcome']\n",
    "\n",
    "for column in x_labels:\n",
    "    remove_outliers(df, column)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-klein",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recreational-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arctic-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "posted-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output variables for the model\n",
    "df_x = df[x_labels]\n",
    "df_y = df[y_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "central-queen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Split the train_valid sub-dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accessory-reason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>425.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.896471</td>\n",
       "      <td>122.360000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.430713</td>\n",
       "      <td>32.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.269876</td>\n",
       "      <td>30.066982</td>\n",
       "      <td>10.805353</td>\n",
       "      <td>9.793471</td>\n",
       "      <td>70.854164</td>\n",
       "      <td>6.341281</td>\n",
       "      <td>0.252835</td>\n",
       "      <td>10.681080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.700000</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   425.000000  425.000000     403.000000     291.000000  205.000000   \n",
       "mean      3.896471  122.360000      71.665012      28.862543  134.414634   \n",
       "std       3.269876   30.066982      10.805353       9.793471   70.854164   \n",
       "min       0.000000   44.000000      40.000000       8.000000   18.000000   \n",
       "25%       1.000000  100.000000      64.000000      21.500000   76.000000   \n",
       "50%       3.000000  118.000000      72.000000      28.000000  125.000000   \n",
       "75%       6.000000  142.000000      78.000000      36.000000  180.000000   \n",
       "max      13.000000  199.000000     104.000000      52.000000  328.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  417.000000                411.000000  425.000000  \n",
       "mean    32.166667                  0.430713   32.494118  \n",
       "std      6.341281                  0.252835   10.681080  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     27.700000                  0.235500   24.000000  \n",
       "50%     31.600000                  0.355000   29.000000  \n",
       "75%     36.300000                  0.600500   39.000000  \n",
       "max     49.600000                  1.189000   66.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set before NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-recorder",
   "metadata": {},
   "source": [
    "# 4. Reemplazo de valores inválidos\n",
    "Como se destacó en el análisis estadístico de datos, el dataset suministrado posee varios valores faltantes en algunos individuos. Se asume que en la etapa de producción el modelo contará con todas las variables correctamente informadas, no admitiendo el faltante de alguna de ellas. Luego, se decide reemplazar aquellos valores inválidos en **train**, **valid** y **test** por la correspondiente media en el dataset de train. En este caso, se considera a la media como un estimador correcto para la ocasión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "criminal-arcade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of training\n",
    "train_means = x_train.mean().to_numpy()\n",
    "\n",
    "# Replacing nan values of the train dataset with training mean values\n",
    "for index, column in enumerate(x_train.columns):\n",
    "    x_train.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "\n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_test.columns):\n",
    "    x_test.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "    \n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_valid.columns):\n",
    "    x_valid.loc[:,column].replace(np.nan, train_means[index], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "asian-german",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.896471</td>\n",
       "      <td>122.360000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.430713</td>\n",
       "      <td>32.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.254560</td>\n",
       "      <td>29.926152</td>\n",
       "      <td>10.472012</td>\n",
       "      <td>8.061461</td>\n",
       "      <td>48.916861</td>\n",
       "      <td>6.251752</td>\n",
       "      <td>0.247461</td>\n",
       "      <td>10.631051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   429.000000  429.000000     429.000000     429.000000  429.000000   \n",
       "mean      3.896471  122.360000      71.665012      28.862543  134.414634   \n",
       "std       3.254560   29.926152      10.472012       8.061461   48.916861   \n",
       "min       0.000000   44.000000      40.000000       8.000000   18.000000   \n",
       "25%       1.000000  100.000000      64.000000      25.000000  129.000000   \n",
       "50%       3.000000  119.000000      71.665012      28.862543  134.414634   \n",
       "75%       6.000000  141.000000      78.000000      32.000000  134.414634   \n",
       "max      13.000000  199.000000     104.000000      52.000000  328.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  429.000000                429.000000  429.000000  \n",
       "mean    32.166667                  0.430713   32.494118  \n",
       "std      6.251752                  0.247461   10.631051  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     27.800000                  0.238000   24.000000  \n",
       "50%     32.000000                  0.371000   29.000000  \n",
       "75%     36.000000                  0.591000   39.000000  \n",
       "max     49.600000                  1.189000   66.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set after NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "awful-applicant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.691892</td>\n",
       "      <td>119.850595</td>\n",
       "      <td>73.313406</td>\n",
       "      <td>28.960620</td>\n",
       "      <td>131.659328</td>\n",
       "      <td>32.292252</td>\n",
       "      <td>0.426025</td>\n",
       "      <td>33.678283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.284878</td>\n",
       "      <td>30.084793</td>\n",
       "      <td>12.045479</td>\n",
       "      <td>8.215413</td>\n",
       "      <td>51.838933</td>\n",
       "      <td>6.738731</td>\n",
       "      <td>0.245125</td>\n",
       "      <td>11.670442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.542000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.159000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   185.000000  185.000000     185.000000     185.000000  185.000000   \n",
       "mean      3.691892  119.850595      73.313406      28.960620  131.659328   \n",
       "std       3.284878   30.084793      12.045479       8.215413   51.838933   \n",
       "min       0.000000   57.000000      44.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      65.000000      26.000000  116.000000   \n",
       "50%       3.000000  114.000000      72.000000      28.862543  134.414634   \n",
       "75%       6.000000  136.000000      82.000000      32.000000  134.414634   \n",
       "max      13.000000  198.000000     100.000000      54.000000  335.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  185.000000                185.000000  185.000000  \n",
       "mean    32.292252                  0.426025   33.678283  \n",
       "std      6.738731                  0.245125   11.670442  \n",
       "min     19.600000                  0.096000   21.000000  \n",
       "25%     26.600000                  0.241000   24.000000  \n",
       "50%     32.500000                  0.365000   30.000000  \n",
       "75%     36.500000                  0.542000   42.000000  \n",
       "max     50.000000                  1.159000   66.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set after NaN replacement\n",
    "x_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "separate-anderson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.597403</td>\n",
       "      <td>122.038961</td>\n",
       "      <td>71.787761</td>\n",
       "      <td>28.887267</td>\n",
       "      <td>133.390719</td>\n",
       "      <td>32.197403</td>\n",
       "      <td>0.432119</td>\n",
       "      <td>32.603820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.304818</td>\n",
       "      <td>32.320876</td>\n",
       "      <td>10.448535</td>\n",
       "      <td>8.867500</td>\n",
       "      <td>58.146512</td>\n",
       "      <td>6.484578</td>\n",
       "      <td>0.238998</td>\n",
       "      <td>11.431621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>26.925000</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>142.750000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>40.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>49.300000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   154.000000  154.000000     154.000000     154.000000  154.000000   \n",
       "mean      3.597403  122.038961      71.787761      28.887267  133.390719   \n",
       "std       3.304818   32.320876      10.448535       8.867500   58.146512   \n",
       "min       0.000000   61.000000      44.000000       7.000000   23.000000   \n",
       "25%       1.000000   95.250000      64.000000      23.250000  108.250000   \n",
       "50%       3.000000  117.000000      71.665012      28.862543  134.414634   \n",
       "75%       5.750000  142.750000      79.500000      33.000000  134.414634   \n",
       "max      13.000000  197.000000      94.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  154.000000                154.000000  154.000000  \n",
       "mean    32.197403                  0.432119   32.603820  \n",
       "std      6.484578                  0.238998   11.431621  \n",
       "min     18.400000                  0.078000   21.000000  \n",
       "25%     26.925000                  0.254000   24.000000  \n",
       "50%     32.166667                  0.376500   28.000000  \n",
       "75%     36.625000                  0.567000   40.750000  \n",
       "max     49.300000                  1.191000   66.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set after NaN replacement\n",
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-video",
   "metadata": {},
   "source": [
    "# 5. Normalización de datos de entrada. Z Score. \n",
    "Dado que todas las variables en juego son numéricas, se puede aplicar z-score a todo el dataset. Esta operación se hace con el objetivo de poder obtener mayor información de los pesos calculados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "built-joseph",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! Backup unnormalized subsets for further utilization\n",
    "x_train_un = x_train\n",
    "x_valid_un = x_valid\n",
    "x_test_un = x_test\n",
    "\n",
    "# Apply z-score to all sub-datasets\n",
    "scalable_variables = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])\n",
    "    x_valid.loc[:, scalable_variables] = scaler.transform(x_valid.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "modified-special",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.140692e-17</td>\n",
       "      <td>2.484415e-17</td>\n",
       "      <td>-3.498885e-16</td>\n",
       "      <td>3.685216e-16</td>\n",
       "      <td>-5.010237e-16</td>\n",
       "      <td>3.457478e-16</td>\n",
       "      <td>1.242208e-16</td>\n",
       "      <td>1.780498e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.198632e+00</td>\n",
       "      <td>-2.621503e+00</td>\n",
       "      <td>-3.027306e+00</td>\n",
       "      <td>-2.590957e+00</td>\n",
       "      <td>-2.382625e+00</td>\n",
       "      <td>-2.236649e+00</td>\n",
       "      <td>-1.402716e+00</td>\n",
       "      <td>-1.082446e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.910121e-01</td>\n",
       "      <td>-7.480449e-01</td>\n",
       "      <td>-7.328068e-01</td>\n",
       "      <td>-4.796963e-01</td>\n",
       "      <td>-1.108198e-01</td>\n",
       "      <td>-6.992863e-01</td>\n",
       "      <td>-7.796693e-01</td>\n",
       "      <td>-7.999242e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.757722e-01</td>\n",
       "      <td>-1.124075e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.412180e-16</td>\n",
       "      <td>-5.816991e-16</td>\n",
       "      <td>-2.669032e-02</td>\n",
       "      <td>-2.415838e-01</td>\n",
       "      <td>-3.290547e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.470876e-01</td>\n",
       "      <td>6.235938e-01</td>\n",
       "      <td>6.056510e-01</td>\n",
       "      <td>3.896465e-01</td>\n",
       "      <td>-5.816991e-16</td>\n",
       "      <td>6.138773e-01</td>\n",
       "      <td>6.484824e-01</td>\n",
       "      <td>6.126843e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.800427e+00</td>\n",
       "      <td>2.563961e+00</td>\n",
       "      <td>3.091358e+00</td>\n",
       "      <td>2.873483e+00</td>\n",
       "      <td>3.962057e+00</td>\n",
       "      <td>2.791807e+00</td>\n",
       "      <td>3.067844e+00</td>\n",
       "      <td>3.155380e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pregnancies       Glucose  BloodPressure  SkinThickness       Insulin  \\\n",
       "count  4.290000e+02  4.290000e+02   4.290000e+02   4.290000e+02  4.290000e+02   \n",
       "mean  -4.140692e-17  2.484415e-17  -3.498885e-16   3.685216e-16 -5.010237e-16   \n",
       "std    1.001168e+00  1.001168e+00   1.001168e+00   1.001168e+00  1.001168e+00   \n",
       "min   -1.198632e+00 -2.621503e+00  -3.027306e+00  -2.590957e+00 -2.382625e+00   \n",
       "25%   -8.910121e-01 -7.480449e-01  -7.328068e-01  -4.796963e-01 -1.108198e-01   \n",
       "50%   -2.757722e-01 -1.124075e-01   0.000000e+00   4.412180e-16 -5.816991e-16   \n",
       "75%    6.470876e-01  6.235938e-01   6.056510e-01   3.896465e-01 -5.816991e-16   \n",
       "max    2.800427e+00  2.563961e+00   3.091358e+00   2.873483e+00  3.962057e+00   \n",
       "\n",
       "                BMI  DiabetesPedigreeFunction           Age  \n",
       "count  4.290000e+02              4.290000e+02  4.290000e+02  \n",
       "mean   3.457478e-16              1.242208e-16  1.780498e-16  \n",
       "std    1.001168e+00              1.001168e+00  1.001168e+00  \n",
       "min   -2.236649e+00             -1.402716e+00 -1.082446e+00  \n",
       "25%   -6.992863e-01             -7.796693e-01 -7.999242e-01  \n",
       "50%   -2.669032e-02             -2.415838e-01 -3.290547e-01  \n",
       "75%    6.138773e-01              6.484824e-01  6.126843e-01  \n",
       "max    2.791807e+00              3.067844e+00  3.155380e+00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-settle",
   "metadata": {},
   "source": [
    "# 6. Regresión Logística - Test #1\n",
    "Primera prueba de regresión logística. Se usa SGD y AUC como métrica principal. Se emplea la Binary Cross-Entropy como loss subrogada, dado que **la AUC no es diferenciable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "funky-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TensorBoard for learning logging\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "respective-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "substantial-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import SensitivityAtSpecificity\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "official-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/logistic_regression_first_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "partial-destination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Get model brief\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "representative-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics definition\n",
    "metrics = ['AUC', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adapted-james",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 13s 123ms/step - loss: 0.8785 - auc: 0.5681 - accuracy: 0.5684 - val_loss: 0.8949 - val_auc: 0.5970 - val_accuracy: 0.5784\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.8606 - auc: 0.5893 - accuracy: 0.5628 - val_loss: 0.8752 - val_auc: 0.6055 - val_accuracy: 0.5892\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7887 - auc: 0.6223 - accuracy: 0.6106 - val_loss: 0.8564 - val_auc: 0.6161 - val_accuracy: 0.6000\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7441 - auc: 0.6727 - accuracy: 0.6515 - val_loss: 0.8387 - val_auc: 0.6256 - val_accuracy: 0.6162\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.8126 - auc: 0.6046 - accuracy: 0.5809 - val_loss: 0.8231 - val_auc: 0.6347 - val_accuracy: 0.6216\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7820 - auc: 0.6420 - accuracy: 0.6163 - val_loss: 0.8079 - val_auc: 0.6425 - val_accuracy: 0.6324\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7609 - auc: 0.6499 - accuracy: 0.6158 - val_loss: 0.7939 - val_auc: 0.6488 - val_accuracy: 0.6270\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7216 - auc: 0.6654 - accuracy: 0.6333 - val_loss: 0.7802 - val_auc: 0.6546 - val_accuracy: 0.6486\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7316 - auc: 0.6565 - accuracy: 0.6325 - val_loss: 0.7677 - val_auc: 0.6608 - val_accuracy: 0.6486\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7183 - auc: 0.6645 - accuracy: 0.6695 - val_loss: 0.7559 - val_auc: 0.6659 - val_accuracy: 0.6432\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6705 - auc: 0.6894 - accuracy: 0.6437 - val_loss: 0.7437 - val_auc: 0.6727 - val_accuracy: 0.6486\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7036 - auc: 0.6731 - accuracy: 0.6563 - val_loss: 0.7328 - val_auc: 0.6789 - val_accuracy: 0.6486\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6422 - auc: 0.7241 - accuracy: 0.6953 - val_loss: 0.7224 - val_auc: 0.6850 - val_accuracy: 0.6486\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6050 - auc: 0.7539 - accuracy: 0.7061 - val_loss: 0.7125 - val_auc: 0.6905 - val_accuracy: 0.6595\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6579 - auc: 0.7179 - accuracy: 0.6921 - val_loss: 0.7029 - val_auc: 0.6940 - val_accuracy: 0.6541\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6334 - auc: 0.7406 - accuracy: 0.6970 - val_loss: 0.6935 - val_auc: 0.6995 - val_accuracy: 0.6486\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6188 - auc: 0.7310 - accuracy: 0.6990 - val_loss: 0.6847 - val_auc: 0.7039 - val_accuracy: 0.6486\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6041 - auc: 0.7512 - accuracy: 0.7077 - val_loss: 0.6762 - val_auc: 0.7096 - val_accuracy: 0.6595\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6335 - auc: 0.7269 - accuracy: 0.6843 - val_loss: 0.6684 - val_auc: 0.7151 - val_accuracy: 0.6649\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5846 - auc: 0.7626 - accuracy: 0.7171 - val_loss: 0.6605 - val_auc: 0.7215 - val_accuracy: 0.6649\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6142 - auc: 0.7389 - accuracy: 0.6781 - val_loss: 0.6533 - val_auc: 0.7255 - val_accuracy: 0.6703\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6159 - auc: 0.7369 - accuracy: 0.6993 - val_loss: 0.6462 - val_auc: 0.7310 - val_accuracy: 0.6757\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6097 - auc: 0.7540 - accuracy: 0.6987 - val_loss: 0.6401 - val_auc: 0.7336 - val_accuracy: 0.6757\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6048 - auc: 0.7499 - accuracy: 0.6906 - val_loss: 0.6340 - val_auc: 0.7370 - val_accuracy: 0.6757\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5895 - auc: 0.7700 - accuracy: 0.7160 - val_loss: 0.6282 - val_auc: 0.7394 - val_accuracy: 0.6703\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5818 - auc: 0.7750 - accuracy: 0.7002 - val_loss: 0.6227 - val_auc: 0.7424 - val_accuracy: 0.6811\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5509 - auc: 0.7882 - accuracy: 0.7156 - val_loss: 0.6173 - val_auc: 0.7457 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6074 - auc: 0.7458 - accuracy: 0.6788 - val_loss: 0.6124 - val_auc: 0.7488 - val_accuracy: 0.6757\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5783 - auc: 0.7693 - accuracy: 0.6892 - val_loss: 0.6077 - val_auc: 0.7512 - val_accuracy: 0.6757\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5696 - auc: 0.7681 - accuracy: 0.6812 - val_loss: 0.6031 - val_auc: 0.7531 - val_accuracy: 0.6757\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5604 - auc: 0.7771 - accuracy: 0.6918 - val_loss: 0.5986 - val_auc: 0.7559 - val_accuracy: 0.6757\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5324 - auc: 0.7911 - accuracy: 0.7139 - val_loss: 0.5940 - val_auc: 0.7581 - val_accuracy: 0.6703\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5838 - auc: 0.7694 - accuracy: 0.7027 - val_loss: 0.5900 - val_auc: 0.7608 - val_accuracy: 0.6757\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5874 - auc: 0.7552 - accuracy: 0.6940 - val_loss: 0.5862 - val_auc: 0.7621 - val_accuracy: 0.6865\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5417 - auc: 0.7961 - accuracy: 0.7097 - val_loss: 0.5823 - val_auc: 0.7662 - val_accuracy: 0.6919\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5498 - auc: 0.7832 - accuracy: 0.7029 - val_loss: 0.5786 - val_auc: 0.7679 - val_accuracy: 0.6865\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5615 - auc: 0.7754 - accuracy: 0.6792 - val_loss: 0.5751 - val_auc: 0.7706 - val_accuracy: 0.6865\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5188 - auc: 0.8177 - accuracy: 0.7318 - val_loss: 0.5715 - val_auc: 0.7728 - val_accuracy: 0.6919\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5291 - auc: 0.8025 - accuracy: 0.7212 - val_loss: 0.5685 - val_auc: 0.7742 - val_accuracy: 0.6919\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5090 - auc: 0.8190 - accuracy: 0.7499 - val_loss: 0.5654 - val_auc: 0.7758 - val_accuracy: 0.6973\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4911 - auc: 0.8278 - accuracy: 0.7543 - val_loss: 0.5621 - val_auc: 0.7778 - val_accuracy: 0.6973\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5091 - auc: 0.8213 - accuracy: 0.7416 - val_loss: 0.5591 - val_auc: 0.7799 - val_accuracy: 0.7027\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4785 - auc: 0.8446 - accuracy: 0.7658 - val_loss: 0.5565 - val_auc: 0.7813 - val_accuracy: 0.6973\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5328 - auc: 0.8046 - accuracy: 0.7434 - val_loss: 0.5538 - val_auc: 0.7829 - val_accuracy: 0.6919\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4879 - auc: 0.8367 - accuracy: 0.7713 - val_loss: 0.5511 - val_auc: 0.7842 - val_accuracy: 0.6919\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5378 - auc: 0.7927 - accuracy: 0.7208 - val_loss: 0.5485 - val_auc: 0.7858 - val_accuracy: 0.6973\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5259 - auc: 0.8104 - accuracy: 0.7456 - val_loss: 0.5459 - val_auc: 0.7888 - val_accuracy: 0.6973\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4479 - auc: 0.8657 - accuracy: 0.7982 - val_loss: 0.5432 - val_auc: 0.7899 - val_accuracy: 0.6973\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4898 - auc: 0.8365 - accuracy: 0.7535 - val_loss: 0.5412 - val_auc: 0.7912 - val_accuracy: 0.7081\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5033 - auc: 0.8248 - accuracy: 0.7435 - val_loss: 0.5393 - val_auc: 0.7928 - val_accuracy: 0.7027\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4832 - auc: 0.8437 - accuracy: 0.7683 - val_loss: 0.5374 - val_auc: 0.7933 - val_accuracy: 0.7081\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4572 - auc: 0.8493 - accuracy: 0.7733 - val_loss: 0.5355 - val_auc: 0.7941 - val_accuracy: 0.7081\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4841 - auc: 0.8369 - accuracy: 0.7588 - val_loss: 0.5338 - val_auc: 0.7952 - val_accuracy: 0.7135\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5014 - auc: 0.8228 - accuracy: 0.7465 - val_loss: 0.5320 - val_auc: 0.7976 - val_accuracy: 0.7135\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4474 - auc: 0.8663 - accuracy: 0.7864 - val_loss: 0.5305 - val_auc: 0.7984 - val_accuracy: 0.7135\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4776 - auc: 0.8385 - accuracy: 0.7659 - val_loss: 0.5290 - val_auc: 0.7990 - val_accuracy: 0.7135\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4880 - auc: 0.8365 - accuracy: 0.7610 - val_loss: 0.5274 - val_auc: 0.8010 - val_accuracy: 0.7189\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5033 - auc: 0.8193 - accuracy: 0.7653 - val_loss: 0.5259 - val_auc: 0.8023 - val_accuracy: 0.7189\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5128 - auc: 0.8187 - accuracy: 0.7574 - val_loss: 0.5245 - val_auc: 0.8031 - val_accuracy: 0.7189\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8303 - accuracy: 0.7561 - val_loss: 0.5231 - val_auc: 0.8039 - val_accuracy: 0.7189\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4791 - auc: 0.8462 - accuracy: 0.7789 - val_loss: 0.5217 - val_auc: 0.8050 - val_accuracy: 0.7189\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4777 - auc: 0.8435 - accuracy: 0.7794 - val_loss: 0.5202 - val_auc: 0.8058 - val_accuracy: 0.7189\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4932 - auc: 0.8318 - accuracy: 0.7684 - val_loss: 0.5189 - val_auc: 0.8063 - val_accuracy: 0.7243\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4757 - auc: 0.8444 - accuracy: 0.7742 - val_loss: 0.5178 - val_auc: 0.8067 - val_accuracy: 0.7189\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4787 - auc: 0.8455 - accuracy: 0.7519 - val_loss: 0.5165 - val_auc: 0.8076 - val_accuracy: 0.7189\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4988 - auc: 0.8196 - accuracy: 0.7535 - val_loss: 0.5155 - val_auc: 0.8086 - val_accuracy: 0.7189\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4503 - auc: 0.8609 - accuracy: 0.7930 - val_loss: 0.5143 - val_auc: 0.8102 - val_accuracy: 0.7135\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4981 - auc: 0.8243 - accuracy: 0.7455 - val_loss: 0.5132 - val_auc: 0.8112 - val_accuracy: 0.7189\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5076 - auc: 0.8216 - accuracy: 0.7496 - val_loss: 0.5122 - val_auc: 0.8123 - val_accuracy: 0.7189\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4620 - auc: 0.8576 - accuracy: 0.7772 - val_loss: 0.5113 - val_auc: 0.8119 - val_accuracy: 0.7189\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4336 - auc: 0.8777 - accuracy: 0.7992 - val_loss: 0.5102 - val_auc: 0.8135 - val_accuracy: 0.7189\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4867 - auc: 0.8286 - accuracy: 0.7607 - val_loss: 0.5093 - val_auc: 0.8143 - val_accuracy: 0.7189\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4789 - auc: 0.8470 - accuracy: 0.7748 - val_loss: 0.5087 - val_auc: 0.8151 - val_accuracy: 0.7189\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4653 - auc: 0.8417 - accuracy: 0.7766 - val_loss: 0.5077 - val_auc: 0.8149 - val_accuracy: 0.7189\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4907 - auc: 0.8255 - accuracy: 0.7598 - val_loss: 0.5069 - val_auc: 0.8152 - val_accuracy: 0.7189\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5066 - auc: 0.8227 - accuracy: 0.7394 - val_loss: 0.5061 - val_auc: 0.8160 - val_accuracy: 0.7243\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4554 - auc: 0.8601 - accuracy: 0.7985 - val_loss: 0.5053 - val_auc: 0.8169 - val_accuracy: 0.7243\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8587 - accuracy: 0.7894 - val_loss: 0.5046 - val_auc: 0.8175 - val_accuracy: 0.7297\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4696 - auc: 0.8461 - accuracy: 0.7672 - val_loss: 0.5038 - val_auc: 0.8172 - val_accuracy: 0.7297\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4861 - auc: 0.8395 - accuracy: 0.7606 - val_loss: 0.5032 - val_auc: 0.8174 - val_accuracy: 0.7297\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4304 - auc: 0.8775 - accuracy: 0.8141 - val_loss: 0.5025 - val_auc: 0.8184 - val_accuracy: 0.7297\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4596 - auc: 0.8545 - accuracy: 0.7690 - val_loss: 0.5019 - val_auc: 0.8194 - val_accuracy: 0.7297\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8511 - accuracy: 0.7939 - val_loss: 0.5011 - val_auc: 0.8201 - val_accuracy: 0.7297\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4526 - auc: 0.8626 - accuracy: 0.7596 - val_loss: 0.5003 - val_auc: 0.8204 - val_accuracy: 0.7297\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4707 - auc: 0.8518 - accuracy: 0.7578 - val_loss: 0.4996 - val_auc: 0.8205 - val_accuracy: 0.7351\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4472 - auc: 0.8594 - accuracy: 0.7711 - val_loss: 0.4989 - val_auc: 0.8214 - val_accuracy: 0.7351\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4554 - auc: 0.8526 - accuracy: 0.7732 - val_loss: 0.4983 - val_auc: 0.8214 - val_accuracy: 0.7297\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4665 - auc: 0.8403 - accuracy: 0.7644 - val_loss: 0.4978 - val_auc: 0.8216 - val_accuracy: 0.7297\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4578 - auc: 0.8567 - accuracy: 0.7719 - val_loss: 0.4973 - val_auc: 0.8218 - val_accuracy: 0.7351\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4871 - auc: 0.8301 - accuracy: 0.7423 - val_loss: 0.4969 - val_auc: 0.8224 - val_accuracy: 0.7405\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4834 - auc: 0.8322 - accuracy: 0.7555 - val_loss: 0.4964 - val_auc: 0.8227 - val_accuracy: 0.7405\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8481 - accuracy: 0.7810 - val_loss: 0.4959 - val_auc: 0.8231 - val_accuracy: 0.7405\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4786 - auc: 0.8421 - accuracy: 0.7590 - val_loss: 0.4956 - val_auc: 0.8229 - val_accuracy: 0.7405\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4663 - auc: 0.8464 - accuracy: 0.7661 - val_loss: 0.4952 - val_auc: 0.8231 - val_accuracy: 0.7405\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4678 - auc: 0.8429 - accuracy: 0.7629 - val_loss: 0.4948 - val_auc: 0.8238 - val_accuracy: 0.7405\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5035 - auc: 0.8190 - accuracy: 0.7459 - val_loss: 0.4944 - val_auc: 0.8236 - val_accuracy: 0.7459\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4899 - auc: 0.8219 - accuracy: 0.7523 - val_loss: 0.4940 - val_auc: 0.8240 - val_accuracy: 0.7514\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4773 - auc: 0.8404 - accuracy: 0.7621 - val_loss: 0.4937 - val_auc: 0.8245 - val_accuracy: 0.7514\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4502 - auc: 0.8586 - accuracy: 0.7611 - val_loss: 0.4934 - val_auc: 0.8247 - val_accuracy: 0.7514\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4114 - auc: 0.8882 - accuracy: 0.8184 - val_loss: 0.4930 - val_auc: 0.8253 - val_accuracy: 0.7514\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4817 - auc: 0.8249 - accuracy: 0.7643 - val_loss: 0.4927 - val_auc: 0.8257 - val_accuracy: 0.7514\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4458 - auc: 0.8589 - accuracy: 0.7860 - val_loss: 0.4924 - val_auc: 0.8257 - val_accuracy: 0.7514\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4617 - auc: 0.8509 - accuracy: 0.7881 - val_loss: 0.4920 - val_auc: 0.8264 - val_accuracy: 0.7514\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8421 - accuracy: 0.7743 - val_loss: 0.4915 - val_auc: 0.8272 - val_accuracy: 0.7514\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8532 - accuracy: 0.7747 - val_loss: 0.4912 - val_auc: 0.8277 - val_accuracy: 0.7568\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4491 - auc: 0.8505 - accuracy: 0.7948 - val_loss: 0.4909 - val_auc: 0.8274 - val_accuracy: 0.7568\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4699 - auc: 0.8430 - accuracy: 0.7812 - val_loss: 0.4906 - val_auc: 0.8281 - val_accuracy: 0.7568\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4657 - auc: 0.8535 - accuracy: 0.7623 - val_loss: 0.4903 - val_auc: 0.8285 - val_accuracy: 0.7622\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4788 - auc: 0.8278 - accuracy: 0.7647 - val_loss: 0.4901 - val_auc: 0.8291 - val_accuracy: 0.7622\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4654 - auc: 0.8491 - accuracy: 0.7581 - val_loss: 0.4897 - val_auc: 0.8298 - val_accuracy: 0.7622\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4870 - auc: 0.8269 - accuracy: 0.7480 - val_loss: 0.4896 - val_auc: 0.8298 - val_accuracy: 0.7622\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4706 - auc: 0.8378 - accuracy: 0.7501 - val_loss: 0.4893 - val_auc: 0.8299 - val_accuracy: 0.7676\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4507 - auc: 0.8618 - accuracy: 0.7855 - val_loss: 0.4891 - val_auc: 0.8298 - val_accuracy: 0.7676\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4519 - auc: 0.8641 - accuracy: 0.7807 - val_loss: 0.4887 - val_auc: 0.8299 - val_accuracy: 0.7676\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4724 - auc: 0.8411 - accuracy: 0.7623 - val_loss: 0.4885 - val_auc: 0.8296 - val_accuracy: 0.7676\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4560 - auc: 0.8544 - accuracy: 0.7790 - val_loss: 0.4884 - val_auc: 0.8301 - val_accuracy: 0.7730\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4758 - auc: 0.8394 - accuracy: 0.7738 - val_loss: 0.4882 - val_auc: 0.8301 - val_accuracy: 0.7730\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4540 - auc: 0.8451 - accuracy: 0.7858 - val_loss: 0.4880 - val_auc: 0.8302 - val_accuracy: 0.7730\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4552 - auc: 0.8587 - accuracy: 0.7734 - val_loss: 0.4878 - val_auc: 0.8306 - val_accuracy: 0.7730\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4624 - auc: 0.8503 - accuracy: 0.7709 - val_loss: 0.4876 - val_auc: 0.8306 - val_accuracy: 0.7730\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4819 - auc: 0.8297 - accuracy: 0.7612 - val_loss: 0.4875 - val_auc: 0.8309 - val_accuracy: 0.7730\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4630 - auc: 0.8464 - accuracy: 0.7762 - val_loss: 0.4874 - val_auc: 0.8310 - val_accuracy: 0.7784\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4565 - auc: 0.8521 - accuracy: 0.7721 - val_loss: 0.4871 - val_auc: 0.8311 - val_accuracy: 0.7784\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4916 - auc: 0.8246 - accuracy: 0.7560 - val_loss: 0.4869 - val_auc: 0.8307 - val_accuracy: 0.7784\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4658 - auc: 0.8452 - accuracy: 0.7653 - val_loss: 0.4867 - val_auc: 0.8314 - val_accuracy: 0.7784\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4878 - auc: 0.8270 - accuracy: 0.7549 - val_loss: 0.4863 - val_auc: 0.8317 - val_accuracy: 0.7784\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8437 - accuracy: 0.7576 - val_loss: 0.4863 - val_auc: 0.8316 - val_accuracy: 0.7784\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4599 - auc: 0.8465 - accuracy: 0.7846 - val_loss: 0.4859 - val_auc: 0.8315 - val_accuracy: 0.7784\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4723 - auc: 0.8439 - accuracy: 0.7671 - val_loss: 0.4858 - val_auc: 0.8318 - val_accuracy: 0.7784\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4655 - auc: 0.8505 - accuracy: 0.7800 - val_loss: 0.4856 - val_auc: 0.8320 - val_accuracy: 0.7784\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4365 - auc: 0.8643 - accuracy: 0.7861 - val_loss: 0.4855 - val_auc: 0.8325 - val_accuracy: 0.7784\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4455 - auc: 0.8593 - accuracy: 0.7924 - val_loss: 0.4852 - val_auc: 0.8331 - val_accuracy: 0.7784\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4673 - auc: 0.8457 - accuracy: 0.7683 - val_loss: 0.4851 - val_auc: 0.8329 - val_accuracy: 0.7784\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4860 - auc: 0.8317 - accuracy: 0.7628 - val_loss: 0.4849 - val_auc: 0.8323 - val_accuracy: 0.7784\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8526 - accuracy: 0.7667 - val_loss: 0.4849 - val_auc: 0.8327 - val_accuracy: 0.7784\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4570 - auc: 0.8532 - accuracy: 0.7766 - val_loss: 0.4848 - val_auc: 0.8335 - val_accuracy: 0.7784\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4686 - auc: 0.8497 - accuracy: 0.7752 - val_loss: 0.4846 - val_auc: 0.8337 - val_accuracy: 0.7784\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4407 - auc: 0.8626 - accuracy: 0.7984 - val_loss: 0.4845 - val_auc: 0.8338 - val_accuracy: 0.7784\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4649 - auc: 0.8474 - accuracy: 0.7871 - val_loss: 0.4845 - val_auc: 0.8333 - val_accuracy: 0.7784\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4561 - auc: 0.8536 - accuracy: 0.7918 - val_loss: 0.4842 - val_auc: 0.8332 - val_accuracy: 0.7784\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8388 - accuracy: 0.7796 - val_loss: 0.4841 - val_auc: 0.8332 - val_accuracy: 0.7784\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4643 - auc: 0.8540 - accuracy: 0.7728 - val_loss: 0.4840 - val_auc: 0.8333 - val_accuracy: 0.7730\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4944 - auc: 0.8174 - accuracy: 0.7341 - val_loss: 0.4839 - val_auc: 0.8331 - val_accuracy: 0.7730\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4599 - auc: 0.8486 - accuracy: 0.7784 - val_loss: 0.4840 - val_auc: 0.8331 - val_accuracy: 0.7784\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4843 - auc: 0.8290 - accuracy: 0.7706 - val_loss: 0.4839 - val_auc: 0.8335 - val_accuracy: 0.7784\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5055 - auc: 0.8208 - accuracy: 0.7505 - val_loss: 0.4835 - val_auc: 0.8335 - val_accuracy: 0.7730\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4538 - auc: 0.8564 - accuracy: 0.7939 - val_loss: 0.4834 - val_auc: 0.8336 - val_accuracy: 0.7730\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4508 - auc: 0.8548 - accuracy: 0.7860 - val_loss: 0.4834 - val_auc: 0.8328 - val_accuracy: 0.7730\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4671 - auc: 0.8494 - accuracy: 0.7656 - val_loss: 0.4832 - val_auc: 0.8329 - val_accuracy: 0.7730\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4569 - auc: 0.8555 - accuracy: 0.7749 - val_loss: 0.4832 - val_auc: 0.8334 - val_accuracy: 0.7730\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4512 - auc: 0.8585 - accuracy: 0.7855 - val_loss: 0.4831 - val_auc: 0.8333 - val_accuracy: 0.7730\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4472 - auc: 0.8568 - accuracy: 0.7814 - val_loss: 0.4831 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4957 - auc: 0.8261 - accuracy: 0.7485 - val_loss: 0.4831 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8130 - accuracy: 0.7629 - val_loss: 0.4830 - val_auc: 0.8338 - val_accuracy: 0.7730\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4775 - auc: 0.8386 - accuracy: 0.7631 - val_loss: 0.4829 - val_auc: 0.8336 - val_accuracy: 0.7730\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4576 - auc: 0.8562 - accuracy: 0.7927 - val_loss: 0.4828 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4690 - auc: 0.8465 - accuracy: 0.7805 - val_loss: 0.4826 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4416 - auc: 0.8677 - accuracy: 0.7944 - val_loss: 0.4826 - val_auc: 0.8331 - val_accuracy: 0.7676\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4737 - auc: 0.8426 - accuracy: 0.7663 - val_loss: 0.4824 - val_auc: 0.8335 - val_accuracy: 0.7676\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4508 - auc: 0.8459 - accuracy: 0.7950 - val_loss: 0.4823 - val_auc: 0.8340 - val_accuracy: 0.7676\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4461 - auc: 0.8624 - accuracy: 0.7800 - val_loss: 0.4822 - val_auc: 0.8335 - val_accuracy: 0.7676\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4480 - auc: 0.8622 - accuracy: 0.7770 - val_loss: 0.4821 - val_auc: 0.8339 - val_accuracy: 0.7676\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8487 - accuracy: 0.7823 - val_loss: 0.4821 - val_auc: 0.8339 - val_accuracy: 0.7676\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4770 - auc: 0.8374 - accuracy: 0.7560 - val_loss: 0.4819 - val_auc: 0.8335 - val_accuracy: 0.7676\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4566 - auc: 0.8544 - accuracy: 0.7879 - val_loss: 0.4818 - val_auc: 0.8336 - val_accuracy: 0.7676\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4640 - auc: 0.8492 - accuracy: 0.7735 - val_loss: 0.4819 - val_auc: 0.8341 - val_accuracy: 0.7676\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4567 - auc: 0.8497 - accuracy: 0.7790 - val_loss: 0.4817 - val_auc: 0.8337 - val_accuracy: 0.7676\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4536 - auc: 0.8530 - accuracy: 0.7783 - val_loss: 0.4816 - val_auc: 0.8338 - val_accuracy: 0.7676\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4554 - auc: 0.8454 - accuracy: 0.7848 - val_loss: 0.4815 - val_auc: 0.8339 - val_accuracy: 0.7676\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4416 - auc: 0.8583 - accuracy: 0.7816 - val_loss: 0.4815 - val_auc: 0.8339 - val_accuracy: 0.7676\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4857 - auc: 0.8372 - accuracy: 0.7603 - val_loss: 0.4814 - val_auc: 0.8338 - val_accuracy: 0.7676\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4499 - auc: 0.8516 - accuracy: 0.7871 - val_loss: 0.4813 - val_auc: 0.8340 - val_accuracy: 0.7676\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4454 - auc: 0.8640 - accuracy: 0.7749 - val_loss: 0.4814 - val_auc: 0.8339 - val_accuracy: 0.7676\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5006 - auc: 0.8049 - accuracy: 0.7619 - val_loss: 0.4814 - val_auc: 0.8340 - val_accuracy: 0.7676\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4422 - auc: 0.8544 - accuracy: 0.7776 - val_loss: 0.4814 - val_auc: 0.8344 - val_accuracy: 0.7676\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4661 - auc: 0.8445 - accuracy: 0.7811 - val_loss: 0.4813 - val_auc: 0.8342 - val_accuracy: 0.7676\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4240 - auc: 0.8746 - accuracy: 0.8054 - val_loss: 0.4812 - val_auc: 0.8346 - val_accuracy: 0.7676\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4506 - auc: 0.8400 - accuracy: 0.7802 - val_loss: 0.4811 - val_auc: 0.8350 - val_accuracy: 0.7676\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4420 - auc: 0.8654 - accuracy: 0.7724 - val_loss: 0.4810 - val_auc: 0.8349 - val_accuracy: 0.7676\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4620 - auc: 0.8567 - accuracy: 0.7749 - val_loss: 0.4810 - val_auc: 0.8348 - val_accuracy: 0.7676\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4378 - auc: 0.8619 - accuracy: 0.7890 - val_loss: 0.4810 - val_auc: 0.8350 - val_accuracy: 0.7676\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4423 - auc: 0.8640 - accuracy: 0.8001 - val_loss: 0.4810 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4566 - auc: 0.8508 - accuracy: 0.7836 - val_loss: 0.4811 - val_auc: 0.8357 - val_accuracy: 0.7676\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4786 - auc: 0.8516 - accuracy: 0.7538 - val_loss: 0.4810 - val_auc: 0.8358 - val_accuracy: 0.7676\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4426 - auc: 0.8598 - accuracy: 0.7841 - val_loss: 0.4809 - val_auc: 0.8353 - val_accuracy: 0.7676\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4293 - auc: 0.8730 - accuracy: 0.7936 - val_loss: 0.4808 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4653 - auc: 0.8522 - accuracy: 0.7666 - val_loss: 0.4808 - val_auc: 0.8357 - val_accuracy: 0.7676\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4326 - auc: 0.8728 - accuracy: 0.7898 - val_loss: 0.4807 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4901 - auc: 0.8349 - accuracy: 0.7632 - val_loss: 0.4807 - val_auc: 0.8356 - val_accuracy: 0.7676\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8459 - accuracy: 0.7804 - val_loss: 0.4806 - val_auc: 0.8356 - val_accuracy: 0.7676\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4500 - auc: 0.8645 - accuracy: 0.7954 - val_loss: 0.4807 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4651 - auc: 0.8449 - accuracy: 0.7683 - val_loss: 0.4807 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4779 - auc: 0.8353 - accuracy: 0.7632 - val_loss: 0.4807 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4818 - auc: 0.8434 - accuracy: 0.7518 - val_loss: 0.4805 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4835 - auc: 0.8292 - accuracy: 0.7574 - val_loss: 0.4806 - val_auc: 0.8350 - val_accuracy: 0.7676\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 0.4469 - auc: 0.8634 - accuracy: 0.8086 - val_loss: 0.4805 - val_auc: 0.8351 - val_accuracy: 0.7676\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4686 - auc: 0.8506 - accuracy: 0.7759 - val_loss: 0.4806 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4362 - auc: 0.8697 - accuracy: 0.7988 - val_loss: 0.4805 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4734 - auc: 0.8428 - accuracy: 0.7656 - val_loss: 0.4804 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4747 - auc: 0.8451 - accuracy: 0.7723 - val_loss: 0.4804 - val_auc: 0.8354 - val_accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137ce46d7c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=1, callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bulgarian-peter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-26a83ecf15064299\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-26a83ecf15064299\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard launch\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "remarkable-selection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5033 - auc: 0.8018 - accuracy: 0.7143\n"
     ]
    }
   ],
   "source": [
    "model = load_model(mc_path)\n",
    "eval = model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-chick",
   "metadata": {},
   "source": [
    "# 7. Elección del umbral usando f2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-forwarding",
   "metadata": {},
   "source": [
    "A la prueba anterior, se suma la selección del umbral (o **threshold**) con el cual el clasificador discrimina entre clases. El mejor umbral de clasificación se calcula para todos los modelos, después del correspondiente entrenamiento. Para esta elección se elije el mejor valor del f2-score sobre el subset de **valid**. También se muestra la evolución de esta métrica respecto al umbral en el subset de **train**. En teoría, este umbral **no modifica la mérica principal del modelo, que es el área bajo la curva ROC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cognitive-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def round_threshold(vector, threshold=0.5):\n",
    "    rounded_vector = []\n",
    "    for element in vector:\n",
    "        if element >= threshold:\n",
    "            rounded_vector.append(1)\n",
    "        else:\n",
    "            rounded_vector.append(0)\n",
    "            \n",
    "    return np.array(rounded_vector)\n",
    "        \n",
    "def f2_threshold_selection(y_probs_valid, y_true_valid, y_probs_train, y_true_train, steps=100, plot=True):\n",
    "    # Thresholds and f2-score vectors\n",
    "    thresholds = np.linspace(0, 1, steps)\n",
    "    f2_score_valid = []\n",
    "    f2_score_train = []\n",
    "    \n",
    "    for thld in thresholds:\n",
    "        # Generate predictions with current threshold\n",
    "        y_pred_valid = round_threshold(vector=y_probs_valid, threshold=thld)\n",
    "        y_pred_train = round_threshold(vector=y_probs_train, threshold=thld)\n",
    "        # Compute f2 score for that threshold and append\n",
    "        score_valid = fbeta_score(y_true=y_true_valid, y_pred=y_pred_valid, beta=2)\n",
    "        score_train = fbeta_score(y_true=y_true_train, y_pred=y_pred_train, beta=2)\n",
    "        f2_score_valid.append(score_valid)\n",
    "        f2_score_train.append(score_train)\n",
    "    \n",
    "    idx = np.argmax(f2_score_valid)\n",
    "    if plot == True:\n",
    "        plt.plot(thresholds, f2_score_valid, label='valid')\n",
    "        plt.plot(thresholds, f2_score_train, label='train')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('F2 score')\n",
    "        plt.axvline(thresholds[idx], color='black', linestyle='--')\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "        plt.grid(b=True)\n",
    "        plt.title('Selecting best threshold')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return thresholds, f2_score_valid, idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "czech-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f2-score for valid is 0.8060 @ threshold = 0.1919\n"
     ]
    }
   ],
   "source": [
    "# Get binary-class probability from model\n",
    "y_probs_valid = model.predict(x_valid)\n",
    "y_probs_train = model.predict(x_train)\n",
    "thresholds, f2_score, idx = f2_threshold_selection(y_probs_valid, y_valid, y_probs_train, y_train, steps=100)\n",
    "print(f'Best f2-score for valid is {f2_score[idx]:.4f} @ threshold = {thresholds[idx]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-florist",
   "metadata": {},
   "source": [
    "# 8. Early Stopping\n",
    "Habiendo concluido el test #1, se cree necesario agregar un callback de early stopping al modelo. Este callback deberá detener el proceso de aprendizaje en el momento en el que la **métrica principal** del modelo **deje de aumentar**. Posteriormente, se recupera el modelo con mejor performance en cuanto a esta métrica (AUC). Cabe aclarar que esta técnica es especialmente útil cuando la métrica principal no es diferenciable, y por ende se debe emplear una **loss subrogada** (en este caso, la binary cross entropy). De esta forma, el número de epochs que recorra el proceso de entrenamiento se verá limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "antique-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Early Stopping callback from keras.\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "distinguished-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "14/14 [==============================] - 3s 170ms/step - loss: 0.9652 - auc: 0.3580 - accuracy: 0.4266 - val_loss: 1.0442 - val_auc: 0.3422 - val_accuracy: 0.4054\n",
      "Epoch 2/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.9799 - auc: 0.3580 - accuracy: 0.4377 - val_loss: 1.0068 - val_auc: 0.3606 - val_accuracy: 0.4162\n",
      "Epoch 3/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.8949 - auc: 0.4075 - accuracy: 0.4751 - val_loss: 0.9726 - val_auc: 0.3798 - val_accuracy: 0.4486\n",
      "Epoch 4/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.9193 - auc: 0.3522 - accuracy: 0.4210 - val_loss: 0.9418 - val_auc: 0.4012 - val_accuracy: 0.4541\n",
      "Epoch 5/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8361 - auc: 0.4585 - accuracy: 0.4978 - val_loss: 0.9134 - val_auc: 0.4212 - val_accuracy: 0.4649\n",
      "Epoch 6/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8435 - auc: 0.4304 - accuracy: 0.4716 - val_loss: 0.8878 - val_auc: 0.4424 - val_accuracy: 0.4865\n",
      "Epoch 7/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7916 - auc: 0.4810 - accuracy: 0.5137 - val_loss: 0.8637 - val_auc: 0.4624 - val_accuracy: 0.4973\n",
      "Epoch 8/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7459 - auc: 0.5256 - accuracy: 0.5773 - val_loss: 0.8425 - val_auc: 0.4785 - val_accuracy: 0.5189\n",
      "Epoch 9/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7221 - auc: 0.5701 - accuracy: 0.5676 - val_loss: 0.8228 - val_auc: 0.4966 - val_accuracy: 0.5405\n",
      "Epoch 10/1000\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 0.7374 - auc: 0.5451 - accuracy: 0.5794 - val_loss: 0.8053 - val_auc: 0.5136 - val_accuracy: 0.5514\n",
      "Epoch 11/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7083 - auc: 0.5714 - accuracy: 0.5767 - val_loss: 0.7885 - val_auc: 0.5286 - val_accuracy: 0.5405\n",
      "Epoch 12/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6818 - auc: 0.6278 - accuracy: 0.6183 - val_loss: 0.7738 - val_auc: 0.5422 - val_accuracy: 0.5514\n",
      "Epoch 13/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6973 - auc: 0.6011 - accuracy: 0.5778 - val_loss: 0.7593 - val_auc: 0.5586 - val_accuracy: 0.5676\n",
      "Epoch 14/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6560 - auc: 0.6495 - accuracy: 0.5968 - val_loss: 0.7464 - val_auc: 0.5726 - val_accuracy: 0.5622\n",
      "Epoch 15/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6623 - auc: 0.6372 - accuracy: 0.6017 - val_loss: 0.7341 - val_auc: 0.5839 - val_accuracy: 0.5730\n",
      "Epoch 16/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6712 - auc: 0.6265 - accuracy: 0.6116 - val_loss: 0.7226 - val_auc: 0.5951 - val_accuracy: 0.5730\n",
      "Epoch 17/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6608 - auc: 0.6469 - accuracy: 0.6222 - val_loss: 0.7123 - val_auc: 0.6053 - val_accuracy: 0.5730\n",
      "Epoch 18/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6404 - auc: 0.6724 - accuracy: 0.6144 - val_loss: 0.7023 - val_auc: 0.6158 - val_accuracy: 0.5730\n",
      "Epoch 19/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5987 - auc: 0.7369 - accuracy: 0.6971 - val_loss: 0.6927 - val_auc: 0.6269 - val_accuracy: 0.5730\n",
      "Epoch 20/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5971 - auc: 0.7356 - accuracy: 0.6593 - val_loss: 0.6843 - val_auc: 0.6349 - val_accuracy: 0.5730\n",
      "Epoch 21/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5952 - auc: 0.7279 - accuracy: 0.6707 - val_loss: 0.6760 - val_auc: 0.6437 - val_accuracy: 0.5784\n",
      "Epoch 22/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5813 - auc: 0.7476 - accuracy: 0.6891 - val_loss: 0.6684 - val_auc: 0.6545 - val_accuracy: 0.5730\n",
      "Epoch 23/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5970 - auc: 0.7370 - accuracy: 0.6768 - val_loss: 0.6609 - val_auc: 0.6623 - val_accuracy: 0.5784\n",
      "Epoch 24/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5645 - auc: 0.7707 - accuracy: 0.7066 - val_loss: 0.6540 - val_auc: 0.6691 - val_accuracy: 0.5730\n",
      "Epoch 25/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5836 - auc: 0.7374 - accuracy: 0.6601 - val_loss: 0.6481 - val_auc: 0.6753 - val_accuracy: 0.5838\n",
      "Epoch 26/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6040 - auc: 0.7269 - accuracy: 0.6511 - val_loss: 0.6421 - val_auc: 0.6806 - val_accuracy: 0.5946\n",
      "Epoch 27/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5866 - auc: 0.7433 - accuracy: 0.6808 - val_loss: 0.6363 - val_auc: 0.6882 - val_accuracy: 0.6108\n",
      "Epoch 28/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5571 - auc: 0.7746 - accuracy: 0.7103 - val_loss: 0.6308 - val_auc: 0.6939 - val_accuracy: 0.6162\n",
      "Epoch 29/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5387 - auc: 0.7960 - accuracy: 0.7187 - val_loss: 0.6256 - val_auc: 0.6970 - val_accuracy: 0.6162\n",
      "Epoch 30/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5428 - auc: 0.7844 - accuracy: 0.7232 - val_loss: 0.6211 - val_auc: 0.7008 - val_accuracy: 0.6216\n",
      "Epoch 31/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5384 - auc: 0.7934 - accuracy: 0.7454 - val_loss: 0.6161 - val_auc: 0.7045 - val_accuracy: 0.6216\n",
      "Epoch 32/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5592 - auc: 0.7567 - accuracy: 0.6961 - val_loss: 0.6119 - val_auc: 0.7077 - val_accuracy: 0.6324\n",
      "Epoch 33/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5546 - auc: 0.7810 - accuracy: 0.7160 - val_loss: 0.6079 - val_auc: 0.7130 - val_accuracy: 0.6378\n",
      "Epoch 34/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5440 - auc: 0.7875 - accuracy: 0.7281 - val_loss: 0.6038 - val_auc: 0.7181 - val_accuracy: 0.6486\n",
      "Epoch 35/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5196 - auc: 0.8187 - accuracy: 0.7396 - val_loss: 0.5995 - val_auc: 0.7229 - val_accuracy: 0.6486\n",
      "Epoch 36/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5263 - auc: 0.8159 - accuracy: 0.7420 - val_loss: 0.5956 - val_auc: 0.7268 - val_accuracy: 0.6541\n",
      "Epoch 37/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5335 - auc: 0.8005 - accuracy: 0.7264 - val_loss: 0.5922 - val_auc: 0.7297 - val_accuracy: 0.6486\n",
      "Epoch 38/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5451 - auc: 0.7823 - accuracy: 0.6870 - val_loss: 0.5890 - val_auc: 0.7337 - val_accuracy: 0.6486\n",
      "Epoch 39/1000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5015 - auc: 0.7604 - accuracy: 0.71 - 0s 3ms/step - loss: 0.5211 - auc: 0.7999 - accuracy: 0.7309 - val_loss: 0.5859 - val_auc: 0.7374 - val_accuracy: 0.6486\n",
      "Epoch 40/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5087 - auc: 0.8204 - accuracy: 0.7514 - val_loss: 0.5830 - val_auc: 0.7402 - val_accuracy: 0.6649\n",
      "Epoch 41/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4979 - auc: 0.8289 - accuracy: 0.7683 - val_loss: 0.5799 - val_auc: 0.7428 - val_accuracy: 0.6649\n",
      "Epoch 42/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5254 - auc: 0.8075 - accuracy: 0.7442 - val_loss: 0.5770 - val_auc: 0.7455 - val_accuracy: 0.6649\n",
      "Epoch 43/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5127 - auc: 0.8292 - accuracy: 0.7408 - val_loss: 0.5744 - val_auc: 0.7482 - val_accuracy: 0.6703\n",
      "Epoch 44/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5233 - auc: 0.8087 - accuracy: 0.7249 - val_loss: 0.5716 - val_auc: 0.7510 - val_accuracy: 0.6811\n",
      "Epoch 45/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5302 - auc: 0.8004 - accuracy: 0.7085 - val_loss: 0.5690 - val_auc: 0.7541 - val_accuracy: 0.6757\n",
      "Epoch 46/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5246 - auc: 0.8073 - accuracy: 0.7231 - val_loss: 0.5665 - val_auc: 0.7574 - val_accuracy: 0.6865\n",
      "Epoch 47/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5250 - auc: 0.7961 - accuracy: 0.7305 - val_loss: 0.5642 - val_auc: 0.7590 - val_accuracy: 0.6919\n",
      "Epoch 48/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5031 - auc: 0.8195 - accuracy: 0.7644 - val_loss: 0.5618 - val_auc: 0.7608 - val_accuracy: 0.6919\n",
      "Epoch 49/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5245 - auc: 0.8005 - accuracy: 0.7322 - val_loss: 0.5594 - val_auc: 0.7627 - val_accuracy: 0.6973\n",
      "Epoch 50/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5301 - auc: 0.8010 - accuracy: 0.7134 - val_loss: 0.5575 - val_auc: 0.7650 - val_accuracy: 0.6973\n",
      "Epoch 51/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8536 - accuracy: 0.7767 - val_loss: 0.5557 - val_auc: 0.7667 - val_accuracy: 0.7027\n",
      "Epoch 52/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4991 - auc: 0.8320 - accuracy: 0.7552 - val_loss: 0.5537 - val_auc: 0.7680 - val_accuracy: 0.7027\n",
      "Epoch 53/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5054 - auc: 0.8267 - accuracy: 0.7499 - val_loss: 0.5520 - val_auc: 0.7693 - val_accuracy: 0.7081\n",
      "Epoch 54/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4984 - auc: 0.8294 - accuracy: 0.7361 - val_loss: 0.5503 - val_auc: 0.7719 - val_accuracy: 0.7081\n",
      "Epoch 55/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4855 - auc: 0.8349 - accuracy: 0.7572 - val_loss: 0.5485 - val_auc: 0.7736 - val_accuracy: 0.7135\n",
      "Epoch 56/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4837 - auc: 0.8363 - accuracy: 0.7592 - val_loss: 0.5467 - val_auc: 0.7755 - val_accuracy: 0.7189\n",
      "Epoch 57/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4909 - auc: 0.8429 - accuracy: 0.7492 - val_loss: 0.5449 - val_auc: 0.7773 - val_accuracy: 0.7243\n",
      "Epoch 58/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4790 - auc: 0.8416 - accuracy: 0.7676 - val_loss: 0.5432 - val_auc: 0.7781 - val_accuracy: 0.7243\n",
      "Epoch 59/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4979 - auc: 0.8314 - accuracy: 0.7388 - val_loss: 0.5417 - val_auc: 0.7791 - val_accuracy: 0.7243\n",
      "Epoch 60/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4785 - auc: 0.8423 - accuracy: 0.7690 - val_loss: 0.5404 - val_auc: 0.7806 - val_accuracy: 0.7243\n",
      "Epoch 61/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4862 - auc: 0.8387 - accuracy: 0.7674 - val_loss: 0.5391 - val_auc: 0.7813 - val_accuracy: 0.7243\n",
      "Epoch 62/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4922 - auc: 0.8301 - accuracy: 0.7546 - val_loss: 0.5376 - val_auc: 0.7827 - val_accuracy: 0.7243\n",
      "Epoch 63/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4946 - auc: 0.8229 - accuracy: 0.7551 - val_loss: 0.5365 - val_auc: 0.7832 - val_accuracy: 0.7243\n",
      "Epoch 64/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4856 - auc: 0.8425 - accuracy: 0.7483 - val_loss: 0.5351 - val_auc: 0.7842 - val_accuracy: 0.7243\n",
      "Epoch 65/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5011 - auc: 0.8191 - accuracy: 0.7564 - val_loss: 0.5339 - val_auc: 0.7850 - val_accuracy: 0.7243\n",
      "Epoch 66/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8556 - accuracy: 0.7729 - val_loss: 0.5322 - val_auc: 0.7866 - val_accuracy: 0.7297\n",
      "Epoch 67/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4940 - auc: 0.8305 - accuracy: 0.7590 - val_loss: 0.5312 - val_auc: 0.7871 - val_accuracy: 0.7351\n",
      "Epoch 68/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4878 - auc: 0.8389 - accuracy: 0.7586 - val_loss: 0.5301 - val_auc: 0.7894 - val_accuracy: 0.7297\n",
      "Epoch 69/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4839 - auc: 0.8372 - accuracy: 0.7586 - val_loss: 0.5289 - val_auc: 0.7908 - val_accuracy: 0.7297\n",
      "Epoch 70/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4695 - auc: 0.8443 - accuracy: 0.7751 - val_loss: 0.5278 - val_auc: 0.7923 - val_accuracy: 0.7297\n",
      "Epoch 71/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8433 - accuracy: 0.7631 - val_loss: 0.5266 - val_auc: 0.7923 - val_accuracy: 0.7351\n",
      "Epoch 72/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4774 - auc: 0.8451 - accuracy: 0.7781 - val_loss: 0.5255 - val_auc: 0.7930 - val_accuracy: 0.7351\n",
      "Epoch 73/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4790 - auc: 0.8438 - accuracy: 0.7639 - val_loss: 0.5245 - val_auc: 0.7944 - val_accuracy: 0.7351\n",
      "Epoch 74/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4976 - auc: 0.8232 - accuracy: 0.7379 - val_loss: 0.5236 - val_auc: 0.7951 - val_accuracy: 0.7351\n",
      "Epoch 75/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4899 - auc: 0.8275 - accuracy: 0.7742 - val_loss: 0.5227 - val_auc: 0.7957 - val_accuracy: 0.7405\n",
      "Epoch 76/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4740 - auc: 0.8437 - accuracy: 0.7545 - val_loss: 0.5217 - val_auc: 0.7964 - val_accuracy: 0.7405\n",
      "Epoch 77/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4907 - auc: 0.8239 - accuracy: 0.7656 - val_loss: 0.5210 - val_auc: 0.7967 - val_accuracy: 0.7405\n",
      "Epoch 78/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4654 - auc: 0.8560 - accuracy: 0.7715 - val_loss: 0.5201 - val_auc: 0.7973 - val_accuracy: 0.7405\n",
      "Epoch 79/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5033 - auc: 0.8215 - accuracy: 0.7373 - val_loss: 0.5191 - val_auc: 0.7977 - val_accuracy: 0.7351\n",
      "Epoch 80/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4839 - auc: 0.8374 - accuracy: 0.7664 - val_loss: 0.5184 - val_auc: 0.7979 - val_accuracy: 0.7351\n",
      "Epoch 81/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4829 - auc: 0.8406 - accuracy: 0.7626 - val_loss: 0.5175 - val_auc: 0.7995 - val_accuracy: 0.7297\n",
      "Epoch 82/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4802 - auc: 0.8428 - accuracy: 0.7548 - val_loss: 0.5167 - val_auc: 0.8002 - val_accuracy: 0.7297\n",
      "Epoch 83/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4575 - auc: 0.8553 - accuracy: 0.7727 - val_loss: 0.5160 - val_auc: 0.8011 - val_accuracy: 0.7351\n",
      "Epoch 84/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4807 - auc: 0.8371 - accuracy: 0.7568 - val_loss: 0.5151 - val_auc: 0.8016 - val_accuracy: 0.7351\n",
      "Epoch 85/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8335 - accuracy: 0.7465 - val_loss: 0.5144 - val_auc: 0.8026 - val_accuracy: 0.7351\n",
      "Epoch 86/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4897 - auc: 0.8353 - accuracy: 0.7601 - val_loss: 0.5136 - val_auc: 0.8035 - val_accuracy: 0.7351\n",
      "Epoch 87/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4783 - auc: 0.8365 - accuracy: 0.7602 - val_loss: 0.5129 - val_auc: 0.8036 - val_accuracy: 0.7351\n",
      "Epoch 88/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8591 - accuracy: 0.7659 - val_loss: 0.5121 - val_auc: 0.8053 - val_accuracy: 0.7351\n",
      "Epoch 89/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5026 - auc: 0.8235 - accuracy: 0.7213 - val_loss: 0.5114 - val_auc: 0.8066 - val_accuracy: 0.7351\n",
      "Epoch 90/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4888 - auc: 0.8371 - accuracy: 0.7620 - val_loss: 0.5108 - val_auc: 0.8071 - val_accuracy: 0.7351\n",
      "Epoch 91/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4622 - auc: 0.8467 - accuracy: 0.7647 - val_loss: 0.5103 - val_auc: 0.8069 - val_accuracy: 0.7351\n",
      "Epoch 92/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8219 - accuracy: 0.7506 - val_loss: 0.5098 - val_auc: 0.8076 - val_accuracy: 0.7351\n",
      "Epoch 93/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4336 - auc: 0.8777 - accuracy: 0.7971 - val_loss: 0.5092 - val_auc: 0.8084 - val_accuracy: 0.7351\n",
      "Epoch 94/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4895 - auc: 0.8265 - accuracy: 0.7445 - val_loss: 0.5087 - val_auc: 0.8088 - val_accuracy: 0.7351\n",
      "Epoch 95/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4639 - auc: 0.8571 - accuracy: 0.7823 - val_loss: 0.5082 - val_auc: 0.8093 - val_accuracy: 0.7405\n",
      "Epoch 96/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8433 - accuracy: 0.7645 - val_loss: 0.5077 - val_auc: 0.8095 - val_accuracy: 0.7405\n",
      "Epoch 97/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8456 - accuracy: 0.7656 - val_loss: 0.5072 - val_auc: 0.8095 - val_accuracy: 0.7405\n",
      "Epoch 98/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8443 - accuracy: 0.7696 - val_loss: 0.5068 - val_auc: 0.8100 - val_accuracy: 0.7405\n",
      "Epoch 99/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4811 - auc: 0.8365 - accuracy: 0.7728 - val_loss: 0.5064 - val_auc: 0.8105 - val_accuracy: 0.7405\n",
      "Epoch 100/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4728 - auc: 0.8455 - accuracy: 0.7724 - val_loss: 0.5056 - val_auc: 0.8114 - val_accuracy: 0.7405\n",
      "Epoch 101/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4617 - auc: 0.8524 - accuracy: 0.7655 - val_loss: 0.5052 - val_auc: 0.8116 - val_accuracy: 0.7405\n",
      "Epoch 102/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4767 - auc: 0.8454 - accuracy: 0.7455 - val_loss: 0.5046 - val_auc: 0.8120 - val_accuracy: 0.7405\n",
      "Epoch 103/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4487 - auc: 0.8674 - accuracy: 0.7896 - val_loss: 0.5041 - val_auc: 0.8124 - val_accuracy: 0.7405\n",
      "Epoch 104/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8326 - accuracy: 0.7501 - val_loss: 0.5038 - val_auc: 0.8128 - val_accuracy: 0.7405\n",
      "Epoch 105/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4420 - auc: 0.8671 - accuracy: 0.7874 - val_loss: 0.5034 - val_auc: 0.8129 - val_accuracy: 0.7405\n",
      "Epoch 106/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4459 - auc: 0.8681 - accuracy: 0.7811 - val_loss: 0.5031 - val_auc: 0.8133 - val_accuracy: 0.7405\n",
      "Epoch 107/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8522 - accuracy: 0.7689 - val_loss: 0.5025 - val_auc: 0.8141 - val_accuracy: 0.7405\n",
      "Epoch 108/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4773 - auc: 0.8419 - accuracy: 0.7821 - val_loss: 0.5021 - val_auc: 0.8144 - val_accuracy: 0.7405\n",
      "Epoch 109/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4601 - auc: 0.8540 - accuracy: 0.7638 - val_loss: 0.5016 - val_auc: 0.8152 - val_accuracy: 0.7405\n",
      "Epoch 110/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4507 - auc: 0.8608 - accuracy: 0.7754 - val_loss: 0.5012 - val_auc: 0.8162 - val_accuracy: 0.7405\n",
      "Epoch 111/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4819 - auc: 0.8300 - accuracy: 0.7392 - val_loss: 0.5009 - val_auc: 0.8163 - val_accuracy: 0.7405\n",
      "Epoch 112/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4372 - auc: 0.8649 - accuracy: 0.7995 - val_loss: 0.5004 - val_auc: 0.8167 - val_accuracy: 0.7405\n",
      "Epoch 113/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4558 - auc: 0.8622 - accuracy: 0.7716 - val_loss: 0.5000 - val_auc: 0.8169 - val_accuracy: 0.7405\n",
      "Epoch 114/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4470 - auc: 0.8686 - accuracy: 0.7857 - val_loss: 0.4996 - val_auc: 0.8171 - val_accuracy: 0.7405\n",
      "Epoch 115/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4508 - auc: 0.8580 - accuracy: 0.7702 - val_loss: 0.4993 - val_auc: 0.8179 - val_accuracy: 0.7405\n",
      "Epoch 116/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4849 - auc: 0.8237 - accuracy: 0.7588 - val_loss: 0.4990 - val_auc: 0.8183 - val_accuracy: 0.7405\n",
      "Epoch 117/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4856 - auc: 0.8345 - accuracy: 0.7642 - val_loss: 0.4987 - val_auc: 0.8190 - val_accuracy: 0.7405\n",
      "Epoch 118/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5027 - auc: 0.8168 - accuracy: 0.7420 - val_loss: 0.4984 - val_auc: 0.8192 - val_accuracy: 0.7405\n",
      "Epoch 119/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4804 - auc: 0.8328 - accuracy: 0.7519 - val_loss: 0.4982 - val_auc: 0.8191 - val_accuracy: 0.7405\n",
      "Epoch 120/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8432 - accuracy: 0.7653 - val_loss: 0.4978 - val_auc: 0.8195 - val_accuracy: 0.7405\n",
      "Epoch 121/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4414 - auc: 0.8672 - accuracy: 0.7857 - val_loss: 0.4977 - val_auc: 0.8193 - val_accuracy: 0.7405\n",
      "Epoch 122/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4576 - auc: 0.8481 - accuracy: 0.7757 - val_loss: 0.4974 - val_auc: 0.8193 - val_accuracy: 0.7405\n",
      "Epoch 123/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4460 - auc: 0.8644 - accuracy: 0.8080 - val_loss: 0.4971 - val_auc: 0.8195 - val_accuracy: 0.7405\n",
      "Epoch 124/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8435 - accuracy: 0.7648 - val_loss: 0.4966 - val_auc: 0.8199 - val_accuracy: 0.7405\n",
      "Epoch 125/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4331 - auc: 0.8772 - accuracy: 0.7952 - val_loss: 0.4964 - val_auc: 0.8201 - val_accuracy: 0.7405\n",
      "Epoch 126/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4653 - auc: 0.8417 - accuracy: 0.7734 - val_loss: 0.4961 - val_auc: 0.8206 - val_accuracy: 0.7405\n",
      "Epoch 127/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4805 - auc: 0.8359 - accuracy: 0.7402 - val_loss: 0.4959 - val_auc: 0.8208 - val_accuracy: 0.7405\n",
      "Epoch 128/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4463 - auc: 0.8604 - accuracy: 0.7829 - val_loss: 0.4957 - val_auc: 0.8210 - val_accuracy: 0.7405\n",
      "Epoch 129/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4650 - auc: 0.8515 - accuracy: 0.7716 - val_loss: 0.4954 - val_auc: 0.8206 - val_accuracy: 0.7405\n",
      "Epoch 130/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4730 - auc: 0.8454 - accuracy: 0.7430 - val_loss: 0.4951 - val_auc: 0.8213 - val_accuracy: 0.7405\n",
      "Epoch 131/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4861 - auc: 0.8302 - accuracy: 0.7459 - val_loss: 0.4949 - val_auc: 0.8219 - val_accuracy: 0.7405\n",
      "Epoch 132/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4572 - auc: 0.8604 - accuracy: 0.7768 - val_loss: 0.4948 - val_auc: 0.8219 - val_accuracy: 0.7405\n",
      "Epoch 133/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8291 - accuracy: 0.7578 - val_loss: 0.4946 - val_auc: 0.8219 - val_accuracy: 0.7405\n",
      "Epoch 134/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4493 - auc: 0.8602 - accuracy: 0.7655 - val_loss: 0.4945 - val_auc: 0.8220 - val_accuracy: 0.7405\n",
      "Epoch 135/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4482 - auc: 0.8641 - accuracy: 0.7725 - val_loss: 0.4942 - val_auc: 0.8218 - val_accuracy: 0.7405\n",
      "Epoch 136/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8585 - accuracy: 0.7725 - val_loss: 0.4938 - val_auc: 0.8219 - val_accuracy: 0.7405\n",
      "Epoch 137/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4766 - auc: 0.8434 - accuracy: 0.7577 - val_loss: 0.4937 - val_auc: 0.8223 - val_accuracy: 0.7405\n",
      "Epoch 138/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4393 - auc: 0.8686 - accuracy: 0.7702 - val_loss: 0.4934 - val_auc: 0.8225 - val_accuracy: 0.7405\n",
      "Epoch 139/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4955 - auc: 0.8262 - accuracy: 0.7402 - val_loss: 0.4932 - val_auc: 0.8227 - val_accuracy: 0.7405\n",
      "Epoch 140/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4856 - auc: 0.8379 - accuracy: 0.7426 - val_loss: 0.4929 - val_auc: 0.8231 - val_accuracy: 0.7405\n",
      "Epoch 141/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4471 - auc: 0.8597 - accuracy: 0.7870 - val_loss: 0.4926 - val_auc: 0.8237 - val_accuracy: 0.7405\n",
      "Epoch 142/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4804 - auc: 0.8330 - accuracy: 0.7426 - val_loss: 0.4925 - val_auc: 0.8234 - val_accuracy: 0.7405\n",
      "Epoch 143/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4694 - auc: 0.8528 - accuracy: 0.7594 - val_loss: 0.4923 - val_auc: 0.8235 - val_accuracy: 0.7405\n",
      "Epoch 144/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4345 - auc: 0.8698 - accuracy: 0.7890 - val_loss: 0.4919 - val_auc: 0.8245 - val_accuracy: 0.7405\n",
      "Epoch 145/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4845 - auc: 0.8216 - accuracy: 0.7512 - val_loss: 0.4919 - val_auc: 0.8246 - val_accuracy: 0.7405\n",
      "Epoch 146/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4750 - auc: 0.8362 - accuracy: 0.7707 - val_loss: 0.4918 - val_auc: 0.8247 - val_accuracy: 0.7405\n",
      "Epoch 147/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4674 - auc: 0.8443 - accuracy: 0.7646 - val_loss: 0.4915 - val_auc: 0.8251 - val_accuracy: 0.7405\n",
      "Epoch 148/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4891 - auc: 0.8305 - accuracy: 0.7421 - val_loss: 0.4913 - val_auc: 0.8252 - val_accuracy: 0.7405\n",
      "Epoch 149/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4581 - auc: 0.8467 - accuracy: 0.7691 - val_loss: 0.4910 - val_auc: 0.8252 - val_accuracy: 0.7405\n",
      "Epoch 150/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4732 - auc: 0.8375 - accuracy: 0.7598 - val_loss: 0.4910 - val_auc: 0.8251 - val_accuracy: 0.7405\n",
      "Epoch 151/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4501 - auc: 0.8494 - accuracy: 0.7657 - val_loss: 0.4909 - val_auc: 0.8255 - val_accuracy: 0.7405\n",
      "Epoch 152/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4858 - auc: 0.8274 - accuracy: 0.7644 - val_loss: 0.4908 - val_auc: 0.8258 - val_accuracy: 0.7405\n",
      "Epoch 153/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4645 - auc: 0.8424 - accuracy: 0.7580 - val_loss: 0.4906 - val_auc: 0.8260 - val_accuracy: 0.7405\n",
      "Epoch 154/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4242 - auc: 0.8719 - accuracy: 0.7989 - val_loss: 0.4905 - val_auc: 0.8257 - val_accuracy: 0.7405\n",
      "Epoch 155/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4403 - auc: 0.8697 - accuracy: 0.7867 - val_loss: 0.4903 - val_auc: 0.8260 - val_accuracy: 0.7459\n",
      "Epoch 156/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8423 - accuracy: 0.7497 - val_loss: 0.4901 - val_auc: 0.8261 - val_accuracy: 0.7514\n",
      "Epoch 157/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4701 - auc: 0.8449 - accuracy: 0.7703 - val_loss: 0.4898 - val_auc: 0.8265 - val_accuracy: 0.7514\n",
      "Epoch 158/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4737 - auc: 0.8341 - accuracy: 0.7517 - val_loss: 0.4896 - val_auc: 0.8273 - val_accuracy: 0.7568\n",
      "Epoch 159/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4318 - auc: 0.8728 - accuracy: 0.7825 - val_loss: 0.4895 - val_auc: 0.8277 - val_accuracy: 0.7568\n",
      "Epoch 160/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4442 - auc: 0.8547 - accuracy: 0.7826 - val_loss: 0.4894 - val_auc: 0.8279 - val_accuracy: 0.7568\n",
      "Epoch 161/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4461 - auc: 0.8654 - accuracy: 0.7726 - val_loss: 0.4892 - val_auc: 0.8279 - val_accuracy: 0.7568\n",
      "Epoch 162/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8411 - accuracy: 0.7626 - val_loss: 0.4892 - val_auc: 0.8277 - val_accuracy: 0.7568\n",
      "Epoch 163/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4443 - auc: 0.8555 - accuracy: 0.7901 - val_loss: 0.4888 - val_auc: 0.8285 - val_accuracy: 0.7514\n",
      "Epoch 164/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4544 - auc: 0.8455 - accuracy: 0.7723 - val_loss: 0.4887 - val_auc: 0.8290 - val_accuracy: 0.7514\n",
      "Epoch 165/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4728 - auc: 0.8348 - accuracy: 0.7500 - val_loss: 0.4886 - val_auc: 0.8291 - val_accuracy: 0.7514\n",
      "Epoch 166/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4429 - auc: 0.8609 - accuracy: 0.7859 - val_loss: 0.4884 - val_auc: 0.8292 - val_accuracy: 0.7622\n",
      "Epoch 167/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4487 - auc: 0.8559 - accuracy: 0.7678 - val_loss: 0.4882 - val_auc: 0.8293 - val_accuracy: 0.7622\n",
      "Epoch 168/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4809 - auc: 0.8359 - accuracy: 0.7566 - val_loss: 0.4879 - val_auc: 0.8292 - val_accuracy: 0.7622\n",
      "Epoch 169/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4601 - auc: 0.8500 - accuracy: 0.7614 - val_loss: 0.4878 - val_auc: 0.8296 - val_accuracy: 0.7622\n",
      "Epoch 170/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4544 - auc: 0.8559 - accuracy: 0.7701 - val_loss: 0.4876 - val_auc: 0.8296 - val_accuracy: 0.7622\n",
      "Epoch 171/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5059 - auc: 0.8109 - accuracy: 0.7466 - val_loss: 0.4876 - val_auc: 0.8298 - val_accuracy: 0.7622\n",
      "Epoch 172/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4573 - auc: 0.8487 - accuracy: 0.7628 - val_loss: 0.4875 - val_auc: 0.8296 - val_accuracy: 0.7622\n",
      "Epoch 173/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8410 - accuracy: 0.7510 - val_loss: 0.4875 - val_auc: 0.8296 - val_accuracy: 0.7622\n",
      "Epoch 174/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4508 - auc: 0.8656 - accuracy: 0.7694 - val_loss: 0.4874 - val_auc: 0.8296 - val_accuracy: 0.7622\n",
      "Epoch 175/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4666 - auc: 0.8392 - accuracy: 0.7653 - val_loss: 0.4872 - val_auc: 0.8298 - val_accuracy: 0.7622\n",
      "Epoch 176/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4461 - auc: 0.8576 - accuracy: 0.7776 - val_loss: 0.4872 - val_auc: 0.8296 - val_accuracy: 0.7622\n",
      "Epoch 177/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4757 - auc: 0.8341 - accuracy: 0.7500 - val_loss: 0.4871 - val_auc: 0.8295 - val_accuracy: 0.7622\n",
      "Epoch 178/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4631 - auc: 0.8490 - accuracy: 0.7610 - val_loss: 0.4870 - val_auc: 0.8299 - val_accuracy: 0.7622\n",
      "Epoch 179/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5078 - auc: 0.8176 - accuracy: 0.7291 - val_loss: 0.4869 - val_auc: 0.8303 - val_accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137d23d56a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Early Stopping callback\n",
    "es_callback = EarlyStopping(monitor='val_auc', mode='max', min_delta=0.001, patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define Model Checkpoint callback\n",
    "mc_path = 'model_checkpoints/early_stopping_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Creating new model\n",
    "es_model = Sequential()\n",
    "es_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Compiling model\n",
    "es_model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/ES/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Training model\n",
    "es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "correct-sociology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4993 - auc: 0.8121 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test subset and predict.\n",
    "es_model = load_model(mc_path)\n",
    "eval = es_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-think",
   "metadata": {},
   "source": [
    "# 9. Learning Rate Scheduling\n",
    "En este apartado se prueba la opción de Learning Rate Scheduling. Esta se encarga de aplicarle una función al Learning Rate entre epochs, de forma tal de encontrar el mínimo de la loss de forma más rápida, y apuntando a evitar mínimos locales y, por ende, overfitting. Se sigue aplicando el concepto de **early stopping** para la AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "floral-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay, PolynomialDecay # API in https://keras.io/api/optimizers/learning_rate_schedules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "rapid-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 158ms/step - loss: 0.7020 - auc: 0.7329 - accuracy: 0.6629 - val_loss: 0.6186 - val_auc: 0.7535 - val_accuracy: 0.7027\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5773 - auc: 0.7817 - accuracy: 0.6986 - val_loss: 0.5725 - val_auc: 0.7789 - val_accuracy: 0.7243\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5181 - auc: 0.8200 - accuracy: 0.7292 - val_loss: 0.5440 - val_auc: 0.7937 - val_accuracy: 0.7405\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5269 - auc: 0.8164 - accuracy: 0.7419 - val_loss: 0.5260 - val_auc: 0.8019 - val_accuracy: 0.7351\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5145 - auc: 0.8163 - accuracy: 0.7352 - val_loss: 0.5120 - val_auc: 0.8093 - val_accuracy: 0.7405\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5176 - auc: 0.8145 - accuracy: 0.7541 - val_loss: 0.5025 - val_auc: 0.8164 - val_accuracy: 0.7514\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4633 - auc: 0.8522 - accuracy: 0.7697 - val_loss: 0.4945 - val_auc: 0.8212 - val_accuracy: 0.7568\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4833 - auc: 0.8342 - accuracy: 0.7683 - val_loss: 0.4893 - val_auc: 0.8247 - val_accuracy: 0.7676\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4747 - auc: 0.8484 - accuracy: 0.7734 - val_loss: 0.4872 - val_auc: 0.8264 - val_accuracy: 0.7568\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4735 - auc: 0.8430 - accuracy: 0.7513 - val_loss: 0.4842 - val_auc: 0.8300 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4646 - auc: 0.8458 - accuracy: 0.7860 - val_loss: 0.4826 - val_auc: 0.8311 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4574 - auc: 0.8496 - accuracy: 0.7801 - val_loss: 0.4830 - val_auc: 0.8312 - val_accuracy: 0.7568\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4496 - auc: 0.8571 - accuracy: 0.7707 - val_loss: 0.4810 - val_auc: 0.8315 - val_accuracy: 0.7676\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4547 - auc: 0.8598 - accuracy: 0.7784 - val_loss: 0.4790 - val_auc: 0.8331 - val_accuracy: 0.7784\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4620 - auc: 0.8445 - accuracy: 0.7845 - val_loss: 0.4789 - val_auc: 0.8330 - val_accuracy: 0.7730\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4465 - auc: 0.8644 - accuracy: 0.7965 - val_loss: 0.4788 - val_auc: 0.8339 - val_accuracy: 0.7730\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4586 - auc: 0.8506 - accuracy: 0.7865 - val_loss: 0.4788 - val_auc: 0.8336 - val_accuracy: 0.7730\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4690 - auc: 0.8434 - accuracy: 0.7750 - val_loss: 0.4781 - val_auc: 0.8347 - val_accuracy: 0.7784\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4931 - auc: 0.8245 - accuracy: 0.7621 - val_loss: 0.4785 - val_auc: 0.8344 - val_accuracy: 0.7676\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8578 - accuracy: 0.7712 - val_loss: 0.4770 - val_auc: 0.8363 - val_accuracy: 0.7622\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4273 - auc: 0.8673 - accuracy: 0.7791 - val_loss: 0.4768 - val_auc: 0.8360 - val_accuracy: 0.7676\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4540 - auc: 0.8443 - accuracy: 0.7895 - val_loss: 0.4785 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4689 - auc: 0.8433 - accuracy: 0.7687 - val_loss: 0.4785 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4674 - auc: 0.8459 - accuracy: 0.7565 - val_loss: 0.4793 - val_auc: 0.8357 - val_accuracy: 0.7784\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4653 - auc: 0.8462 - accuracy: 0.7799 - val_loss: 0.4795 - val_auc: 0.8357 - val_accuracy: 0.7730\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4495 - auc: 0.8623 - accuracy: 0.7819 - val_loss: 0.4796 - val_auc: 0.8355 - val_accuracy: 0.7784\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4554 - auc: 0.8449 - accuracy: 0.7641 - val_loss: 0.4786 - val_auc: 0.8359 - val_accuracy: 0.7784\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4946 - auc: 0.8210 - accuracy: 0.7417 - val_loss: 0.4786 - val_auc: 0.8365 - val_accuracy: 0.7784\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4781 - auc: 0.8465 - accuracy: 0.7652 - val_loss: 0.4792 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4858 - auc: 0.8272 - accuracy: 0.7486 - val_loss: 0.4781 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4290 - auc: 0.8728 - accuracy: 0.7850 - val_loss: 0.4775 - val_auc: 0.8370 - val_accuracy: 0.7784\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4544 - auc: 0.8420 - accuracy: 0.7620 - val_loss: 0.4785 - val_auc: 0.8365 - val_accuracy: 0.7784\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4732 - auc: 0.8416 - accuracy: 0.7681 - val_loss: 0.4786 - val_auc: 0.8369 - val_accuracy: 0.7676\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4889 - auc: 0.8230 - accuracy: 0.7523 - val_loss: 0.4792 - val_auc: 0.8360 - val_accuracy: 0.7784\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4304 - auc: 0.8678 - accuracy: 0.7894 - val_loss: 0.4789 - val_auc: 0.8378 - val_accuracy: 0.7838\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4404 - auc: 0.8657 - accuracy: 0.7844 - val_loss: 0.4783 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4460 - auc: 0.8603 - accuracy: 0.7822 - val_loss: 0.4778 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4685 - auc: 0.8447 - accuracy: 0.7683 - val_loss: 0.4767 - val_auc: 0.8385 - val_accuracy: 0.7838\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4567 - auc: 0.8593 - accuracy: 0.7765 - val_loss: 0.4759 - val_auc: 0.8384 - val_accuracy: 0.7730\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4502 - auc: 0.8604 - accuracy: 0.7906 - val_loss: 0.4757 - val_auc: 0.8381 - val_accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137d3f55eb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/lrs_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/LRS/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model\n",
    "lrs_model = Sequential()\n",
    "lrs_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Define learning rate at start\n",
    "ilr = 0.1\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=100000, decay_rate=0.96, staircase=False) # Decay every (decay_steps) steps with a base of (decay_rate).\n",
    "\n",
    "# Compiling model\n",
    "lrs_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "supreme-senegal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5041 - auc: 0.8066 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-basket",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Exponential Decay se lleva bien con Early Stopping?, ya que si reduzco el learning rate \"me muevo menos\", con lo cual el callback de Early Stopping cortaría prematuramente. Ahora probamos sin Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "mental-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 118ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4778 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4774 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4771 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8490 - accuracy: 0.7762 - val_loss: 0.4772 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4765 - val_auc: 0.8377 - val_accuracy: 0.7676\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4758 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4746 - val_auc: 0.8393 - val_accuracy: 0.7784\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7832 - val_loss: 0.4752 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4766 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4766 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4764 - val_auc: 0.8390 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8491 - accuracy: 0.7716 - val_loss: 0.4771 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4603 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4775 - val_auc: 0.8384 - val_accuracy: 0.7676\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4779 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4786 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4786 - val_auc: 0.8373 - val_accuracy: 0.7784\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8489 - accuracy: 0.7832 - val_loss: 0.4791 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8494 - accuracy: 0.7716 - val_loss: 0.4781 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4782 - val_auc: 0.8370 - val_accuracy: 0.7676\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4783 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4614 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4782 - val_auc: 0.8367 - val_accuracy: 0.7676\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4607 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4776 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4610 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4779 - val_auc: 0.8364 - val_accuracy: 0.7730\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4607 - auc: 0.8501 - accuracy: 0.7809 - val_loss: 0.4762 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7855 - val_loss: 0.4767 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8371 - val_accuracy: 0.7730\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4773 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4779 - val_auc: 0.8366 - val_accuracy: 0.7730\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7832 - val_loss: 0.4774 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4786 - val_auc: 0.8355 - val_accuracy: 0.7730\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7879 - val_loss: 0.4804 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4796 - val_auc: 0.8357 - val_accuracy: 0.7730\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4793 - val_auc: 0.8357 - val_accuracy: 0.7730\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4783 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4785 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4783 - val_auc: 0.8373 - val_accuracy: 0.7730\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4790 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8496 - accuracy: 0.7762 - val_loss: 0.4781 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4774 - val_auc: 0.8386 - val_accuracy: 0.7730\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7739 - val_loss: 0.4782 - val_auc: 0.8379 - val_accuracy: 0.7730\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4783 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4611 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4765 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8379 - val_accuracy: 0.7730\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4773 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4770 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8488 - accuracy: 0.7716 - val_loss: 0.4770 - val_auc: 0.8384 - val_accuracy: 0.7730\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7739 - val_loss: 0.4768 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7692 - val_loss: 0.4771 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4777 - val_auc: 0.8379 - val_accuracy: 0.7730\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7762 - val_loss: 0.4780 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4785 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8490 - accuracy: 0.7832 - val_loss: 0.4785 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8487 - accuracy: 0.7786 - val_loss: 0.4791 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8499 - accuracy: 0.7762 - val_loss: 0.4770 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8495 - accuracy: 0.7716 - val_loss: 0.4766 - val_auc: 0.8391 - val_accuracy: 0.7730\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8493 - accuracy: 0.7739 - val_loss: 0.4779 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4782 - val_auc: 0.8376 - val_accuracy: 0.7838\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4768 - val_auc: 0.8386 - val_accuracy: 0.7784\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4776 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8383 - val_accuracy: 0.7730\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4781 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4779 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7739 - val_loss: 0.4784 - val_auc: 0.8361 - val_accuracy: 0.7676\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4609 - auc: 0.8498 - accuracy: 0.7716 - val_loss: 0.4788 - val_auc: 0.8358 - val_accuracy: 0.7784\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7832 - val_loss: 0.4770 - val_auc: 0.8384 - val_accuracy: 0.7730\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4614 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4784 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8489 - accuracy: 0.7716 - val_loss: 0.4788 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8498 - accuracy: 0.7739 - val_loss: 0.4781 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8370 - val_accuracy: 0.7784\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8502 - accuracy: 0.7739 - val_loss: 0.4788 - val_auc: 0.8370 - val_accuracy: 0.7784\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7739 - val_loss: 0.4781 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8496 - accuracy: 0.7762 - val_loss: 0.4775 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4774 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4770 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8497 - accuracy: 0.7739 - val_loss: 0.4786 - val_auc: 0.8362 - val_accuracy: 0.7784\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8499 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8367 - val_accuracy: 0.7784\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4617 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4785 - val_auc: 0.8366 - val_accuracy: 0.7730\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4794 - val_auc: 0.8363 - val_accuracy: 0.7784\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8495 - accuracy: 0.7739 - val_loss: 0.4795 - val_auc: 0.8373 - val_accuracy: 0.7838\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8498 - accuracy: 0.7739 - val_loss: 0.4802 - val_auc: 0.8363 - val_accuracy: 0.7838\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4794 - val_auc: 0.8371 - val_accuracy: 0.7784\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7739 - val_loss: 0.4788 - val_auc: 0.8373 - val_accuracy: 0.7838\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4790 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4791 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4795 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8485 - accuracy: 0.7832 - val_loss: 0.4792 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4794 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4795 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7855 - val_loss: 0.4785 - val_auc: 0.8379 - val_accuracy: 0.7730\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4795 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4787 - val_auc: 0.8378 - val_accuracy: 0.7676\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4788 - val_auc: 0.8366 - val_accuracy: 0.7730\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4784 - val_auc: 0.8373 - val_accuracy: 0.7676\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8487 - accuracy: 0.7739 - val_loss: 0.4768 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4771 - val_auc: 0.8388 - val_accuracy: 0.7730\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4778 - val_auc: 0.8389 - val_accuracy: 0.7730\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4794 - val_auc: 0.8371 - val_accuracy: 0.7784\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4792 - val_auc: 0.8376 - val_accuracy: 0.7838\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4788 - val_auc: 0.8374 - val_accuracy: 0.7838\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4780 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8481 - accuracy: 0.7832 - val_loss: 0.4790 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4779 - val_auc: 0.8373 - val_accuracy: 0.7784\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4794 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4784 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4789 - val_auc: 0.8362 - val_accuracy: 0.7784\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7832 - val_loss: 0.4774 - val_auc: 0.8383 - val_accuracy: 0.7730\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4781 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7832 - val_loss: 0.4788 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4794 - val_auc: 0.8366 - val_accuracy: 0.7676\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4801 - val_auc: 0.8358 - val_accuracy: 0.7784\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8488 - accuracy: 0.7739 - val_loss: 0.4789 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4776 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4777 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4786 - val_auc: 0.8373 - val_accuracy: 0.7784\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4786 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8494 - accuracy: 0.7739 - val_loss: 0.4778 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4776 - val_auc: 0.8370 - val_accuracy: 0.7676\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8492 - accuracy: 0.7739 - val_loss: 0.4778 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4777 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8490 - accuracy: 0.7855 - val_loss: 0.4782 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4788 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4778 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8489 - accuracy: 0.7832 - val_loss: 0.4787 - val_auc: 0.8382 - val_accuracy: 0.7730\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8490 - accuracy: 0.7879 - val_loss: 0.4780 - val_auc: 0.8390 - val_accuracy: 0.7784\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4778 - val_auc: 0.8386 - val_accuracy: 0.7676\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7739 - val_loss: 0.4770 - val_auc: 0.8386 - val_accuracy: 0.7730\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4761 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8500 - accuracy: 0.7739 - val_loss: 0.4754 - val_auc: 0.8396 - val_accuracy: 0.7784\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8482 - accuracy: 0.7832 - val_loss: 0.4765 - val_auc: 0.8393 - val_accuracy: 0.7730\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4768 - val_auc: 0.8385 - val_accuracy: 0.7730\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4769 - val_auc: 0.8391 - val_accuracy: 0.7784\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8492 - accuracy: 0.7692 - val_loss: 0.4770 - val_auc: 0.8394 - val_accuracy: 0.7784\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8483 - accuracy: 0.7739 - val_loss: 0.4765 - val_auc: 0.8389 - val_accuracy: 0.7730\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8485 - accuracy: 0.7832 - val_loss: 0.4775 - val_auc: 0.8373 - val_accuracy: 0.7676\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4783 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8490 - accuracy: 0.7716 - val_loss: 0.4768 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4772 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4766 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4759 - val_auc: 0.8393 - val_accuracy: 0.7784\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7832 - val_loss: 0.4758 - val_auc: 0.8391 - val_accuracy: 0.7784\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8487 - accuracy: 0.7832 - val_loss: 0.4758 - val_auc: 0.8389 - val_accuracy: 0.7784\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4767 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4762 - val_auc: 0.8384 - val_accuracy: 0.7784\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4769 - val_auc: 0.8384 - val_accuracy: 0.7838\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4763 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8491 - accuracy: 0.7855 - val_loss: 0.4773 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4787 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8499 - accuracy: 0.7739 - val_loss: 0.4794 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4778 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8498 - accuracy: 0.7739 - val_loss: 0.4780 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8493 - accuracy: 0.7739 - val_loss: 0.4763 - val_auc: 0.8382 - val_accuracy: 0.7838\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4762 - val_auc: 0.8390 - val_accuracy: 0.7784\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4772 - val_auc: 0.8383 - val_accuracy: 0.7730\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4769 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4768 - val_auc: 0.8382 - val_accuracy: 0.7730\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4777 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8489 - accuracy: 0.7809 - val_loss: 0.4788 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4784 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8487 - accuracy: 0.7855 - val_loss: 0.4793 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4788 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8485 - accuracy: 0.7832 - val_loss: 0.4782 - val_auc: 0.8385 - val_accuracy: 0.7730\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4776 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4772 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4613 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4777 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8489 - accuracy: 0.7762 - val_loss: 0.4777 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4773 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4781 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4791 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8497 - accuracy: 0.7809 - val_loss: 0.4781 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4782 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4784 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8490 - accuracy: 0.7762 - val_loss: 0.4784 - val_auc: 0.8371 - val_accuracy: 0.7676\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4787 - val_auc: 0.8357 - val_accuracy: 0.7676\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4787 - val_auc: 0.8364 - val_accuracy: 0.7676\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4785 - val_auc: 0.8377 - val_accuracy: 0.7676\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4778 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8482 - accuracy: 0.7832 - val_loss: 0.4768 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4773 - val_auc: 0.8382 - val_accuracy: 0.7784\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8486 - accuracy: 0.7832 - val_loss: 0.4766 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8485 - accuracy: 0.7786 - val_loss: 0.4774 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8484 - accuracy: 0.7809 - val_loss: 0.4779 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8490 - accuracy: 0.7855 - val_loss: 0.4787 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8499 - accuracy: 0.7809 - val_loss: 0.4785 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4777 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8494 - accuracy: 0.7855 - val_loss: 0.4776 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7832 - val_loss: 0.4773 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4775 - val_auc: 0.8372 - val_accuracy: 0.7838\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4763 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8498 - accuracy: 0.7786 - val_loss: 0.4751 - val_auc: 0.8382 - val_accuracy: 0.7730\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8492 - accuracy: 0.7739 - val_loss: 0.4764 - val_auc: 0.8381 - val_accuracy: 0.7838\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4761 - val_auc: 0.8376 - val_accuracy: 0.7838\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4613 - auc: 0.8492 - accuracy: 0.7832 - val_loss: 0.4752 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7832 - val_loss: 0.4766 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4782 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8498 - accuracy: 0.7762 - val_loss: 0.4783 - val_auc: 0.8366 - val_accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137d47b9880>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/LRS/noES\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "under-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5037 - auc: 0.8039 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-mount",
   "metadata": {},
   "source": [
    "# 10. Regularización\n",
    "La idea de la regularización es la de limitar aquellos pesos que son altos. De esta forma, se agrega una capa previa a la capa densa que contiene la capa de regularización. Se probarán dos regularizaciones distintas: L1 y L2 (donde el número significa el grado del término adicional que se suma a la función de costo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-store",
   "metadata": {},
   "source": [
    "# 10.1. Regularización L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "super-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "steady-regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 150ms/step - loss: 0.6229 - auc: 0.7084 - accuracy: 0.6594 - val_loss: 0.5521 - val_auc: 0.7929 - val_accuracy: 0.7135\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5058 - auc: 0.8211 - accuracy: 0.7329 - val_loss: 0.5144 - val_auc: 0.8167 - val_accuracy: 0.7622\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4939 - auc: 0.8337 - accuracy: 0.7502 - val_loss: 0.4963 - val_auc: 0.8239 - val_accuracy: 0.7568\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4654 - auc: 0.8564 - accuracy: 0.7843 - val_loss: 0.4851 - val_auc: 0.8342 - val_accuracy: 0.7730\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4601 - auc: 0.8552 - accuracy: 0.7856 - val_loss: 0.4826 - val_auc: 0.8354 - val_accuracy: 0.7784\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5063 - auc: 0.8071 - accuracy: 0.7561 - val_loss: 0.4845 - val_auc: 0.8352 - val_accuracy: 0.7784\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4808 - auc: 0.8280 - accuracy: 0.7803 - val_loss: 0.4863 - val_auc: 0.8342 - val_accuracy: 0.7784\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4494 - auc: 0.8536 - accuracy: 0.7892 - val_loss: 0.4855 - val_auc: 0.8348 - val_accuracy: 0.7838\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4507 - auc: 0.8498 - accuracy: 0.7958 - val_loss: 0.4850 - val_auc: 0.8362 - val_accuracy: 0.7838\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4431 - auc: 0.8664 - accuracy: 0.7868 - val_loss: 0.4838 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4570 - auc: 0.8541 - accuracy: 0.7707 - val_loss: 0.4831 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4672 - auc: 0.8449 - accuracy: 0.7590 - val_loss: 0.4836 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4505 - auc: 0.8561 - accuracy: 0.7774 - val_loss: 0.4857 - val_auc: 0.8350 - val_accuracy: 0.7784\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4943 - auc: 0.8355 - accuracy: 0.7637 - val_loss: 0.4858 - val_auc: 0.8335 - val_accuracy: 0.7622\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4264 - auc: 0.8744 - accuracy: 0.7982 - val_loss: 0.4856 - val_auc: 0.8342 - val_accuracy: 0.7730\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4883 - auc: 0.8350 - accuracy: 0.7424 - val_loss: 0.4808 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4563 - auc: 0.8577 - accuracy: 0.7656 - val_loss: 0.4801 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4988 - auc: 0.8192 - accuracy: 0.7362 - val_loss: 0.4823 - val_auc: 0.8365 - val_accuracy: 0.7838\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8435 - accuracy: 0.7720 - val_loss: 0.4801 - val_auc: 0.8371 - val_accuracy: 0.7784\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4107 - auc: 0.8885 - accuracy: 0.8208 - val_loss: 0.4785 - val_auc: 0.8379 - val_accuracy: 0.7730\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4774 - auc: 0.8480 - accuracy: 0.7629 - val_loss: 0.4793 - val_auc: 0.8390 - val_accuracy: 0.7730\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4701 - auc: 0.8499 - accuracy: 0.7710 - val_loss: 0.4804 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8435 - accuracy: 0.7633 - val_loss: 0.4820 - val_auc: 0.8362 - val_accuracy: 0.7784\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4491 - auc: 0.8585 - accuracy: 0.7774 - val_loss: 0.4807 - val_auc: 0.8372 - val_accuracy: 0.7892\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4623 - auc: 0.8386 - accuracy: 0.7721 - val_loss: 0.4815 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4571 - auc: 0.8521 - accuracy: 0.7858 - val_loss: 0.4804 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4699 - auc: 0.8462 - accuracy: 0.7754 - val_loss: 0.4824 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4457 - auc: 0.8558 - accuracy: 0.7923 - val_loss: 0.4808 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4603 - auc: 0.8537 - accuracy: 0.7896 - val_loss: 0.4782 - val_auc: 0.8394 - val_accuracy: 0.7838\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4383 - auc: 0.8570 - accuracy: 0.8069 - val_loss: 0.4788 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4500 - auc: 0.8613 - accuracy: 0.7724 - val_loss: 0.4808 - val_auc: 0.8375 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137e2c1c220>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/L1/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model for L1 Regularization\n",
    "l1_model = Sequential()\n",
    "\n",
    "# Adding dense layer to model\n",
    "l1_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, kernel_regularizer=l1(1e-3)))\n",
    "\n",
    "# Compiling model\n",
    "l1_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "l1_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "mineral-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5074 - auc: 0.8050 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "l1_model = load_model(mc_path)\n",
    "eval = l1_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-facility",
   "metadata": {},
   "source": [
    "# 10.2. Regularización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "higher-medline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 152ms/step - loss: 0.7761 - auc: 0.6312 - accuracy: 0.5955 - val_loss: 0.5253 - val_auc: 0.8112 - val_accuracy: 0.7514\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5527 - auc: 0.7838 - accuracy: 0.7032 - val_loss: 0.4875 - val_auc: 0.8463 - val_accuracy: 0.7838\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4945 - auc: 0.8390 - accuracy: 0.7576 - val_loss: 0.4831 - val_auc: 0.8496 - val_accuracy: 0.7838\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4752 - auc: 0.8486 - accuracy: 0.7719 - val_loss: 0.4836 - val_auc: 0.8451 - val_accuracy: 0.7730\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4559 - auc: 0.8682 - accuracy: 0.8017 - val_loss: 0.4829 - val_auc: 0.8438 - val_accuracy: 0.7892\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4828 - auc: 0.8457 - accuracy: 0.7527 - val_loss: 0.4842 - val_auc: 0.8440 - val_accuracy: 0.7784\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4915 - auc: 0.8380 - accuracy: 0.7776 - val_loss: 0.4857 - val_auc: 0.8417 - val_accuracy: 0.7784\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5092 - auc: 0.8176 - accuracy: 0.7481 - val_loss: 0.4879 - val_auc: 0.8389 - val_accuracy: 0.7784\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4740 - auc: 0.8495 - accuracy: 0.7880 - val_loss: 0.4916 - val_auc: 0.8359 - val_accuracy: 0.7730\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4469 - auc: 0.8665 - accuracy: 0.8043 - val_loss: 0.4883 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4946 - auc: 0.8360 - accuracy: 0.7597 - val_loss: 0.4933 - val_auc: 0.8357 - val_accuracy: 0.7784\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4881 - auc: 0.8454 - accuracy: 0.7649 - val_loss: 0.4954 - val_auc: 0.8335 - val_accuracy: 0.7676\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4841 - auc: 0.8502 - accuracy: 0.7727 - val_loss: 0.4947 - val_auc: 0.8337 - val_accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137dc3bcdc0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/L2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model for L2 Regularization\n",
    "l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(1e-2)))\n",
    "# Compiling model\n",
    "l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "responsible-terminology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 1ms/step - loss: 0.5329 - auc: 0.8019 - accuracy: 0.7338\n"
     ]
    }
   ],
   "source": [
    "l2_model = load_model(mc_path)\n",
    "eval = l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-excitement",
   "metadata": {},
   "source": [
    "# 10.3. Regularización L1+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fossil-position",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 124ms/step - loss: 0.6663 - auc: 0.6594 - accuracy: 0.6342 - val_loss: 0.6167 - val_auc: 0.7239 - val_accuracy: 0.6486\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5871 - auc: 0.7527 - accuracy: 0.6900 - val_loss: 0.5709 - val_auc: 0.7700 - val_accuracy: 0.7081\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5380 - auc: 0.8141 - accuracy: 0.7515 - val_loss: 0.5448 - val_auc: 0.7929 - val_accuracy: 0.7027\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5129 - auc: 0.8231 - accuracy: 0.7404 - val_loss: 0.5300 - val_auc: 0.8049 - val_accuracy: 0.7135\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5127 - auc: 0.8274 - accuracy: 0.7435 - val_loss: 0.5203 - val_auc: 0.8096 - val_accuracy: 0.7189\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5048 - auc: 0.8216 - accuracy: 0.7339 - val_loss: 0.5123 - val_auc: 0.8162 - val_accuracy: 0.7351\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4969 - auc: 0.8333 - accuracy: 0.7509 - val_loss: 0.5090 - val_auc: 0.8190 - val_accuracy: 0.7405\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8457 - accuracy: 0.7557 - val_loss: 0.5049 - val_auc: 0.8216 - val_accuracy: 0.7405\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4673 - auc: 0.8542 - accuracy: 0.7871 - val_loss: 0.5044 - val_auc: 0.8242 - val_accuracy: 0.7405\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5089 - auc: 0.7997 - accuracy: 0.7472 - val_loss: 0.5031 - val_auc: 0.8270 - val_accuracy: 0.7459\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4699 - auc: 0.8574 - accuracy: 0.7657 - val_loss: 0.5020 - val_auc: 0.8252 - val_accuracy: 0.7514\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4822 - auc: 0.8476 - accuracy: 0.7570 - val_loss: 0.5009 - val_auc: 0.8267 - val_accuracy: 0.7459\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4300 - auc: 0.8823 - accuracy: 0.7901 - val_loss: 0.4984 - val_auc: 0.8280 - val_accuracy: 0.7514\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4781 - auc: 0.8438 - accuracy: 0.7615 - val_loss: 0.4973 - val_auc: 0.8298 - val_accuracy: 0.7514\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4743 - auc: 0.8498 - accuracy: 0.7677 - val_loss: 0.4960 - val_auc: 0.8300 - val_accuracy: 0.7514\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4767 - auc: 0.8495 - accuracy: 0.7694 - val_loss: 0.4961 - val_auc: 0.8310 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4757 - auc: 0.8494 - accuracy: 0.7544 - val_loss: 0.4968 - val_auc: 0.8311 - val_accuracy: 0.7676\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4799 - auc: 0.8353 - accuracy: 0.7622 - val_loss: 0.4963 - val_auc: 0.8305 - val_accuracy: 0.7622\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4354 - auc: 0.8802 - accuracy: 0.7981 - val_loss: 0.4943 - val_auc: 0.8316 - val_accuracy: 0.7568\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4601 - auc: 0.8604 - accuracy: 0.7742 - val_loss: 0.4937 - val_auc: 0.8315 - val_accuracy: 0.7568\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4659 - auc: 0.8518 - accuracy: 0.7853 - val_loss: 0.4937 - val_auc: 0.8318 - val_accuracy: 0.7568\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4812 - auc: 0.8490 - accuracy: 0.7680 - val_loss: 0.4940 - val_auc: 0.8331 - val_accuracy: 0.7568\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5018 - auc: 0.8304 - accuracy: 0.7504 - val_loss: 0.4939 - val_auc: 0.8328 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4829 - auc: 0.8419 - accuracy: 0.7620 - val_loss: 0.4947 - val_auc: 0.8323 - val_accuracy: 0.7676\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8456 - accuracy: 0.7758 - val_loss: 0.4934 - val_auc: 0.8326 - val_accuracy: 0.7568\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4700 - auc: 0.8528 - accuracy: 0.7822 - val_loss: 0.4941 - val_auc: 0.8326 - val_accuracy: 0.7622\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4905 - auc: 0.8289 - accuracy: 0.7727 - val_loss: 0.4934 - val_auc: 0.8342 - val_accuracy: 0.7568\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4553 - auc: 0.8704 - accuracy: 0.7846 - val_loss: 0.4941 - val_auc: 0.8341 - val_accuracy: 0.7622\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5209 - auc: 0.8184 - accuracy: 0.7561 - val_loss: 0.4934 - val_auc: 0.8344 - val_accuracy: 0.7568\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4934 - auc: 0.8430 - accuracy: 0.7325 - val_loss: 0.4931 - val_auc: 0.8346 - val_accuracy: 0.7622\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4283 - auc: 0.8755 - accuracy: 0.8102 - val_loss: 0.4934 - val_auc: 0.8342 - val_accuracy: 0.7622\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4544 - auc: 0.8647 - accuracy: 0.7974 - val_loss: 0.4948 - val_auc: 0.8334 - val_accuracy: 0.7514\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4922 - auc: 0.8454 - accuracy: 0.7643 - val_loss: 0.4944 - val_auc: 0.8327 - val_accuracy: 0.7514\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4849 - auc: 0.8338 - accuracy: 0.7587 - val_loss: 0.4951 - val_auc: 0.8328 - val_accuracy: 0.7514\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5058 - auc: 0.8287 - accuracy: 0.7481 - val_loss: 0.4959 - val_auc: 0.8328 - val_accuracy: 0.7568\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4647 - auc: 0.8530 - accuracy: 0.7824 - val_loss: 0.4967 - val_auc: 0.8338 - val_accuracy: 0.7622\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8643 - accuracy: 0.7816 - val_loss: 0.4956 - val_auc: 0.8338 - val_accuracy: 0.7568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137d6e49ee0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1+L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/L1L2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model for L1 and L2 Regularization\n",
    "l1l2_model = Sequential()\n",
    "\n",
    "# Adding dense layer to model\n",
    "l1l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compiling model\n",
    "l1l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "l1l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "universal-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5062 - auc: 0.8090 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "l1l2_model = load_model(mc_path)\n",
    "eval = l1l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-cosmetic",
   "metadata": {},
   "source": [
    "En este caso, se nota una leve mejora en la métrica empleando regularización L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-principal",
   "metadata": {},
   "source": [
    "# 11. Dropout\n",
    "Se emplea una capa extra de dropout para minimizar el overfitting. Este regularizador funciona ignorando a neuronas de forma aleatoria. Se realiza dropout **solo en la etapa de entrenamiento**. **En teoría, no se lleva muy bien con la normalización por capas**. No tiene sentido probar esto en una red de una sola capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "framed-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "incredible-advisory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4776 - auc: 0.8376 - accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4776472747325897, 0.8375617861747742, 0.7783783674240112]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/dropout_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/Dropout/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating model\n",
    "do_model = Sequential()\n",
    "\n",
    "# Adding dropout layer to network\n",
    "do_model.add(Dropout(0))\n",
    "\n",
    "# Adding Dense layer\n",
    "do_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "\n",
    "# Compiling model\n",
    "do_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "do_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "do_model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "urban-fence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4761 - auc: 0.8372 - accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47607603669166565, 0.8371714949607849, 0.7783783674240112]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-gauge",
   "metadata": {},
   "source": [
    "# 12. Feature Engineering. Features Polinomiales\n",
    "El objertivo de esta sección es el de agregar variables de entrada al modelo, que surgen de combinar las variables originales. El grado del polinomio determina la cantidad de nuevas variables que se suman al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-livestock",
   "metadata": {},
   "source": [
    "A continuación se observa la progresión de la métrica en **train** y **valid** en función del grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "expanded-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-maker",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: Realizar la normalización de los datos **después** de aplicar el feature polinomial, sino se rompe todo :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "first-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LRS callback\n",
    "# Define learning rate at start\n",
    "ilr = 0.2 # ilr=0.5, ds = 100000, dr=0.8, stc=False\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=1000, decay_rate=0.8, staircase=True) # Decay every (decay_steps) steps with a base of (decay_rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-mileage",
   "metadata": {},
   "source": [
    "En este punto cabe aclarar que se probó el parámetro *interaction_only* del preprocesador de polinomios y se llegó a la conclusión de que el desempeño mejora con este valor en *True*. Esto es así dado que, al activarlo, se logra un número mucho menor de variables en cada orden. Esto contribuye ampliamente a **reducir el overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "educational-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Polynomial order = 1 ---\n",
      "Input count = 8\n",
      "AUC for TRAIN subset is 0.8477\n",
      "AUC for VALID subset is 0.8393\n",
      "--- Polynomial order = 2 ---\n",
      "Input count = 36\n",
      "AUC for TRAIN subset is 0.8717\n",
      "AUC for VALID subset is 0.8391\n",
      "--- Polynomial order = 3 ---\n",
      "Input count = 92\n",
      "AUC for TRAIN subset is 0.9045\n",
      "AUC for VALID subset is 0.8213\n",
      "--- Polynomial order = 4 ---\n",
      "Input count = 162\n",
      "AUC for TRAIN subset is 0.9130\n",
      "AUC for VALID subset is 0.8181\n",
      "--- Polynomial order = 5 ---\n",
      "Input count = 218\n",
      "AUC for TRAIN subset is 0.9109\n",
      "AUC for VALID subset is 0.8157\n",
      "--- Polynomial order = 6 ---\n",
      "Input count = 246\n",
      "AUC for TRAIN subset is 0.9265\n",
      "AUC for VALID subset is 0.8022\n",
      "--- Polynomial order = 7 ---\n",
      "Input count = 254\n",
      "AUC for TRAIN subset is 0.9266\n",
      "AUC for VALID subset is 0.8318\n",
      "--- Polynomial order = 8 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9354\n",
      "AUC for VALID subset is 0.8083\n",
      "--- Polynomial order = 9 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9006\n",
      "AUC for VALID subset is 0.8236\n",
      "--- Polynomial order = 10 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9207\n",
      "AUC for VALID subset is 0.8300\n",
      "Wall time: 38.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "# Define polynomial degrees to train and compute metrics\n",
    "poly_degrees = np.arange(1, 11, 1)\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=True)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    # Configuring TensorBoard to log learning process\n",
    "    log_dir = \"logs/fit/POLY/ioenabled\" + str(deg)\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Define the Model Checkpoint callback.\n",
    "    mc_path = 'model_checkpoints/get_best_poly_' + str(deg) + '_ioenabled_checkpoint.h5'\n",
    "    mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "    \n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(1e-4)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback, tensorboard_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "unknown-convert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-continuity",
   "metadata": {},
   "source": [
    "Ahora volvemos a probar lo mismo, esta vez desactivando el parámetro *interactions_only*. Aumento el grado de regularización dado que la cantidad de variables escala rápidamente con el orden del polinimio, produciendo overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "illegal-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Polynomial order = 1 ---\n",
      "Input count = 8\n",
      "AUC for TRAIN subset is 0.8468\n",
      "AUC for VALID subset is 0.8370\n",
      "--- Polynomial order = 2 ---\n",
      "Input count = 44\n",
      "AUC for TRAIN subset is 0.8912\n",
      "AUC for VALID subset is 0.8422\n",
      "--- Polynomial order = 3 ---\n",
      "Input count = 164\n",
      "AUC for TRAIN subset is 0.9190\n",
      "AUC for VALID subset is 0.8233\n",
      "--- Polynomial order = 4 ---\n",
      "Input count = 494\n",
      "AUC for TRAIN subset is 0.9722\n",
      "AUC for VALID subset is 0.8086\n",
      "--- Polynomial order = 5 ---\n",
      "Input count = 1286\n",
      "AUC for TRAIN subset is 0.9792\n",
      "AUC for VALID subset is 0.7821\n",
      "--- Polynomial order = 6 ---\n",
      "Input count = 3002\n",
      "AUC for TRAIN subset is 0.9395\n",
      "AUC for VALID subset is 0.7728\n",
      "--- Polynomial order = 7 ---\n",
      "Input count = 6434\n",
      "AUC for TRAIN subset is 0.9379\n",
      "AUC for VALID subset is 0.7705\n",
      "--- Polynomial order = 8 ---\n",
      "Input count = 12869\n",
      "AUC for TRAIN subset is 0.9892\n",
      "AUC for VALID subset is 0.7521\n",
      "--- Polynomial order = 9 ---\n",
      "Input count = 24309\n",
      "AUC for TRAIN subset is 0.9384\n",
      "AUC for VALID subset is 0.7482\n",
      "--- Polynomial order = 10 ---\n",
      "Input count = 43757\n",
      "AUC for TRAIN subset is 0.9759\n",
      "AUC for VALID subset is 0.7421\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    \n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=False)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    # Configuring TensorBoard to log learning process\n",
    "    log_dir = \"logs/fit/POLY/iodisabled\" + str(deg)\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Define the Model Checkpoint callback.\n",
    "    mc_path = 'model_checkpoints/get_best_poly_' + str(deg) + '_iodisabled_checkpoint.h5'\n",
    "    mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(1e-2)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback, tensorboard_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "awful-activity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-tiger",
   "metadata": {},
   "source": [
    "# 13. Agregando K-Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "opponent-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KFold from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_crossval(model, x, y, metrics, n_splits=2, shuffle=False, random_state=None, epochs=1, verbose='auto', callbacks=None, batch_size=32):\n",
    "    x = x.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    # Initialize kfold splitter\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    fold_num = 1\n",
    "    \n",
    "    # Create arrays for metrics\n",
    "    auc_valid = []\n",
    "    auc_train = []\n",
    "    \n",
    "    for train_index, valid_index in kf.split(x):\n",
    "        print(f'----- Fold N = {fold_num} -----')\n",
    "        # Get train and valid splits\n",
    "        x_train, y_train = x[train_index], y[train_index]\n",
    "        x_valid, y_valid = x[valid_index], y[valid_index]\n",
    "        \n",
    "        # Fit model\n",
    "        history = model.fit(x=x_train, \n",
    "                  y=y_train, \n",
    "                  validation_data=(x_valid, y_valid), \n",
    "                  shuffle=shuffle, \n",
    "                  epochs=epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  verbose=verbose, \n",
    "                  callbacks=callbacks\n",
    "                 )\n",
    "        \n",
    "        # Get metrics\n",
    "        eval_train = model.evaluate(x=x_train, y=y_train, return_dict=True, verbose=0)\n",
    "        eval_valid = model.evaluate(x=x_valid, y=y_valid, return_dict=True, verbose=0)\n",
    "        \n",
    "        tscore = eval_train['auc']\n",
    "        vscore = eval_valid['auc']\n",
    "        \n",
    "        tsize = x_train.shape[0]\n",
    "        vsize = x_valid.shape[0]\n",
    "        \n",
    "        # Append metrics\n",
    "        auc_train.append(tscore)\n",
    "        auc_valid.append(vscore)\n",
    "        \n",
    "        print(f'Size for TRAIN is {tsize}')\n",
    "        print(f'Size for VALID is {vsize}')\n",
    "        \n",
    "        print(f'AUC for TRAIN is {tscore:.4f}')\n",
    "        print(f'AUC for VALID is {vscore:.4f}')\n",
    "        \n",
    "        # Increase fold number\n",
    "        fold_num = fold_num + 1\n",
    "        \n",
    "    return auc_train, auc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "accessible-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold N = 1 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8084\n",
      "AUC for VALID is 0.9301\n",
      "----- Fold N = 2 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8527\n",
      "AUC for VALID is 0.7848\n",
      "----- Fold N = 3 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8422\n",
      "AUC for VALID is 0.8177\n",
      "----- Fold N = 4 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8403\n",
      "AUC for VALID is 0.8416\n",
      "----- Fold N = 5 -----\n",
      "Size for TRAIN is 344\n",
      "Size for VALID is 85\n",
      "AUC for TRAIN is 0.8464\n",
      "AUC for VALID is 0.8222\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "auct, aucv = kfold_crossval(model=model, x=x_train, y=y_train, metrics=metrics,\n",
    "                            n_splits=5, shuffle=False, random_state=None, epochs=1000, verbose=0, \n",
    "                            callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "#es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "first-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC for TRAIN is 0.8380\n",
      "Average AUC for VALID is 0.8393\n"
     ]
    }
   ],
   "source": [
    "aucv_mean = np.mean(aucv)\n",
    "auct_mean = np.mean(auct)\n",
    "print(f'Average AUC for TRAIN is {auct_mean:.4f}')\n",
    "print(f'Average AUC for VALID is {aucv_mean:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
