{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sixth-longitude",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 2 - Ejercicio 1 - Regresión Logística\n",
    "# Notebook #2: Implementación de una Regresión Lineal\n",
    "En esta notebook se busca implementar una regresión logística para poder estimar la condición de diabético de un paciente, perteneciente al Pima Indians Dataset analizado en la notebook anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-bracket",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "* Chequear correcto reemplazo de NaN por mean.\n",
    "* Meter el z-score en scripts comunes a ambos ejercicios. Chequear StandardScaler **correctamente inicializado**. **¿Errores de discretización?**\n",
    "    * ¿Dónde meto el área bajo la curva ROC y el F2? -> Respondido por Luqui y Karina.\n",
    "* Añadir **tensorboard** para log entre epochs. Migrar **TODOS LOS GRÁFICOS** a TensorBoard.\n",
    "    * Agregar evolución de f2-score sobre train en selección del umbral.\n",
    "* Graficar **learning rate**.\n",
    "* Sacar los evaluate con **test**, para evitar malas interpretaciones.\n",
    "* PRIMERA PRUEBA DE POLY (2) ESTÁ MAL! **Falta normalizar despues del poly**\n",
    "* Informar métricas secundarias\n",
    "\n",
    "# ¿Qué cosas puedo variar?\n",
    "* Función de activación:\n",
    "    * Sigmoid\n",
    "    * RELU\n",
    "    * ELU\n",
    "    * tanh\n",
    "    * Leaky RELU\n",
    "    \n",
    "* Optimizador:\n",
    "    * SGD\n",
    "    * Adam\n",
    "    \n",
    "* Early Stopping: Para el entrenamiento cuando la **loss** deja de mejorar. Se pasa a través de un **callback**. (https://keras.io/api/callbacks/early_stopping/) (https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "* Kernel Initializer: Esto es, como son los pesos y bias iniciales. (https://keras.io/api/layers/initializers/)\n",
    "* Model Checkpoint: Guarda un checkpoint del modelo. Puede configurarse para elegir el mejor. Se pasa por **callback**. (https://keras.io/api/callbacks/model_checkpoint/)\n",
    "* Scheduling Learning Rate: Se hace variar el **learning rate** con una función. Es un **callback**. (https://keras.io/api/callbacks/learning_rate_scheduler/)\n",
    "* Reg. dropout: Para evitar overfitting, la capa de dropout \"borra\" una entrada de forma aleatoria y escala el resto. Es una **capa**. (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "* Regularización L1 y L2: Limita el espacio de soluciones agregando un término a la **función de costo**. (https://keras.io/api/layers/regularizers/)\n",
    "* Data Augmentation\n",
    "* Batch Normalization: Normaliza las entradas (media=0, dev=1). Es una **capa**. (https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-plasma",
   "metadata": {},
   "source": [
    "# Dudas\n",
    "* Al generar la métrica F2, ¿me devuelve por batch o por epoch? -> Esto finalmente se explica más adelante.\n",
    "* Al evaluar el predict en threshold selection ¿batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-moderator",
   "metadata": {},
   "source": [
    "# ¿Cuáles son los requerimientos para el **clasificador**?\n",
    "* Métrica principal: **Área bajo la curva ROC**\n",
    "* Buscar el **umbral de decisión** para maximizar el **f2 score** \n",
    "* Informar métricas secundarias:\n",
    "    * Especificidad - Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "    * Sensibilidad\n",
    "    * Valor predictivo positivo\n",
    "    * Valor predictivo negativo\n",
    "    \n",
    "* **Pregunta adicional**:\n",
    "Dada la situación en la cual cambia la prevalencia de la enfermedad en la población a ser del 20%. Se desea reutilizar el modelo sin volver a entrenar, ¿Cómo lo harían? ¿Qué métricas se mantienen igual y cuáles cambiarian?. **¿clases desbalanceadas -> class weight?**. Las f-score son buenas para casos no balanceados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-lewis",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "transsexual-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exciting-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "embedded-funeral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read database from .csv\n",
    "df = pd.read_csv('../../databases/diabetes.csv', delimiter=',')\n",
    "\n",
    "# Show first rows of data\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-saver",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-brass",
   "metadata": {},
   "source": [
    "## 2.1 Filtrado de valores inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varied-percentage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>733.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.153420</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>12.382158</td>\n",
       "      <td>10.476982</td>\n",
       "      <td>118.775855</td>\n",
       "      <td>6.924988</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  763.000000     733.000000     541.000000  394.000000   \n",
       "mean      3.845052  121.686763      72.405184      29.153420  155.548223   \n",
       "std       3.369578   30.535641      12.382158      10.476982  118.775855   \n",
       "min       0.000000   44.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   76.250000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  125.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  190.000000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  757.000000                768.000000  768.000000  768.000000  \n",
       "mean    32.457464                  0.471876   33.240885    0.348958  \n",
       "std      6.924988                  0.331329   11.760232    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.500000                  0.243750   24.000000    0.000000  \n",
       "50%     32.300000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering Glucose values\n",
    "df['Glucose'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Blood Pressure values\n",
    "df['BloodPressure'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Skin Thickness values\n",
    "df['SkinThickness'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Insulin values\n",
    "df['Insulin'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Body Mass Index values\n",
    "df['BMI'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-flood",
   "metadata": {},
   "source": [
    "## 2.2 Remoción de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affiliated-malaysia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "frequent-lithuania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>764.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>759.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.786649</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.115438</td>\n",
       "      <td>28.903346</td>\n",
       "      <td>132.610811</td>\n",
       "      <td>32.204005</td>\n",
       "      <td>0.429832</td>\n",
       "      <td>32.805007</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.278714</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>11.239072</td>\n",
       "      <td>9.865480</td>\n",
       "      <td>74.285393</td>\n",
       "      <td>6.491385</td>\n",
       "      <td>0.249684</td>\n",
       "      <td>11.113182</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   764.000000  763.000000     719.000000     538.000000  370.000000   \n",
       "mean      3.786649  121.686763      72.115438      28.903346  132.610811   \n",
       "std       3.278714   30.535641      11.239072       9.865480   74.285393   \n",
       "min       0.000000   44.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   75.000000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  120.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  177.500000   \n",
       "max      13.000000  199.000000     104.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  749.000000                739.000000  759.000000  768.000000  \n",
       "mean    32.204005                  0.429832   32.805007    0.348958  \n",
       "std      6.491385                  0.249684   11.113182    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.400000                  0.238000   24.000000    0.000000  \n",
       "50%     32.000000                  0.356000   29.000000    0.000000  \n",
       "75%     36.500000                  0.587000   40.000000    1.000000  \n",
       "max     50.000000                  1.191000   66.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "y_labels = ['Outcome']\n",
    "\n",
    "for column in x_labels:\n",
    "    remove_outliers(df, column)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-train",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "affecting-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bacterial-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hungry-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output variables for the model\n",
    "df_x = df[x_labels]\n",
    "df_y = df[y_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "honest-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Split the train_valid sub-dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "determined-continuity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>427.000000</td>\n",
       "      <td>426.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.859485</td>\n",
       "      <td>120.514085</td>\n",
       "      <td>72.045113</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000962</td>\n",
       "      <td>0.430853</td>\n",
       "      <td>32.868235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.285896</td>\n",
       "      <td>29.742282</td>\n",
       "      <td>11.556850</td>\n",
       "      <td>9.853631</td>\n",
       "      <td>70.564358</td>\n",
       "      <td>6.568853</td>\n",
       "      <td>0.255626</td>\n",
       "      <td>11.111848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>36.100000</td>\n",
       "      <td>0.603250</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.182000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   427.000000  426.000000     399.000000     286.000000  203.000000   \n",
       "mean      3.859485  120.514085      72.045113      28.811189  132.458128   \n",
       "std       3.285896   29.742282      11.556850       9.853631   70.564358   \n",
       "min       0.000000   56.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000  100.000000      64.000000      22.000000   76.000000   \n",
       "50%       3.000000  115.000000      72.000000      29.000000  122.000000   \n",
       "75%       6.000000  138.000000      80.000000      35.750000  179.000000   \n",
       "max      13.000000  198.000000     102.000000      52.000000  335.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  416.000000                414.000000  425.000000  \n",
       "mean    32.000962                  0.430853   32.868235  \n",
       "std      6.568853                  0.255626   11.111848  \n",
       "min     18.200000                  0.085000   21.000000  \n",
       "25%     27.100000                  0.238000   24.000000  \n",
       "50%     31.600000                  0.343000   29.000000  \n",
       "75%     36.100000                  0.603250   40.000000  \n",
       "max     50.000000                  1.182000   66.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set before NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-armenia",
   "metadata": {},
   "source": [
    "# 4. Reemplazo de valores inválidos\n",
    "Como se destacó en el análisis estadístico de datos, el dataset suministrado posee varios valores faltantes en algunos individuos. Se asume que en la etapa de producción el modelo contará con todas las variables correctamente informadas, no admitiendo el faltante de alguna de ellas. Luego, se decide reemplazar aquellos valores inválidos en **train**, **valid** y **test** por la correspondiente media en el dataset de train. En este caso, se considera a la media como un estimador correcto para la ocasión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "correct-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of training\n",
    "train_means = x_train.mean().to_numpy()\n",
    "\n",
    "# Replacing nan values of the train dataset with training mean values\n",
    "for index, column in enumerate(x_train.columns):\n",
    "    x_train.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "\n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_test.columns):\n",
    "    x_test.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "    \n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_valid.columns):\n",
    "    x_valid.loc[:,column].replace(np.nan, train_means[index], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "minute-finder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.859485</td>\n",
       "      <td>120.514085</td>\n",
       "      <td>72.045113</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000962</td>\n",
       "      <td>0.430853</td>\n",
       "      <td>32.868235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.278209</td>\n",
       "      <td>29.637862</td>\n",
       "      <td>11.144462</td>\n",
       "      <td>8.040755</td>\n",
       "      <td>48.477386</td>\n",
       "      <td>6.468323</td>\n",
       "      <td>0.251107</td>\n",
       "      <td>11.059801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>72.045113</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.182000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   429.000000  429.000000     429.000000     429.000000  429.000000   \n",
       "mean      3.859485  120.514085      72.045113      28.811189  132.458128   \n",
       "std       3.278209   29.637862      11.144462       8.040755   48.477386   \n",
       "min       0.000000   56.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000  100.000000      64.000000      26.000000  126.000000   \n",
       "50%       3.000000  116.000000      72.045113      28.811189  132.458128   \n",
       "75%       6.000000  138.000000      80.000000      32.000000  132.458128   \n",
       "max      13.000000  198.000000     102.000000      52.000000  335.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  429.000000                429.000000  429.000000  \n",
       "mean    32.000962                  0.430853   32.868235  \n",
       "std      6.468323                  0.251107   11.059801  \n",
       "min     18.200000                  0.085000   21.000000  \n",
       "25%     27.300000                  0.240000   24.000000  \n",
       "50%     32.000000                  0.351000   29.000000  \n",
       "75%     36.000000                  0.591000   40.000000  \n",
       "max     50.000000                  1.182000   66.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set after NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enclosed-unknown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.776859</td>\n",
       "      <td>124.081233</td>\n",
       "      <td>72.510059</td>\n",
       "      <td>29.026687</td>\n",
       "      <td>132.833231</td>\n",
       "      <td>32.662178</td>\n",
       "      <td>0.425718</td>\n",
       "      <td>32.824890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.233510</td>\n",
       "      <td>30.668200</td>\n",
       "      <td>10.629606</td>\n",
       "      <td>8.261217</td>\n",
       "      <td>52.817100</td>\n",
       "      <td>6.221732</td>\n",
       "      <td>0.236337</td>\n",
       "      <td>10.749211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>28.800000</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   185.000000  185.000000     185.000000     185.000000  185.000000   \n",
       "mean      3.776859  124.081233      72.510059      29.026687  132.833231   \n",
       "std       3.233510   30.668200      10.629606       8.261217   52.817100   \n",
       "min       0.000000   44.000000      50.000000      11.000000   15.000000   \n",
       "25%       1.000000  101.000000      65.000000      24.000000  115.000000   \n",
       "50%       3.000000  120.000000      72.000000      28.811189  132.458128   \n",
       "75%       6.000000  145.000000      78.000000      33.000000  132.458128   \n",
       "max      13.000000  199.000000     104.000000      54.000000  330.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  185.000000                185.000000  185.000000  \n",
       "mean    32.662178                  0.425718   32.824890  \n",
       "std      6.221732                  0.236337   10.749211  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     28.800000                  0.236000   24.000000  \n",
       "50%     32.400000                  0.389000   30.000000  \n",
       "75%     36.800000                  0.549000   39.000000  \n",
       "max     49.600000                  1.189000   63.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set after NaN replacement\n",
    "x_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "innovative-salad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.597403</td>\n",
       "      <td>122.038961</td>\n",
       "      <td>71.814911</td>\n",
       "      <td>28.874262</td>\n",
       "      <td>132.374352</td>\n",
       "      <td>32.194175</td>\n",
       "      <td>0.432124</td>\n",
       "      <td>32.608678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.304818</td>\n",
       "      <td>32.320876</td>\n",
       "      <td>10.448675</td>\n",
       "      <td>8.867564</td>\n",
       "      <td>58.136767</td>\n",
       "      <td>6.484634</td>\n",
       "      <td>0.238998</td>\n",
       "      <td>11.431653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>26.925000</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000962</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>142.750000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>40.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>49.300000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   154.000000  154.000000     154.000000     154.000000  154.000000   \n",
       "mean      3.597403  122.038961      71.814911      28.874262  132.374352   \n",
       "std       3.304818   32.320876      10.448675       8.867564   58.136767   \n",
       "min       0.000000   61.000000      44.000000       7.000000   23.000000   \n",
       "25%       1.000000   95.250000      64.000000      23.250000  108.250000   \n",
       "50%       3.000000  117.000000      72.000000      28.811189  132.458128   \n",
       "75%       5.750000  142.750000      79.500000      33.000000  132.458128   \n",
       "max      13.000000  197.000000      94.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  154.000000                154.000000  154.000000  \n",
       "mean    32.194175                  0.432124   32.608678  \n",
       "std      6.484634                  0.238998   11.431653  \n",
       "min     18.400000                  0.078000   21.000000  \n",
       "25%     26.925000                  0.254000   24.000000  \n",
       "50%     32.000962                  0.376500   28.000000  \n",
       "75%     36.625000                  0.567000   40.750000  \n",
       "max     49.300000                  1.191000   66.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set after NaN replacement\n",
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-paintball",
   "metadata": {},
   "source": [
    "# 5. Normalización de datos de entrada. Z Score. \n",
    "Dado que todas las variables en juego son numéricas, se puede aplicar z-score a todo el dataset. Esta operación se hace con el objetivo de poder obtener mayor información de los pesos calculados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "upper-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! Backup unnormalized subsets for further utilization\n",
    "x_train_un = x_train\n",
    "x_valid_un = x_valid\n",
    "x_test_un = x_test\n",
    "\n",
    "# Apply z-score to all sub-datasets\n",
    "scalable_variables = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])\n",
    "    x_valid.loc[:, scalable_variables] = scaler.transform(x_valid.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "widespread-assist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.832142e-17</td>\n",
       "      <td>-7.349728e-17</td>\n",
       "      <td>2.204918e-16</td>\n",
       "      <td>5.465713e-16</td>\n",
       "      <td>-1.656277e-17</td>\n",
       "      <td>-3.519588e-17</td>\n",
       "      <td>-9.937661e-17</td>\n",
       "      <td>7.867315e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.178689e+00</td>\n",
       "      <td>-2.179287e+00</td>\n",
       "      <td>-2.878786e+00</td>\n",
       "      <td>-2.715747e+00</td>\n",
       "      <td>-2.446428e+00</td>\n",
       "      <td>-2.136114e+00</td>\n",
       "      <td>-1.378921e+00</td>\n",
       "      <td>-1.074349e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.732887e-01</td>\n",
       "      <td>-6.929662e-01</td>\n",
       "      <td>-7.227362e-01</td>\n",
       "      <td>-3.500257e-01</td>\n",
       "      <td>-1.333749e-01</td>\n",
       "      <td>-7.276152e-01</td>\n",
       "      <td>-7.609333e-01</td>\n",
       "      <td>-8.027802e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.624873e-01</td>\n",
       "      <td>-1.524859e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.423542e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.488270e-04</td>\n",
       "      <td>-3.183741e-01</td>\n",
       "      <td>-3.501647e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.537149e-01</td>\n",
       "      <td>5.906746e-01</td>\n",
       "      <td>7.146307e-01</td>\n",
       "      <td>3.970441e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.189715e-01</td>\n",
       "      <td>6.385106e-01</td>\n",
       "      <td>6.455895e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.791520e+00</td>\n",
       "      <td>2.617476e+00</td>\n",
       "      <td>2.691010e+00</td>\n",
       "      <td>2.887277e+00</td>\n",
       "      <td>4.182947e+00</td>\n",
       "      <td>2.785893e+00</td>\n",
       "      <td>2.994839e+00</td>\n",
       "      <td>2.999190e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pregnancies       Glucose  BloodPressure  SkinThickness       Insulin  \\\n",
       "count  4.290000e+02  4.290000e+02   4.290000e+02   4.290000e+02  4.290000e+02   \n",
       "mean   6.832142e-17 -7.349728e-17   2.204918e-16   5.465713e-16 -1.656277e-17   \n",
       "std    1.001168e+00  1.001168e+00   1.001168e+00   1.001168e+00  1.001168e+00   \n",
       "min   -1.178689e+00 -2.179287e+00  -2.878786e+00  -2.715747e+00 -2.446428e+00   \n",
       "25%   -8.732887e-01 -6.929662e-01  -7.227362e-01  -3.500257e-01 -1.333749e-01   \n",
       "50%   -2.624873e-01 -1.524859e-01   0.000000e+00   4.423542e-16  0.000000e+00   \n",
       "75%    6.537149e-01  5.906746e-01   7.146307e-01   3.970441e-01  0.000000e+00   \n",
       "max    2.791520e+00  2.617476e+00   2.691010e+00   2.887277e+00  4.182947e+00   \n",
       "\n",
       "                BMI  DiabetesPedigreeFunction           Age  \n",
       "count  4.290000e+02              4.290000e+02  4.290000e+02  \n",
       "mean  -3.519588e-17             -9.937661e-17  7.867315e-17  \n",
       "std    1.001168e+00              1.001168e+00  1.001168e+00  \n",
       "min   -2.136114e+00             -1.378921e+00 -1.074349e+00  \n",
       "25%   -7.276152e-01             -7.609333e-01 -8.027802e-01  \n",
       "50%   -1.488270e-04             -3.183741e-01 -3.501647e-01  \n",
       "75%    6.189715e-01              6.385106e-01  6.455895e-01  \n",
       "max    2.785893e+00              2.994839e+00  2.999190e+00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-attendance",
   "metadata": {},
   "source": [
    "# 6. Regresión Logística - Test #1\n",
    "Primera prueba de regresión logística. Se usa SGD y AUC como métrica principal. Se emplea la Binary Cross-Entropy como loss subrogada, dado que **la AUC no es diferenciable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "considerable-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TensorBoard for learning logging\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "protected-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "addressed-teacher",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import SensitivityAtSpecificity\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "horizontal-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/logistic_regression_first_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bottom-passport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Get model brief\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "expressed-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics definition\n",
    "metrics = ['AUC', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "responsible-iceland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 12s 127ms/step - loss: 1.2364 - auc: 0.2731 - accuracy: 0.3589 - val_loss: 1.1307 - val_auc: 0.2514 - val_accuracy: 0.3568\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.2328 - auc: 0.2240 - accuracy: 0.3289 - val_loss: 1.0744 - val_auc: 0.2711 - val_accuracy: 0.3568\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.1465 - auc: 0.2568 - accuracy: 0.3621 - val_loss: 1.0234 - val_auc: 0.2898 - val_accuracy: 0.3730\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.0978 - auc: 0.2444 - accuracy: 0.3561 - val_loss: 0.9755 - val_auc: 0.3102 - val_accuracy: 0.4000\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.0695 - auc: 0.2709 - accuracy: 0.3720 - val_loss: 0.9333 - val_auc: 0.3338 - val_accuracy: 0.4054\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.9968 - auc: 0.3064 - accuracy: 0.4081 - val_loss: 0.8942 - val_auc: 0.3592 - val_accuracy: 0.4216\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.9597 - auc: 0.3045 - accuracy: 0.4167 - val_loss: 0.8594 - val_auc: 0.3847 - val_accuracy: 0.4595\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.9486 - auc: 0.3103 - accuracy: 0.4203 - val_loss: 0.8285 - val_auc: 0.4105 - val_accuracy: 0.4703\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.8855 - auc: 0.3444 - accuracy: 0.4317 - val_loss: 0.7995 - val_auc: 0.4390 - val_accuracy: 0.4757\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.8656 - auc: 0.3753 - accuracy: 0.4304 - val_loss: 0.7740 - val_auc: 0.4677 - val_accuracy: 0.4919\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7980 - auc: 0.4319 - accuracy: 0.4825 - val_loss: 0.7506 - val_auc: 0.4974 - val_accuracy: 0.5027\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7969 - auc: 0.4408 - accuracy: 0.4780 - val_loss: 0.7295 - val_auc: 0.5247 - val_accuracy: 0.5351\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7880 - auc: 0.4613 - accuracy: 0.5049 - val_loss: 0.7107 - val_auc: 0.5520 - val_accuracy: 0.5568\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7778 - auc: 0.4550 - accuracy: 0.4998 - val_loss: 0.6934 - val_auc: 0.5761 - val_accuracy: 0.5838\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7681 - auc: 0.4689 - accuracy: 0.5049 - val_loss: 0.6779 - val_auc: 0.5970 - val_accuracy: 0.5946\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7421 - auc: 0.5251 - accuracy: 0.5296 - val_loss: 0.6634 - val_auc: 0.6171 - val_accuracy: 0.6108\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.7444 - auc: 0.5305 - accuracy: 0.5322 - val_loss: 0.6502 - val_auc: 0.6379 - val_accuracy: 0.6108\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6908 - auc: 0.5766 - accuracy: 0.5763 - val_loss: 0.6381 - val_auc: 0.6598 - val_accuracy: 0.6270\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6912 - auc: 0.6020 - accuracy: 0.5875 - val_loss: 0.6270 - val_auc: 0.6795 - val_accuracy: 0.6378\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6838 - auc: 0.6095 - accuracy: 0.6053 - val_loss: 0.6168 - val_auc: 0.7012 - val_accuracy: 0.6541\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6616 - auc: 0.6458 - accuracy: 0.6537 - val_loss: 0.6077 - val_auc: 0.7196 - val_accuracy: 0.6541\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6621 - auc: 0.6425 - accuracy: 0.6332 - val_loss: 0.5994 - val_auc: 0.7334 - val_accuracy: 0.6865\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6713 - auc: 0.6543 - accuracy: 0.6307 - val_loss: 0.5917 - val_auc: 0.7450 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6515 - auc: 0.6494 - accuracy: 0.6531 - val_loss: 0.5844 - val_auc: 0.7583 - val_accuracy: 0.6811\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6061 - auc: 0.7159 - accuracy: 0.6940 - val_loss: 0.5778 - val_auc: 0.7668 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6367 - auc: 0.6740 - accuracy: 0.6468 - val_loss: 0.5714 - val_auc: 0.7744 - val_accuracy: 0.6973\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6377 - auc: 0.6668 - accuracy: 0.6375 - val_loss: 0.5655 - val_auc: 0.7802 - val_accuracy: 0.6973\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5792 - auc: 0.7616 - accuracy: 0.7141 - val_loss: 0.5605 - val_auc: 0.7854 - val_accuracy: 0.7081\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6105 - auc: 0.7247 - accuracy: 0.6904 - val_loss: 0.5553 - val_auc: 0.7914 - val_accuracy: 0.7081\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5976 - auc: 0.7350 - accuracy: 0.6966 - val_loss: 0.5505 - val_auc: 0.7971 - val_accuracy: 0.7135\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5968 - auc: 0.7463 - accuracy: 0.6942 - val_loss: 0.5460 - val_auc: 0.8021 - val_accuracy: 0.7189\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6048 - auc: 0.7171 - accuracy: 0.6573 - val_loss: 0.5416 - val_auc: 0.8067 - val_accuracy: 0.7243\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5879 - auc: 0.7488 - accuracy: 0.6953 - val_loss: 0.5375 - val_auc: 0.8100 - val_accuracy: 0.7351\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5686 - auc: 0.7559 - accuracy: 0.7062 - val_loss: 0.5337 - val_auc: 0.8139 - val_accuracy: 0.7405\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5713 - auc: 0.7643 - accuracy: 0.7177 - val_loss: 0.5301 - val_auc: 0.8162 - val_accuracy: 0.7514\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5325 - auc: 0.7962 - accuracy: 0.7252 - val_loss: 0.5268 - val_auc: 0.8181 - val_accuracy: 0.7622\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5792 - auc: 0.7561 - accuracy: 0.7071 - val_loss: 0.5236 - val_auc: 0.8194 - val_accuracy: 0.7514\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5594 - auc: 0.7753 - accuracy: 0.6884 - val_loss: 0.5207 - val_auc: 0.8204 - val_accuracy: 0.7514\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5432 - auc: 0.7866 - accuracy: 0.7279 - val_loss: 0.5177 - val_auc: 0.8228 - val_accuracy: 0.7405\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5695 - auc: 0.7570 - accuracy: 0.7135 - val_loss: 0.5149 - val_auc: 0.8252 - val_accuracy: 0.7459\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5514 - auc: 0.7770 - accuracy: 0.7253 - val_loss: 0.5123 - val_auc: 0.8276 - val_accuracy: 0.7568\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5357 - auc: 0.7939 - accuracy: 0.7290 - val_loss: 0.5099 - val_auc: 0.8292 - val_accuracy: 0.7622\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5384 - auc: 0.7891 - accuracy: 0.7424 - val_loss: 0.5074 - val_auc: 0.8313 - val_accuracy: 0.7622\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5518 - auc: 0.7784 - accuracy: 0.7214 - val_loss: 0.5051 - val_auc: 0.8326 - val_accuracy: 0.7568\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5432 - auc: 0.7910 - accuracy: 0.7270 - val_loss: 0.5030 - val_auc: 0.8342 - val_accuracy: 0.7568\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5219 - auc: 0.8092 - accuracy: 0.7682 - val_loss: 0.5008 - val_auc: 0.8355 - val_accuracy: 0.7568\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5231 - auc: 0.8059 - accuracy: 0.7553 - val_loss: 0.4989 - val_auc: 0.8372 - val_accuracy: 0.7568\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5304 - auc: 0.8056 - accuracy: 0.7361 - val_loss: 0.4969 - val_auc: 0.8391 - val_accuracy: 0.7622\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5220 - auc: 0.7991 - accuracy: 0.7442 - val_loss: 0.4950 - val_auc: 0.8408 - val_accuracy: 0.7622\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5210 - auc: 0.8150 - accuracy: 0.7450 - val_loss: 0.4932 - val_auc: 0.8412 - val_accuracy: 0.7622\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5419 - auc: 0.7865 - accuracy: 0.7359 - val_loss: 0.4915 - val_auc: 0.8426 - val_accuracy: 0.7622\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5053 - auc: 0.8209 - accuracy: 0.7496 - val_loss: 0.4899 - val_auc: 0.8445 - val_accuracy: 0.7622\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5524 - auc: 0.7829 - accuracy: 0.7070 - val_loss: 0.4885 - val_auc: 0.8453 - val_accuracy: 0.7622\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5300 - auc: 0.7912 - accuracy: 0.7361 - val_loss: 0.4871 - val_auc: 0.8460 - val_accuracy: 0.7676\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5107 - auc: 0.8177 - accuracy: 0.7439 - val_loss: 0.4857 - val_auc: 0.8470 - val_accuracy: 0.7676\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5432 - auc: 0.7766 - accuracy: 0.7285 - val_loss: 0.4843 - val_auc: 0.8477 - val_accuracy: 0.7676\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5248 - auc: 0.7952 - accuracy: 0.7231 - val_loss: 0.4830 - val_auc: 0.8485 - val_accuracy: 0.7676\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4980 - auc: 0.8367 - accuracy: 0.7785 - val_loss: 0.4817 - val_auc: 0.8490 - val_accuracy: 0.7676\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5229 - auc: 0.8025 - accuracy: 0.7429 - val_loss: 0.4805 - val_auc: 0.8498 - val_accuracy: 0.7676\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5206 - auc: 0.7952 - accuracy: 0.7357 - val_loss: 0.4793 - val_auc: 0.8507 - val_accuracy: 0.7676\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5107 - auc: 0.8143 - accuracy: 0.7312 - val_loss: 0.4782 - val_auc: 0.8511 - val_accuracy: 0.7676\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5292 - auc: 0.7979 - accuracy: 0.7247 - val_loss: 0.4770 - val_auc: 0.8522 - val_accuracy: 0.7676\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4964 - auc: 0.8169 - accuracy: 0.7635 - val_loss: 0.4759 - val_auc: 0.8521 - val_accuracy: 0.7730\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5139 - auc: 0.8061 - accuracy: 0.7231 - val_loss: 0.4749 - val_auc: 0.8524 - val_accuracy: 0.7730\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5147 - auc: 0.8083 - accuracy: 0.7359 - val_loss: 0.4739 - val_auc: 0.8528 - val_accuracy: 0.7730\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4780 - auc: 0.8416 - accuracy: 0.7703 - val_loss: 0.4730 - val_auc: 0.8538 - val_accuracy: 0.7676\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4793 - auc: 0.8399 - accuracy: 0.7599 - val_loss: 0.4721 - val_auc: 0.8538 - val_accuracy: 0.7676\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5181 - auc: 0.8003 - accuracy: 0.7288 - val_loss: 0.4712 - val_auc: 0.8546 - val_accuracy: 0.7622\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4797 - auc: 0.8360 - accuracy: 0.7582 - val_loss: 0.4703 - val_auc: 0.8547 - val_accuracy: 0.7622\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4904 - auc: 0.8288 - accuracy: 0.7443 - val_loss: 0.4696 - val_auc: 0.8555 - val_accuracy: 0.7622\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5112 - auc: 0.8132 - accuracy: 0.7117 - val_loss: 0.4687 - val_auc: 0.8565 - val_accuracy: 0.7568\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5137 - auc: 0.8190 - accuracy: 0.7207 - val_loss: 0.4679 - val_auc: 0.8568 - val_accuracy: 0.7568\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5074 - auc: 0.8203 - accuracy: 0.7360 - val_loss: 0.4672 - val_auc: 0.8578 - val_accuracy: 0.7568\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4665 - auc: 0.8436 - accuracy: 0.7746 - val_loss: 0.4665 - val_auc: 0.8585 - val_accuracy: 0.7568\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5116 - auc: 0.8105 - accuracy: 0.7164 - val_loss: 0.4658 - val_auc: 0.8581 - val_accuracy: 0.7568\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3100 - auc: 0.8966 - accuracy: 0.93 - 0s 3ms/step - loss: 0.4564 - auc: 0.8491 - accuracy: 0.7866 - val_loss: 0.4651 - val_auc: 0.8585 - val_accuracy: 0.7568\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4902 - auc: 0.8272 - accuracy: 0.7724 - val_loss: 0.4645 - val_auc: 0.8586 - val_accuracy: 0.7568\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4861 - auc: 0.8468 - accuracy: 0.7346 - val_loss: 0.4638 - val_auc: 0.8596 - val_accuracy: 0.7568\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4787 - auc: 0.8377 - accuracy: 0.7520 - val_loss: 0.4633 - val_auc: 0.8601 - val_accuracy: 0.7676\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4911 - auc: 0.8248 - accuracy: 0.7363 - val_loss: 0.4628 - val_auc: 0.8607 - val_accuracy: 0.7676\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5151 - auc: 0.8184 - accuracy: 0.7318 - val_loss: 0.4622 - val_auc: 0.8608 - val_accuracy: 0.7676\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4877 - auc: 0.8321 - accuracy: 0.7535 - val_loss: 0.4617 - val_auc: 0.8609 - val_accuracy: 0.7676\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4930 - auc: 0.8317 - accuracy: 0.7430 - val_loss: 0.4612 - val_auc: 0.8617 - val_accuracy: 0.7676\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4848 - auc: 0.8422 - accuracy: 0.7484 - val_loss: 0.4606 - val_auc: 0.8616 - val_accuracy: 0.7676\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5043 - auc: 0.8131 - accuracy: 0.7386 - val_loss: 0.4602 - val_auc: 0.8622 - val_accuracy: 0.7676\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4768 - auc: 0.8437 - accuracy: 0.7495 - val_loss: 0.4597 - val_auc: 0.8621 - val_accuracy: 0.7676\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5063 - auc: 0.8175 - accuracy: 0.7300 - val_loss: 0.4592 - val_auc: 0.8618 - val_accuracy: 0.7676\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4810 - auc: 0.8559 - accuracy: 0.7646 - val_loss: 0.4588 - val_auc: 0.8619 - val_accuracy: 0.7676\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5050 - auc: 0.8218 - accuracy: 0.7433 - val_loss: 0.4584 - val_auc: 0.8617 - val_accuracy: 0.7730\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4816 - auc: 0.8328 - accuracy: 0.7534 - val_loss: 0.4580 - val_auc: 0.8616 - val_accuracy: 0.7730\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4597 - auc: 0.8534 - accuracy: 0.7651 - val_loss: 0.4576 - val_auc: 0.8621 - val_accuracy: 0.7730\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4844 - auc: 0.8295 - accuracy: 0.7348 - val_loss: 0.4572 - val_auc: 0.8619 - val_accuracy: 0.7730\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8467 - accuracy: 0.7436 - val_loss: 0.4569 - val_auc: 0.8620 - val_accuracy: 0.7730\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4803 - auc: 0.8305 - accuracy: 0.7635 - val_loss: 0.4566 - val_auc: 0.8623 - val_accuracy: 0.7730\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4850 - auc: 0.8417 - accuracy: 0.7492 - val_loss: 0.4562 - val_auc: 0.8627 - val_accuracy: 0.7730\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4780 - auc: 0.8379 - accuracy: 0.7701 - val_loss: 0.4559 - val_auc: 0.8626 - val_accuracy: 0.7730\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4706 - auc: 0.8485 - accuracy: 0.7508 - val_loss: 0.4556 - val_auc: 0.8630 - val_accuracy: 0.7730\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5171 - auc: 0.8195 - accuracy: 0.7245 - val_loss: 0.4553 - val_auc: 0.8633 - val_accuracy: 0.7730\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5206 - auc: 0.8027 - accuracy: 0.7369 - val_loss: 0.4549 - val_auc: 0.8638 - val_accuracy: 0.7730\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4764 - auc: 0.8445 - accuracy: 0.7639 - val_loss: 0.4546 - val_auc: 0.8635 - val_accuracy: 0.7730\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4904 - auc: 0.8215 - accuracy: 0.7439 - val_loss: 0.4543 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8453 - accuracy: 0.7490 - val_loss: 0.4540 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4527 - auc: 0.8618 - accuracy: 0.7870 - val_loss: 0.4537 - val_auc: 0.8638 - val_accuracy: 0.7730\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4812 - auc: 0.8299 - accuracy: 0.7568 - val_loss: 0.4534 - val_auc: 0.8635 - val_accuracy: 0.7730\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4814 - auc: 0.8382 - accuracy: 0.7456 - val_loss: 0.4532 - val_auc: 0.8636 - val_accuracy: 0.7784\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5203 - auc: 0.7887 - accuracy: 0.7187 - val_loss: 0.4529 - val_auc: 0.8637 - val_accuracy: 0.7730\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4880 - auc: 0.8231 - accuracy: 0.7644 - val_loss: 0.4527 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5022 - auc: 0.8194 - accuracy: 0.7518 - val_loss: 0.4525 - val_auc: 0.8642 - val_accuracy: 0.7784\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4594 - auc: 0.8539 - accuracy: 0.7963 - val_loss: 0.4522 - val_auc: 0.8643 - val_accuracy: 0.7784\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4752 - auc: 0.8347 - accuracy: 0.7574 - val_loss: 0.4520 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4828 - auc: 0.8326 - accuracy: 0.7509 - val_loss: 0.4518 - val_auc: 0.8644 - val_accuracy: 0.7838\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5056 - auc: 0.8130 - accuracy: 0.7300 - val_loss: 0.4516 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4760 - auc: 0.8426 - accuracy: 0.7538 - val_loss: 0.4514 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5017 - auc: 0.8217 - accuracy: 0.7311 - val_loss: 0.4512 - val_auc: 0.8642 - val_accuracy: 0.7838\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4797 - auc: 0.8355 - accuracy: 0.7549 - val_loss: 0.4510 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4970 - auc: 0.8206 - accuracy: 0.7516 - val_loss: 0.4508 - val_auc: 0.8640 - val_accuracy: 0.7838\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4490 - auc: 0.8615 - accuracy: 0.7903 - val_loss: 0.4506 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4519 - auc: 0.8619 - accuracy: 0.7689 - val_loss: 0.4505 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8400 - accuracy: 0.7766 - val_loss: 0.4503 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4779 - auc: 0.8365 - accuracy: 0.7833 - val_loss: 0.4502 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4571 - auc: 0.8462 - accuracy: 0.7803 - val_loss: 0.4500 - val_auc: 0.8640 - val_accuracy: 0.7838\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4854 - auc: 0.8288 - accuracy: 0.7528 - val_loss: 0.4498 - val_auc: 0.8635 - val_accuracy: 0.7838\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4803 - auc: 0.8355 - accuracy: 0.7718 - val_loss: 0.4497 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4699 - auc: 0.8418 - accuracy: 0.7628 - val_loss: 0.4495 - val_auc: 0.8644 - val_accuracy: 0.7838\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8268 - accuracy: 0.7695 - val_loss: 0.4494 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4969 - auc: 0.8212 - accuracy: 0.7527 - val_loss: 0.4492 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5115 - auc: 0.8155 - accuracy: 0.7466 - val_loss: 0.4491 - val_auc: 0.8646 - val_accuracy: 0.7892\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4575 - auc: 0.8452 - accuracy: 0.7913 - val_loss: 0.4490 - val_auc: 0.8644 - val_accuracy: 0.7892\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4779 - auc: 0.8409 - accuracy: 0.7678 - val_loss: 0.4488 - val_auc: 0.8643 - val_accuracy: 0.7892\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4563 - auc: 0.8550 - accuracy: 0.7813 - val_loss: 0.4487 - val_auc: 0.8638 - val_accuracy: 0.7946\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4741 - auc: 0.8435 - accuracy: 0.7714 - val_loss: 0.4485 - val_auc: 0.8638 - val_accuracy: 0.7946\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4764 - auc: 0.8420 - accuracy: 0.7692 - val_loss: 0.4485 - val_auc: 0.8641 - val_accuracy: 0.7946\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4985 - auc: 0.8154 - accuracy: 0.7520 - val_loss: 0.4484 - val_auc: 0.8641 - val_accuracy: 0.7946\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4731 - auc: 0.8385 - accuracy: 0.7659 - val_loss: 0.4483 - val_auc: 0.8641 - val_accuracy: 0.7946\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4895 - auc: 0.8216 - accuracy: 0.7637 - val_loss: 0.4482 - val_auc: 0.8641 - val_accuracy: 0.8000\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5140 - auc: 0.8003 - accuracy: 0.7398 - val_loss: 0.4481 - val_auc: 0.8639 - val_accuracy: 0.8000\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4636 - auc: 0.8408 - accuracy: 0.7817 - val_loss: 0.4480 - val_auc: 0.8638 - val_accuracy: 0.8000\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4507 - auc: 0.8573 - accuracy: 0.7853 - val_loss: 0.4479 - val_auc: 0.8642 - val_accuracy: 0.8000\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4701 - auc: 0.8360 - accuracy: 0.7719 - val_loss: 0.4478 - val_auc: 0.8643 - val_accuracy: 0.8000\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4776 - auc: 0.8357 - accuracy: 0.7670 - val_loss: 0.4477 - val_auc: 0.8643 - val_accuracy: 0.8000\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4859 - auc: 0.8374 - accuracy: 0.7681 - val_loss: 0.4476 - val_auc: 0.8643 - val_accuracy: 0.8000\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4481 - auc: 0.8619 - accuracy: 0.7839 - val_loss: 0.4476 - val_auc: 0.8643 - val_accuracy: 0.8000\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8317 - accuracy: 0.7764 - val_loss: 0.4475 - val_auc: 0.8647 - val_accuracy: 0.8000\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4829 - auc: 0.8301 - accuracy: 0.7576 - val_loss: 0.4474 - val_auc: 0.8647 - val_accuracy: 0.8000\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8315 - accuracy: 0.7754 - val_loss: 0.4473 - val_auc: 0.8642 - val_accuracy: 0.8000\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4912 - auc: 0.8211 - accuracy: 0.7673 - val_loss: 0.4473 - val_auc: 0.8643 - val_accuracy: 0.8000\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4954 - auc: 0.8374 - accuracy: 0.7487 - val_loss: 0.4472 - val_auc: 0.8646 - val_accuracy: 0.8000\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4909 - auc: 0.8261 - accuracy: 0.7613 - val_loss: 0.4472 - val_auc: 0.8644 - val_accuracy: 0.8000\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4787 - auc: 0.8345 - accuracy: 0.7519 - val_loss: 0.4471 - val_auc: 0.8646 - val_accuracy: 0.8000\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4976 - auc: 0.8217 - accuracy: 0.7700 - val_loss: 0.4470 - val_auc: 0.8643 - val_accuracy: 0.8054\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4639 - auc: 0.8422 - accuracy: 0.7847 - val_loss: 0.4469 - val_auc: 0.8643 - val_accuracy: 0.8054\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4384 - auc: 0.8692 - accuracy: 0.8008 - val_loss: 0.4469 - val_auc: 0.8646 - val_accuracy: 0.8000\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4571 - auc: 0.8379 - accuracy: 0.7696 - val_loss: 0.4469 - val_auc: 0.8644 - val_accuracy: 0.8054\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4923 - auc: 0.7997 - accuracy: 0.7718 - val_loss: 0.4469 - val_auc: 0.8648 - val_accuracy: 0.8054\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8441 - accuracy: 0.7646 - val_loss: 0.4468 - val_auc: 0.8648 - val_accuracy: 0.8054\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4814 - auc: 0.8314 - accuracy: 0.7694 - val_loss: 0.4468 - val_auc: 0.8649 - val_accuracy: 0.8054\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8327 - accuracy: 0.7584 - val_loss: 0.4467 - val_auc: 0.8649 - val_accuracy: 0.8054\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4572 - auc: 0.8449 - accuracy: 0.7692 - val_loss: 0.4467 - val_auc: 0.8649 - val_accuracy: 0.8054\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4834 - auc: 0.8414 - accuracy: 0.7781 - val_loss: 0.4466 - val_auc: 0.8653 - val_accuracy: 0.8054\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4859 - auc: 0.8396 - accuracy: 0.7568 - val_loss: 0.4465 - val_auc: 0.8654 - val_accuracy: 0.8054\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4622 - auc: 0.8402 - accuracy: 0.7806 - val_loss: 0.4465 - val_auc: 0.8656 - val_accuracy: 0.8054\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4692 - auc: 0.8240 - accuracy: 0.7599 - val_loss: 0.4464 - val_auc: 0.8653 - val_accuracy: 0.8054\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8410 - accuracy: 0.7846 - val_loss: 0.4464 - val_auc: 0.8653 - val_accuracy: 0.8054\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4619 - auc: 0.8468 - accuracy: 0.7634 - val_loss: 0.4464 - val_auc: 0.8654 - val_accuracy: 0.8054\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8438 - accuracy: 0.7883 - val_loss: 0.4463 - val_auc: 0.8653 - val_accuracy: 0.8054\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4730 - auc: 0.8383 - accuracy: 0.7732 - val_loss: 0.4463 - val_auc: 0.8654 - val_accuracy: 0.8054\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4693 - auc: 0.8488 - accuracy: 0.7815 - val_loss: 0.4463 - val_auc: 0.8651 - val_accuracy: 0.8054\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4839 - auc: 0.8366 - accuracy: 0.7617 - val_loss: 0.4462 - val_auc: 0.8653 - val_accuracy: 0.8054\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4773 - auc: 0.8356 - accuracy: 0.7604 - val_loss: 0.4462 - val_auc: 0.8656 - val_accuracy: 0.8054\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4780 - auc: 0.8351 - accuracy: 0.7618 - val_loss: 0.4462 - val_auc: 0.8661 - val_accuracy: 0.8054\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4782 - auc: 0.8457 - accuracy: 0.7680 - val_loss: 0.4461 - val_auc: 0.8660 - val_accuracy: 0.8054\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4656 - auc: 0.8482 - accuracy: 0.7797 - val_loss: 0.4461 - val_auc: 0.8658 - val_accuracy: 0.8054\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4539 - auc: 0.8554 - accuracy: 0.7934 - val_loss: 0.4460 - val_auc: 0.8659 - val_accuracy: 0.8054\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4595 - auc: 0.8446 - accuracy: 0.7850 - val_loss: 0.4460 - val_auc: 0.8656 - val_accuracy: 0.8054\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8539 - accuracy: 0.7744 - val_loss: 0.4459 - val_auc: 0.8658 - val_accuracy: 0.8054\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4632 - auc: 0.8457 - accuracy: 0.7739 - val_loss: 0.4459 - val_auc: 0.8658 - val_accuracy: 0.8054\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4510 - auc: 0.8560 - accuracy: 0.7934 - val_loss: 0.4459 - val_auc: 0.8662 - val_accuracy: 0.8054\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4793 - auc: 0.8328 - accuracy: 0.7604 - val_loss: 0.4459 - val_auc: 0.8661 - val_accuracy: 0.8000\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8503 - accuracy: 0.7849 - val_loss: 0.4459 - val_auc: 0.8661 - val_accuracy: 0.8000\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4995 - auc: 0.8168 - accuracy: 0.7246 - val_loss: 0.4458 - val_auc: 0.8660 - val_accuracy: 0.7946\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4861 - auc: 0.8356 - accuracy: 0.7592 - val_loss: 0.4458 - val_auc: 0.8661 - val_accuracy: 0.7946\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4978 - auc: 0.8165 - accuracy: 0.7398 - val_loss: 0.4458 - val_auc: 0.8663 - val_accuracy: 0.7946\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8470 - accuracy: 0.7819 - val_loss: 0.4457 - val_auc: 0.8656 - val_accuracy: 0.7946\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4974 - auc: 0.7985 - accuracy: 0.7529 - val_loss: 0.4457 - val_auc: 0.8660 - val_accuracy: 0.7946\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4916 - auc: 0.8301 - accuracy: 0.7621 - val_loss: 0.4456 - val_auc: 0.8661 - val_accuracy: 0.7946\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4528 - auc: 0.8571 - accuracy: 0.7989 - val_loss: 0.4456 - val_auc: 0.8662 - val_accuracy: 0.7946\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4699 - auc: 0.8463 - accuracy: 0.7780 - val_loss: 0.4456 - val_auc: 0.8666 - val_accuracy: 0.7838\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8524 - accuracy: 0.7598 - val_loss: 0.4456 - val_auc: 0.8663 - val_accuracy: 0.7838\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4726 - auc: 0.8312 - accuracy: 0.7681 - val_loss: 0.4455 - val_auc: 0.8665 - val_accuracy: 0.7892\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4891 - auc: 0.8276 - accuracy: 0.7562 - val_loss: 0.4455 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5194 - auc: 0.8031 - accuracy: 0.7466 - val_loss: 0.4454 - val_auc: 0.8666 - val_accuracy: 0.7892\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8376 - accuracy: 0.7655 - val_loss: 0.4454 - val_auc: 0.8665 - val_accuracy: 0.7892\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4791 - auc: 0.8326 - accuracy: 0.7630 - val_loss: 0.4454 - val_auc: 0.8665 - val_accuracy: 0.7892\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8431 - accuracy: 0.7901 - val_loss: 0.4454 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8359 - accuracy: 0.7634 - val_loss: 0.4453 - val_auc: 0.8665 - val_accuracy: 0.7892\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4676 - auc: 0.8497 - accuracy: 0.7728 - val_loss: 0.4453 - val_auc: 0.8664 - val_accuracy: 0.7892\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4665 - auc: 0.8414 - accuracy: 0.7718 - val_loss: 0.4453 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4569 - auc: 0.8462 - accuracy: 0.7820 - val_loss: 0.4453 - val_auc: 0.8665 - val_accuracy: 0.7892\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4333 - auc: 0.8590 - accuracy: 0.8089 - val_loss: 0.4453 - val_auc: 0.8668 - val_accuracy: 0.7892\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4286 - auc: 0.8685 - accuracy: 0.8103 - val_loss: 0.4453 - val_auc: 0.8666 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9ecbaf130>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=1, callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hourly-centre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 27864), started 0:27:16 ago. (Use '!kill 27864' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-faaaa429aa05de70\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-faaaa429aa05de70\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard launch\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "amateur-officer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 0.5026 - auc: 0.8022 - accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "model = load_model(mc_path)\n",
    "eval = model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-avatar",
   "metadata": {},
   "source": [
    "# 7. Elección del umbral usando f2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-meter",
   "metadata": {},
   "source": [
    "A la prueba anterior, se suma la selección del umbral (o **threshiold**) con el cual el clasificador discrimina entre clases. El mejor umbral de clasificación se calcula para todos los modelos, después del correspondiente entrenamiento. Para esta elección se elije el mejor valor del f2-score sobre el subset de **valid**. También se muestra la evolución de esta métrica respecto al umbral en el subset de **train**. En teoría, este umbral **no modifica la mérica principal del modelo, que es el área bajo la curva ROC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efficient-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def round_threshold(vector, threshold=0.5):\n",
    "    rounded_vector = []\n",
    "    for element in vector:\n",
    "        if element >= threshold:\n",
    "            rounded_vector.append(1)\n",
    "        else:\n",
    "            rounded_vector.append(0)\n",
    "            \n",
    "    return np.array(rounded_vector)\n",
    "        \n",
    "def f2_threshold_selection(y_probs_valid, y_true_valid, y_probs_train, y_true_train, steps=100, plot=True):\n",
    "    # Thresholds and f2-score vectors\n",
    "    thresholds = np.linspace(0, 1, steps)\n",
    "    f2_score_valid = []\n",
    "    f2_score_train = []\n",
    "    \n",
    "    for thld in thresholds:\n",
    "        # Generate predictions with current threshold\n",
    "        y_pred_valid = round_threshold(vector=y_probs_valid, threshold=thld)\n",
    "        y_pred_train = round_threshold(vector=y_probs_train, threshold=thld)\n",
    "        # Compute f2 score for that threshold and append\n",
    "        score_valid = fbeta_score(y_true=y_true_valid, y_pred=y_pred_valid, beta=2)\n",
    "        score_train = fbeta_score(y_true=y_true_train, y_pred=y_pred_train, beta=2)\n",
    "        f2_score_valid.append(score_valid)\n",
    "        f2_score_train.append(score_train)\n",
    "    \n",
    "    idx = np.argmax(f2_score_valid)\n",
    "    if plot == True:\n",
    "        plt.plot(thresholds, f2_score_valid, label='valid')\n",
    "        plt.plot(thresholds, f2_score_train, label='train')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('F2 score')\n",
    "        plt.axvline(thresholds[idx], color='black', linestyle='--')\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "        plt.grid(b=True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return thresholds, f2_score_valid, idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "normal-apache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.24242424242424243"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get binary-class probability from model\n",
    "y_probs_valid = model.predict(x_valid)\n",
    "y_probs_train = model.predict(x_train)\n",
    "thresholds, f2_score, idx = f2_threshold_selection(y_probs_valid, y_valid, y_probs_train, y_train, steps=100)\n",
    "thresholds[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-destiny",
   "metadata": {},
   "source": [
    "# 8. Early Stopping\n",
    "Habiendo concluido el test #1, se cree necesario agregar un callback de early stopping al modelo. Este callback deberá detener el proceso de aprendizaje en el momento en el que la **métrica principal** del modelo **deje de aumentar**. Posteriormente, se recupera el modelo con mejor performance en cuanto a esta métrica (AUC). Cabe aclarar que esta técnica es especialmente útil cuando la métrica principal no es diferenciable, y por ende se debe emplear una **loss subrogada** (en este caso, la binary cross entropy). De esta forma, el número de epochs que recorra el proceso de entrenamiento se verá limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daily-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Early Stopping callback from keras.\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "medium-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Early Stopping callback\n",
    "es_callback = EarlyStopping(monitor='val_auc', mode='max', min_delta=0.001, patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "blessed-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Checkpoint callback\n",
    "mc_path = 'model_checkpoints/early_stopping_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "incorrect-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "es_model = Sequential()\n",
    "es_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "clear-aberdeen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "14/14 [==============================] - 3s 171ms/step - loss: 0.7010 - auc: 0.6385 - accuracy: 0.6197 - val_loss: 0.7099 - val_auc: 0.6540 - val_accuracy: 0.5514\n",
      "Epoch 2/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7119 - auc: 0.6330 - accuracy: 0.6009 - val_loss: 0.6929 - val_auc: 0.6656 - val_accuracy: 0.5568\n",
      "Epoch 3/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6834 - auc: 0.6561 - accuracy: 0.6132 - val_loss: 0.6772 - val_auc: 0.6784 - val_accuracy: 0.5892\n",
      "Epoch 4/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6772 - auc: 0.6647 - accuracy: 0.6321 - val_loss: 0.6632 - val_auc: 0.6937 - val_accuracy: 0.5892\n",
      "Epoch 5/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6334 - auc: 0.7026 - accuracy: 0.6327 - val_loss: 0.6498 - val_auc: 0.7064 - val_accuracy: 0.6108\n",
      "Epoch 6/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6138 - auc: 0.7274 - accuracy: 0.6519 - val_loss: 0.6376 - val_auc: 0.7179 - val_accuracy: 0.6270\n",
      "Epoch 7/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6387 - auc: 0.6953 - accuracy: 0.6249 - val_loss: 0.6266 - val_auc: 0.7279 - val_accuracy: 0.6432\n",
      "Epoch 8/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5988 - auc: 0.7342 - accuracy: 0.6861 - val_loss: 0.6159 - val_auc: 0.7372 - val_accuracy: 0.6595\n",
      "Epoch 9/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6112 - auc: 0.7225 - accuracy: 0.6625 - val_loss: 0.6062 - val_auc: 0.7469 - val_accuracy: 0.6595\n",
      "Epoch 10/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6006 - auc: 0.7338 - accuracy: 0.6719 - val_loss: 0.5969 - val_auc: 0.7571 - val_accuracy: 0.6865\n",
      "Epoch 11/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6054 - auc: 0.7306 - accuracy: 0.6469 - val_loss: 0.5885 - val_auc: 0.7667 - val_accuracy: 0.6865\n",
      "Epoch 12/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5917 - auc: 0.7478 - accuracy: 0.6715 - val_loss: 0.5809 - val_auc: 0.7751 - val_accuracy: 0.6919\n",
      "Epoch 13/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5440 - auc: 0.7934 - accuracy: 0.7204 - val_loss: 0.5739 - val_auc: 0.7836 - val_accuracy: 0.6919\n",
      "Epoch 14/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5807 - auc: 0.7520 - accuracy: 0.6940 - val_loss: 0.5670 - val_auc: 0.7896 - val_accuracy: 0.6919\n",
      "Epoch 15/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5845 - auc: 0.7420 - accuracy: 0.6722 - val_loss: 0.5609 - val_auc: 0.7939 - val_accuracy: 0.7027\n",
      "Epoch 16/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5557 - auc: 0.7654 - accuracy: 0.6966 - val_loss: 0.5547 - val_auc: 0.8000 - val_accuracy: 0.7027\n",
      "Epoch 17/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5836 - auc: 0.7380 - accuracy: 0.6868 - val_loss: 0.5494 - val_auc: 0.8055 - val_accuracy: 0.7081\n",
      "Epoch 18/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5510 - auc: 0.7760 - accuracy: 0.7062 - val_loss: 0.5442 - val_auc: 0.8095 - val_accuracy: 0.7135\n",
      "Epoch 19/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5026 - auc: 0.8341 - accuracy: 0.7593 - val_loss: 0.5395 - val_auc: 0.8138 - val_accuracy: 0.7189\n",
      "Epoch 20/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5543 - auc: 0.7734 - accuracy: 0.7071 - val_loss: 0.5350 - val_auc: 0.8174 - val_accuracy: 0.7189\n",
      "Epoch 21/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5662 - auc: 0.7490 - accuracy: 0.6728 - val_loss: 0.5307 - val_auc: 0.8199 - val_accuracy: 0.7189\n",
      "Epoch 22/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5341 - auc: 0.7916 - accuracy: 0.7278 - val_loss: 0.5270 - val_auc: 0.8219 - val_accuracy: 0.7189\n",
      "Epoch 23/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5472 - auc: 0.7834 - accuracy: 0.7028 - val_loss: 0.5233 - val_auc: 0.8250 - val_accuracy: 0.7243\n",
      "Epoch 24/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5393 - auc: 0.7857 - accuracy: 0.6950 - val_loss: 0.5197 - val_auc: 0.8274 - val_accuracy: 0.7243\n",
      "Epoch 25/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5202 - auc: 0.8143 - accuracy: 0.7567 - val_loss: 0.5164 - val_auc: 0.8303 - val_accuracy: 0.7297\n",
      "Epoch 26/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5092 - auc: 0.8076 - accuracy: 0.7476 - val_loss: 0.5132 - val_auc: 0.8324 - val_accuracy: 0.7459\n",
      "Epoch 27/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5333 - auc: 0.7966 - accuracy: 0.7243 - val_loss: 0.5103 - val_auc: 0.8331 - val_accuracy: 0.7459\n",
      "Epoch 28/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5208 - auc: 0.8024 - accuracy: 0.7318 - val_loss: 0.5076 - val_auc: 0.8347 - val_accuracy: 0.7568\n",
      "Epoch 29/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5382 - auc: 0.7859 - accuracy: 0.7335 - val_loss: 0.5049 - val_auc: 0.8362 - val_accuracy: 0.7568\n",
      "Epoch 30/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5084 - auc: 0.8192 - accuracy: 0.7666 - val_loss: 0.5023 - val_auc: 0.8385 - val_accuracy: 0.7568\n",
      "Epoch 31/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4948 - auc: 0.8231 - accuracy: 0.7600 - val_loss: 0.5002 - val_auc: 0.8396 - val_accuracy: 0.7514\n",
      "Epoch 32/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5179 - auc: 0.8126 - accuracy: 0.7345 - val_loss: 0.4981 - val_auc: 0.8407 - val_accuracy: 0.7459\n",
      "Epoch 33/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5423 - auc: 0.7833 - accuracy: 0.7182 - val_loss: 0.4958 - val_auc: 0.8424 - val_accuracy: 0.7514\n",
      "Epoch 34/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5086 - auc: 0.8066 - accuracy: 0.7489 - val_loss: 0.4938 - val_auc: 0.8429 - val_accuracy: 0.7459\n",
      "Epoch 35/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5218 - auc: 0.8053 - accuracy: 0.7364 - val_loss: 0.4919 - val_auc: 0.8448 - val_accuracy: 0.7622\n",
      "Epoch 36/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5006 - auc: 0.8220 - accuracy: 0.7600 - val_loss: 0.4902 - val_auc: 0.8463 - val_accuracy: 0.7622\n",
      "Epoch 37/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4946 - auc: 0.8262 - accuracy: 0.7550 - val_loss: 0.4885 - val_auc: 0.8468 - val_accuracy: 0.7622\n",
      "Epoch 38/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4933 - auc: 0.8335 - accuracy: 0.7494 - val_loss: 0.4868 - val_auc: 0.8483 - val_accuracy: 0.7622\n",
      "Epoch 39/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4759 - auc: 0.8400 - accuracy: 0.7804 - val_loss: 0.4853 - val_auc: 0.8491 - val_accuracy: 0.7622\n",
      "Epoch 40/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5068 - auc: 0.8193 - accuracy: 0.7683 - val_loss: 0.4838 - val_auc: 0.8501 - val_accuracy: 0.7622\n",
      "Epoch 41/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4633 - auc: 0.8480 - accuracy: 0.7861 - val_loss: 0.4826 - val_auc: 0.8501 - val_accuracy: 0.7676\n",
      "Epoch 42/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4966 - auc: 0.8174 - accuracy: 0.7495 - val_loss: 0.4812 - val_auc: 0.8503 - val_accuracy: 0.7730\n",
      "Epoch 43/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4875 - auc: 0.8268 - accuracy: 0.7738 - val_loss: 0.4800 - val_auc: 0.8514 - val_accuracy: 0.7730\n",
      "Epoch 44/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4656 - auc: 0.8495 - accuracy: 0.7739 - val_loss: 0.4787 - val_auc: 0.8516 - val_accuracy: 0.7784\n",
      "Epoch 45/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4826 - auc: 0.8366 - accuracy: 0.7553 - val_loss: 0.4775 - val_auc: 0.8523 - val_accuracy: 0.7784\n",
      "Epoch 46/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4473 - auc: 0.8652 - accuracy: 0.7969 - val_loss: 0.4764 - val_auc: 0.8536 - val_accuracy: 0.7730\n",
      "Epoch 47/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4880 - auc: 0.8359 - accuracy: 0.7804 - val_loss: 0.4754 - val_auc: 0.8534 - val_accuracy: 0.7730\n",
      "Epoch 48/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4971 - auc: 0.8306 - accuracy: 0.7617 - val_loss: 0.4743 - val_auc: 0.8538 - val_accuracy: 0.7730\n",
      "Epoch 49/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4788 - auc: 0.8361 - accuracy: 0.7748 - val_loss: 0.4734 - val_auc: 0.8543 - val_accuracy: 0.7730\n",
      "Epoch 50/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4810 - auc: 0.8392 - accuracy: 0.7815 - val_loss: 0.4724 - val_auc: 0.8549 - val_accuracy: 0.7730\n",
      "Epoch 51/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4929 - auc: 0.8237 - accuracy: 0.7624 - val_loss: 0.4715 - val_auc: 0.8551 - val_accuracy: 0.7730\n",
      "Epoch 52/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5234 - auc: 0.7993 - accuracy: 0.7412 - val_loss: 0.4706 - val_auc: 0.8551 - val_accuracy: 0.7676\n",
      "Epoch 53/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4891 - auc: 0.8237 - accuracy: 0.7661 - val_loss: 0.4699 - val_auc: 0.8561 - val_accuracy: 0.7676\n",
      "Epoch 54/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4680 - auc: 0.8499 - accuracy: 0.7898 - val_loss: 0.4690 - val_auc: 0.8561 - val_accuracy: 0.7676\n",
      "Epoch 55/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4952 - auc: 0.8183 - accuracy: 0.7567 - val_loss: 0.4682 - val_auc: 0.8566 - val_accuracy: 0.7676\n",
      "Epoch 56/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4696 - auc: 0.8447 - accuracy: 0.7947 - val_loss: 0.4676 - val_auc: 0.8561 - val_accuracy: 0.7676\n",
      "Epoch 57/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4771 - auc: 0.8358 - accuracy: 0.7760 - val_loss: 0.4668 - val_auc: 0.8566 - val_accuracy: 0.7676\n",
      "Epoch 58/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4783 - auc: 0.8367 - accuracy: 0.7617 - val_loss: 0.4662 - val_auc: 0.8578 - val_accuracy: 0.7730\n",
      "Epoch 59/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4680 - auc: 0.8480 - accuracy: 0.7751 - val_loss: 0.4655 - val_auc: 0.8580 - val_accuracy: 0.7730\n",
      "Epoch 60/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5257 - auc: 0.7988 - accuracy: 0.7427 - val_loss: 0.4649 - val_auc: 0.8583 - val_accuracy: 0.7730\n",
      "Epoch 61/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4766 - auc: 0.8374 - accuracy: 0.7705 - val_loss: 0.4645 - val_auc: 0.8578 - val_accuracy: 0.7730\n",
      "Epoch 62/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4755 - auc: 0.8337 - accuracy: 0.7781 - val_loss: 0.4639 - val_auc: 0.8587 - val_accuracy: 0.7730\n",
      "Epoch 63/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4790 - auc: 0.8441 - accuracy: 0.7852 - val_loss: 0.4633 - val_auc: 0.8592 - val_accuracy: 0.7784\n",
      "Epoch 64/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5283 - auc: 0.7915 - accuracy: 0.7345 - val_loss: 0.4628 - val_auc: 0.8599 - val_accuracy: 0.7838\n",
      "Epoch 65/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4645 - auc: 0.8488 - accuracy: 0.7792 - val_loss: 0.4623 - val_auc: 0.8598 - val_accuracy: 0.7838\n",
      "Epoch 66/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4821 - auc: 0.8292 - accuracy: 0.7771 - val_loss: 0.4618 - val_auc: 0.8601 - val_accuracy: 0.7838\n",
      "Epoch 67/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4752 - auc: 0.8434 - accuracy: 0.7627 - val_loss: 0.4614 - val_auc: 0.8604 - val_accuracy: 0.7838\n",
      "Epoch 68/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4507 - auc: 0.8585 - accuracy: 0.7874 - val_loss: 0.4610 - val_auc: 0.8603 - val_accuracy: 0.7838\n",
      "Epoch 69/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4964 - auc: 0.8179 - accuracy: 0.7412 - val_loss: 0.4605 - val_auc: 0.8604 - val_accuracy: 0.7838\n",
      "Epoch 70/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4826 - auc: 0.8313 - accuracy: 0.7639 - val_loss: 0.4601 - val_auc: 0.8602 - val_accuracy: 0.7838\n",
      "Epoch 71/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4808 - auc: 0.8161 - accuracy: 0.7463 - val_loss: 0.4597 - val_auc: 0.8604 - val_accuracy: 0.7838\n",
      "Epoch 72/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4913 - auc: 0.8330 - accuracy: 0.7758 - val_loss: 0.4593 - val_auc: 0.8609 - val_accuracy: 0.7838\n",
      "Epoch 73/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4543 - auc: 0.8578 - accuracy: 0.7725 - val_loss: 0.4589 - val_auc: 0.8609 - val_accuracy: 0.7838\n",
      "Epoch 74/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4750 - auc: 0.8351 - accuracy: 0.7768 - val_loss: 0.4585 - val_auc: 0.8606 - val_accuracy: 0.7838\n",
      "Epoch 75/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4847 - auc: 0.8342 - accuracy: 0.7764 - val_loss: 0.4581 - val_auc: 0.8607 - val_accuracy: 0.7838\n",
      "Epoch 76/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4664 - auc: 0.8499 - accuracy: 0.7773 - val_loss: 0.4578 - val_auc: 0.8602 - val_accuracy: 0.7838\n",
      "Epoch 77/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5061 - auc: 0.8083 - accuracy: 0.7323 - val_loss: 0.4574 - val_auc: 0.8605 - val_accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9f6bf7ca0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "es_model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/ES/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "genuine-generic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5086 - auc: 0.7995 - accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test subset and predict.\n",
    "es_model = load_model(mc_path)\n",
    "eval = es_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-intersection",
   "metadata": {},
   "source": [
    "# 9. Learning Rate Scheduling\n",
    "En este apartado se prueba la opción de Learning Rate Scheduling. Esta se encarga de aplicarle una función al Learning Rate entre epochs, de forma tal de encontrar el mínimo de la loss de forma más rápida, y apuntando a evitar mínimos locales y, por ende, overfitting. Se sigue aplicando el concepto de **early stopping** para la AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "local-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay, PolynomialDecay # API in https://keras.io/api/optimizers/learning_rate_schedules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "interpreted-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/lrs_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "exempt-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "lrs_model = Sequential()\n",
    "lrs_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "accredited-village",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 158ms/step - loss: 1.0122 - auc: 0.3697 - accuracy: 0.3835 - val_loss: 0.7254 - val_auc: 0.5321 - val_accuracy: 0.5405\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6830 - auc: 0.5809 - accuracy: 0.6249 - val_loss: 0.5583 - val_auc: 0.8022 - val_accuracy: 0.7351\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5478 - auc: 0.7911 - accuracy: 0.7186 - val_loss: 0.5030 - val_auc: 0.8528 - val_accuracy: 0.7730\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4999 - auc: 0.8264 - accuracy: 0.7526 - val_loss: 0.4797 - val_auc: 0.8616 - val_accuracy: 0.7730\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5023 - auc: 0.8164 - accuracy: 0.7423 - val_loss: 0.4689 - val_auc: 0.8653 - val_accuracy: 0.7676\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5204 - auc: 0.7922 - accuracy: 0.7340 - val_loss: 0.4618 - val_auc: 0.8658 - val_accuracy: 0.7784\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4787 - auc: 0.8382 - accuracy: 0.7716 - val_loss: 0.4567 - val_auc: 0.8680 - val_accuracy: 0.7892\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4809 - auc: 0.8177 - accuracy: 0.7567 - val_loss: 0.4536 - val_auc: 0.8670 - val_accuracy: 0.7892\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4914 - auc: 0.8169 - accuracy: 0.7445 - val_loss: 0.4516 - val_auc: 0.8675 - val_accuracy: 0.7946\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4482 - auc: 0.8491 - accuracy: 0.7910 - val_loss: 0.4509 - val_auc: 0.8661 - val_accuracy: 0.7946\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8316 - accuracy: 0.7566 - val_loss: 0.4491 - val_auc: 0.8668 - val_accuracy: 0.7946\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4326 - auc: 0.8680 - accuracy: 0.8087 - val_loss: 0.4479 - val_auc: 0.8674 - val_accuracy: 0.7946\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5028 - auc: 0.8100 - accuracy: 0.7488 - val_loss: 0.4476 - val_auc: 0.8673 - val_accuracy: 0.7892\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4550 - auc: 0.8425 - accuracy: 0.7822 - val_loss: 0.4464 - val_auc: 0.8670 - val_accuracy: 0.7946\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4497 - auc: 0.8564 - accuracy: 0.7701 - val_loss: 0.4469 - val_auc: 0.8664 - val_accuracy: 0.8000\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4615 - auc: 0.8427 - accuracy: 0.7746 - val_loss: 0.4470 - val_auc: 0.8675 - val_accuracy: 0.7892\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4510 - auc: 0.8596 - accuracy: 0.7979 - val_loss: 0.4464 - val_auc: 0.8665 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9f8aa87f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define learning rate at start\n",
    "ilr = 0.1\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=100000, decay_rate=0.96, staircase=False) # Decay every (decay_steps) steps with a base of (decay_rate).\n",
    "# Compiling model\n",
    "lrs_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "attached-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5113 - auc: 0.8063 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-translator",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Exponential Decay se lleva bien con Early Stopping?, ya que si reduzco el learning rate \"me muevo menos\", con lo cual el callback de Early Stopping cortaría prematuramente. Ahora probamos sin Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fresh-train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 105ms/step - loss: 0.4809 - auc: 0.8342 - accuracy: 0.7529 - val_loss: 0.4539 - val_auc: 0.8671 - val_accuracy: 0.7892\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4797 - auc: 0.8340 - accuracy: 0.7552 - val_loss: 0.4515 - val_auc: 0.8665 - val_accuracy: 0.7946\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4772 - auc: 0.8355 - accuracy: 0.7692 - val_loss: 0.4496 - val_auc: 0.8676 - val_accuracy: 0.7946\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4757 - auc: 0.8368 - accuracy: 0.7646 - val_loss: 0.4475 - val_auc: 0.8671 - val_accuracy: 0.7892\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4748 - auc: 0.8370 - accuracy: 0.7599 - val_loss: 0.4462 - val_auc: 0.8673 - val_accuracy: 0.7946\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4739 - auc: 0.8372 - accuracy: 0.7739 - val_loss: 0.4458 - val_auc: 0.8670 - val_accuracy: 0.8000\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4736 - auc: 0.8378 - accuracy: 0.7692 - val_loss: 0.4462 - val_auc: 0.8675 - val_accuracy: 0.7892\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4729 - auc: 0.8383 - accuracy: 0.7669 - val_loss: 0.4455 - val_auc: 0.8675 - val_accuracy: 0.8000\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4723 - auc: 0.8388 - accuracy: 0.7692 - val_loss: 0.4453 - val_auc: 0.8675 - val_accuracy: 0.7892\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4729 - auc: 0.8384 - accuracy: 0.7646 - val_loss: 0.4449 - val_auc: 0.8671 - val_accuracy: 0.7892\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4722 - auc: 0.8390 - accuracy: 0.7669 - val_loss: 0.4449 - val_auc: 0.8666 - val_accuracy: 0.7784\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8390 - accuracy: 0.7669 - val_loss: 0.4448 - val_auc: 0.8666 - val_accuracy: 0.7838\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8386 - accuracy: 0.7669 - val_loss: 0.4451 - val_auc: 0.8657 - val_accuracy: 0.7838\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8386 - accuracy: 0.7716 - val_loss: 0.4449 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8386 - accuracy: 0.7622 - val_loss: 0.4450 - val_auc: 0.8660 - val_accuracy: 0.7838\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8387 - accuracy: 0.7692 - val_loss: 0.4453 - val_auc: 0.8661 - val_accuracy: 0.7784\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8387 - accuracy: 0.7669 - val_loss: 0.4453 - val_auc: 0.8657 - val_accuracy: 0.7838\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4452 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8387 - accuracy: 0.7692 - val_loss: 0.4452 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8386 - accuracy: 0.7739 - val_loss: 0.4450 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8395 - accuracy: 0.7739 - val_loss: 0.4448 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4454 - val_auc: 0.8661 - val_accuracy: 0.8000\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8398 - accuracy: 0.7716 - val_loss: 0.4453 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8398 - accuracy: 0.7692 - val_loss: 0.4458 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4456 - val_auc: 0.8661 - val_accuracy: 0.7946\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8399 - accuracy: 0.7739 - val_loss: 0.4451 - val_auc: 0.8668 - val_accuracy: 0.7946\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4449 - val_auc: 0.8664 - val_accuracy: 0.7946\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8399 - accuracy: 0.7739 - val_loss: 0.4450 - val_auc: 0.8658 - val_accuracy: 0.7838\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8387 - accuracy: 0.7739 - val_loss: 0.4448 - val_auc: 0.8663 - val_accuracy: 0.7838\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4451 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8388 - accuracy: 0.7669 - val_loss: 0.4459 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4458 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8393 - accuracy: 0.7809 - val_loss: 0.4455 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8386 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8393 - accuracy: 0.7716 - val_loss: 0.4457 - val_auc: 0.8650 - val_accuracy: 0.7892\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8390 - accuracy: 0.7762 - val_loss: 0.4458 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8385 - accuracy: 0.7669 - val_loss: 0.4458 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8385 - accuracy: 0.7692 - val_loss: 0.4453 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8386 - accuracy: 0.7669 - val_loss: 0.4453 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8392 - accuracy: 0.7692 - val_loss: 0.4453 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8391 - accuracy: 0.7692 - val_loss: 0.4453 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8393 - accuracy: 0.7692 - val_loss: 0.4450 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8386 - accuracy: 0.7692 - val_loss: 0.4454 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7692 - val_loss: 0.4458 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4456 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8389 - accuracy: 0.7716 - val_loss: 0.4453 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4710 - auc: 0.8393 - accuracy: 0.7692 - val_loss: 0.4454 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8396 - accuracy: 0.7692 - val_loss: 0.4460 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7669 - val_loss: 0.4458 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4712 - auc: 0.8393 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8385 - accuracy: 0.7669 - val_loss: 0.4452 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8389 - accuracy: 0.7739 - val_loss: 0.4449 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8395 - accuracy: 0.7716 - val_loss: 0.4450 - val_auc: 0.8655 - val_accuracy: 0.7838\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8397 - accuracy: 0.7786 - val_loss: 0.4450 - val_auc: 0.8663 - val_accuracy: 0.7838\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7692 - val_loss: 0.4446 - val_auc: 0.8655 - val_accuracy: 0.7838\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7716 - val_loss: 0.4445 - val_auc: 0.8652 - val_accuracy: 0.7838\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8393 - accuracy: 0.7646 - val_loss: 0.4447 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8398 - accuracy: 0.7716 - val_loss: 0.4450 - val_auc: 0.8657 - val_accuracy: 0.7946\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4448 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8388 - accuracy: 0.7646 - val_loss: 0.4446 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8395 - accuracy: 0.7646 - val_loss: 0.4448 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4449 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8395 - accuracy: 0.7716 - val_loss: 0.4448 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8391 - accuracy: 0.7646 - val_loss: 0.4447 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4714 - auc: 0.8391 - accuracy: 0.7669 - val_loss: 0.4448 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4447 - val_auc: 0.8650 - val_accuracy: 0.7892\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8397 - accuracy: 0.7669 - val_loss: 0.4453 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8396 - accuracy: 0.7692 - val_loss: 0.4453 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4450 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8402 - accuracy: 0.7692 - val_loss: 0.4454 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8395 - accuracy: 0.7692 - val_loss: 0.4456 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8390 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7692 - val_loss: 0.4457 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4458 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4720 - auc: 0.8388 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8390 - accuracy: 0.7762 - val_loss: 0.4456 - val_auc: 0.8649 - val_accuracy: 0.7946\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7762 - val_loss: 0.4456 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8390 - accuracy: 0.7762 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4452 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4451 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7692 - val_loss: 0.4451 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8389 - accuracy: 0.7786 - val_loss: 0.4459 - val_auc: 0.8646 - val_accuracy: 0.7892\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8386 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4458 - val_auc: 0.8658 - val_accuracy: 0.7838\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8389 - accuracy: 0.7692 - val_loss: 0.4456 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8398 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4712 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4454 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4451 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8387 - accuracy: 0.7716 - val_loss: 0.4451 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4458 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4723 - auc: 0.8377 - accuracy: 0.7622 - val_loss: 0.4455 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8379 - accuracy: 0.7716 - val_loss: 0.4455 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8391 - accuracy: 0.7692 - val_loss: 0.4457 - val_auc: 0.8660 - val_accuracy: 0.7838\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8383 - accuracy: 0.7739 - val_loss: 0.4458 - val_auc: 0.8662 - val_accuracy: 0.7838\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8383 - accuracy: 0.7669 - val_loss: 0.4465 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4459 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8387 - accuracy: 0.7762 - val_loss: 0.4463 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8388 - accuracy: 0.7762 - val_loss: 0.4463 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4469 - val_auc: 0.8655 - val_accuracy: 0.7946\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8384 - accuracy: 0.7762 - val_loss: 0.4464 - val_auc: 0.8654 - val_accuracy: 0.7946\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4720 - auc: 0.8383 - accuracy: 0.7762 - val_loss: 0.4463 - val_auc: 0.8650 - val_accuracy: 0.7946\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8401 - accuracy: 0.7739 - val_loss: 0.4467 - val_auc: 0.8650 - val_accuracy: 0.7838\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8394 - accuracy: 0.7716 - val_loss: 0.4473 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4470 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8389 - accuracy: 0.7716 - val_loss: 0.4470 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8389 - accuracy: 0.7669 - val_loss: 0.4463 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8396 - accuracy: 0.7762 - val_loss: 0.4457 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8396 - accuracy: 0.7762 - val_loss: 0.4459 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8385 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8396 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8395 - accuracy: 0.7739 - val_loss: 0.4458 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4457 - val_auc: 0.8650 - val_accuracy: 0.7892\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8390 - accuracy: 0.7809 - val_loss: 0.4458 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8398 - accuracy: 0.7716 - val_loss: 0.4465 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8388 - accuracy: 0.7739 - val_loss: 0.4469 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8385 - accuracy: 0.7786 - val_loss: 0.4462 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8397 - accuracy: 0.7762 - val_loss: 0.4464 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8386 - accuracy: 0.7786 - val_loss: 0.4460 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4460 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7786 - val_loss: 0.4459 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8399 - accuracy: 0.7786 - val_loss: 0.4458 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4454 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8650 - val_accuracy: 0.7892\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8387 - accuracy: 0.7692 - val_loss: 0.4455 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4463 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8399 - accuracy: 0.7716 - val_loss: 0.4463 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8393 - accuracy: 0.7809 - val_loss: 0.4463 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8386 - accuracy: 0.7739 - val_loss: 0.4458 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8390 - accuracy: 0.7786 - val_loss: 0.4453 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4451 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4451 - val_auc: 0.8663 - val_accuracy: 0.7838\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4453 - val_auc: 0.8663 - val_accuracy: 0.7838\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8400 - accuracy: 0.7762 - val_loss: 0.4453 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8389 - accuracy: 0.7786 - val_loss: 0.4456 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8388 - accuracy: 0.7786 - val_loss: 0.4457 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4457 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8394 - accuracy: 0.7692 - val_loss: 0.4462 - val_auc: 0.8646 - val_accuracy: 0.7892\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8395 - accuracy: 0.7692 - val_loss: 0.4459 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4463 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8389 - accuracy: 0.7716 - val_loss: 0.4457 - val_auc: 0.8650 - val_accuracy: 0.7892\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8393 - accuracy: 0.7692 - val_loss: 0.4460 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8381 - accuracy: 0.7762 - val_loss: 0.4456 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8387 - accuracy: 0.7739 - val_loss: 0.4453 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8388 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8664 - val_accuracy: 0.7838\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8387 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4453 - val_auc: 0.8654 - val_accuracy: 0.7838\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8386 - accuracy: 0.7669 - val_loss: 0.4456 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4457 - val_auc: 0.8657 - val_accuracy: 0.7838\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4712 - auc: 0.8390 - accuracy: 0.7692 - val_loss: 0.4454 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4463 - val_auc: 0.8650 - val_accuracy: 0.7838\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8655 - val_accuracy: 0.7838\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8393 - accuracy: 0.7716 - val_loss: 0.4462 - val_auc: 0.8647 - val_accuracy: 0.7892\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8387 - accuracy: 0.7739 - val_loss: 0.4461 - val_auc: 0.8644 - val_accuracy: 0.8000\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8398 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8644 - val_accuracy: 0.7892\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4708 - auc: 0.8394 - accuracy: 0.7786 - val_loss: 0.4457 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4459 - val_auc: 0.8652 - val_accuracy: 0.7838\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8390 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8390 - accuracy: 0.7692 - val_loss: 0.4461 - val_auc: 0.8650 - val_accuracy: 0.7838\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8385 - accuracy: 0.7762 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8395 - accuracy: 0.7786 - val_loss: 0.4457 - val_auc: 0.8643 - val_accuracy: 0.7892\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4462 - val_auc: 0.8660 - val_accuracy: 0.7838\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7786 - val_loss: 0.4466 - val_auc: 0.8665 - val_accuracy: 0.7838\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4461 - val_auc: 0.8662 - val_accuracy: 0.7892\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4707 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4460 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8394 - accuracy: 0.7786 - val_loss: 0.4461 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8397 - accuracy: 0.7786 - val_loss: 0.4458 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8398 - accuracy: 0.7739 - val_loss: 0.4457 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8396 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8662 - val_accuracy: 0.7838\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8396 - accuracy: 0.7739 - val_loss: 0.4457 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8386 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4454 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4455 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8386 - accuracy: 0.7692 - val_loss: 0.4457 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4456 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8386 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8398 - accuracy: 0.7739 - val_loss: 0.4453 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4449 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8393 - accuracy: 0.7786 - val_loss: 0.4449 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8393 - accuracy: 0.7692 - val_loss: 0.4450 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8388 - accuracy: 0.7762 - val_loss: 0.4449 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.3876 - auc: 0.9187 - accuracy: 0.81 - 0s 3ms/step - loss: 0.4711 - auc: 0.8396 - accuracy: 0.7762 - val_loss: 0.4450 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8390 - accuracy: 0.7786 - val_loss: 0.4455 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8389 - accuracy: 0.7786 - val_loss: 0.4453 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8395 - accuracy: 0.7786 - val_loss: 0.4451 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8384 - accuracy: 0.7739 - val_loss: 0.4454 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4455 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8385 - accuracy: 0.7739 - val_loss: 0.4456 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7716 - val_loss: 0.4455 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8387 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8388 - accuracy: 0.7762 - val_loss: 0.4457 - val_auc: 0.8654 - val_accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9f877bd90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "preceding-pharmaceutical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5137 - auc: 0.7995 - accuracy: 0.7338\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-quantum",
   "metadata": {},
   "source": [
    "# 10. Regularización\n",
    "La idea de la regularización es la de limitar aquellos pesos que son altos. De esta forma, se agrega una capa previa a la capa densa que contiene la capa de regularización. Se probarán dos regularizaciones distintas: L1 y L2 (donde el número significa el grado del término adicional que se suma a la función de costo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-packet",
   "metadata": {},
   "source": [
    "# 10.1. Regularización L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "central-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "given-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "utility-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 4s 167ms/step - loss: 0.6135 - auc: 0.7668 - accuracy: 0.6893 - val_loss: 0.5625 - val_auc: 0.7961 - val_accuracy: 0.7351\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5688 - auc: 0.7837 - accuracy: 0.7185 - val_loss: 0.5278 - val_auc: 0.8186 - val_accuracy: 0.7568\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5485 - auc: 0.8231 - accuracy: 0.7705 - val_loss: 0.5043 - val_auc: 0.8331 - val_accuracy: 0.7784\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5252 - auc: 0.8079 - accuracy: 0.7454 - val_loss: 0.4891 - val_auc: 0.8433 - val_accuracy: 0.7730\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5331 - auc: 0.8031 - accuracy: 0.7341 - val_loss: 0.4787 - val_auc: 0.8514 - val_accuracy: 0.7838\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5021 - auc: 0.8288 - accuracy: 0.7506 - val_loss: 0.4718 - val_auc: 0.8565 - val_accuracy: 0.7838\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4908 - auc: 0.8343 - accuracy: 0.7569 - val_loss: 0.4655 - val_auc: 0.8595 - val_accuracy: 0.7892\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4832 - auc: 0.8338 - accuracy: 0.7568 - val_loss: 0.4613 - val_auc: 0.8597 - val_accuracy: 0.7946\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4468 - auc: 0.8646 - accuracy: 0.7916 - val_loss: 0.4584 - val_auc: 0.8623 - val_accuracy: 0.8000\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4666 - auc: 0.8436 - accuracy: 0.7785 - val_loss: 0.4570 - val_auc: 0.8642 - val_accuracy: 0.8000\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4706 - auc: 0.8425 - accuracy: 0.7899 - val_loss: 0.4549 - val_auc: 0.8642 - val_accuracy: 0.8000\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4827 - auc: 0.8405 - accuracy: 0.7577 - val_loss: 0.4534 - val_auc: 0.8647 - val_accuracy: 0.8054\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4864 - auc: 0.8381 - accuracy: 0.7657 - val_loss: 0.4522 - val_auc: 0.8653 - val_accuracy: 0.7946\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4731 - auc: 0.8434 - accuracy: 0.7872 - val_loss: 0.4513 - val_auc: 0.8662 - val_accuracy: 0.7946\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4732 - auc: 0.8413 - accuracy: 0.7888 - val_loss: 0.4507 - val_auc: 0.8662 - val_accuracy: 0.8000\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4696 - auc: 0.8426 - accuracy: 0.7812 - val_loss: 0.4501 - val_auc: 0.8676 - val_accuracy: 0.8000\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4619 - auc: 0.8508 - accuracy: 0.7640 - val_loss: 0.4508 - val_auc: 0.8670 - val_accuracy: 0.7946\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4747 - auc: 0.8533 - accuracy: 0.7734 - val_loss: 0.4501 - val_auc: 0.8668 - val_accuracy: 0.8000\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5007 - auc: 0.8196 - accuracy: 0.7355 - val_loss: 0.4495 - val_auc: 0.8666 - val_accuracy: 0.7892\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4896 - auc: 0.8417 - accuracy: 0.7556 - val_loss: 0.4488 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8418 - accuracy: 0.7831 - val_loss: 0.4489 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4767 - auc: 0.8384 - accuracy: 0.7663 - val_loss: 0.4489 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8387 - accuracy: 0.7831 - val_loss: 0.4491 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4659 - auc: 0.8442 - accuracy: 0.7599 - val_loss: 0.4497 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8374 - accuracy: 0.7700 - val_loss: 0.4498 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - auc: 0.8377 - accuracy: 0.7859 - val_loss: 0.4502 - val_auc: 0.8648 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9fa675e50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 Regularization\n",
    "l1_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l1(0.01)))\n",
    "# Compiling model\n",
    "l1_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "interim-throw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5080 - auc: 0.8003 - accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "l1_model = load_model(mc_path)\n",
    "eval = l1_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-individual",
   "metadata": {},
   "source": [
    "# 10.2. Regularización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "recovered-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "focused-rouge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 149ms/step - loss: 0.5800 - auc: 0.7794 - accuracy: 0.6823 - val_loss: 0.5462 - val_auc: 0.8198 - val_accuracy: 0.7243\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5913 - auc: 0.7541 - accuracy: 0.6792 - val_loss: 0.5136 - val_auc: 0.8350 - val_accuracy: 0.7730\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5301 - auc: 0.8090 - accuracy: 0.7368 - val_loss: 0.4935 - val_auc: 0.8463 - val_accuracy: 0.7784\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5054 - auc: 0.8332 - accuracy: 0.7575 - val_loss: 0.4809 - val_auc: 0.8519 - val_accuracy: 0.7946\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4823 - auc: 0.8402 - accuracy: 0.7837 - val_loss: 0.4718 - val_auc: 0.8561 - val_accuracy: 0.7892\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8443 - accuracy: 0.7918 - val_loss: 0.4654 - val_auc: 0.8590 - val_accuracy: 0.7892\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5068 - auc: 0.8171 - accuracy: 0.7519 - val_loss: 0.4612 - val_auc: 0.8605 - val_accuracy: 0.7784\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8225 - accuracy: 0.7625 - val_loss: 0.4586 - val_auc: 0.8608 - val_accuracy: 0.7730\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4942 - auc: 0.8236 - accuracy: 0.7532 - val_loss: 0.4562 - val_auc: 0.8631 - val_accuracy: 0.7730\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4817 - auc: 0.8365 - accuracy: 0.7519 - val_loss: 0.4545 - val_auc: 0.8635 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8327 - accuracy: 0.7565 - val_loss: 0.4529 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4635 - auc: 0.8502 - accuracy: 0.7790 - val_loss: 0.4519 - val_auc: 0.8646 - val_accuracy: 0.7730\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4815 - auc: 0.8244 - accuracy: 0.7607 - val_loss: 0.4509 - val_auc: 0.8637 - val_accuracy: 0.7730\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4778 - auc: 0.8286 - accuracy: 0.7512 - val_loss: 0.4506 - val_auc: 0.8639 - val_accuracy: 0.7784\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4876 - auc: 0.8266 - accuracy: 0.7690 - val_loss: 0.4497 - val_auc: 0.8636 - val_accuracy: 0.7784\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4974 - auc: 0.8264 - accuracy: 0.7544 - val_loss: 0.4491 - val_auc: 0.8641 - val_accuracy: 0.7784\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4879 - auc: 0.8278 - accuracy: 0.7646 - val_loss: 0.4486 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8385 - accuracy: 0.7572 - val_loss: 0.4490 - val_auc: 0.8633 - val_accuracy: 0.7730\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4786 - auc: 0.8277 - accuracy: 0.7506 - val_loss: 0.4491 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4501 - auc: 0.8537 - accuracy: 0.7808 - val_loss: 0.4487 - val_auc: 0.8644 - val_accuracy: 0.7730\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4554 - auc: 0.8466 - accuracy: 0.7850 - val_loss: 0.4483 - val_auc: 0.8653 - val_accuracy: 0.7784\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4787 - auc: 0.8403 - accuracy: 0.7660 - val_loss: 0.4484 - val_auc: 0.8641 - val_accuracy: 0.7838\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4651 - auc: 0.8491 - accuracy: 0.7745 - val_loss: 0.4480 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5009 - auc: 0.8068 - accuracy: 0.7558 - val_loss: 0.4475 - val_auc: 0.8655 - val_accuracy: 0.7784\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4796 - auc: 0.8355 - accuracy: 0.7705 - val_loss: 0.4476 - val_auc: 0.8657 - val_accuracy: 0.7838\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4464 - auc: 0.8575 - accuracy: 0.8017 - val_loss: 0.4475 - val_auc: 0.8656 - val_accuracy: 0.7784\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4749 - auc: 0.8444 - accuracy: 0.7888 - val_loss: 0.4479 - val_auc: 0.8660 - val_accuracy: 0.7730\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4720 - auc: 0.8436 - accuracy: 0.7732 - val_loss: 0.4476 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4656 - auc: 0.8443 - accuracy: 0.7801 - val_loss: 0.4475 - val_auc: 0.8658 - val_accuracy: 0.7838\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4737 - auc: 0.8424 - accuracy: 0.7758 - val_loss: 0.4476 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4883 - auc: 0.8185 - accuracy: 0.7766 - val_loss: 0.4479 - val_auc: 0.8643 - val_accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9f9000070>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L2 Regularization\n",
    "l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "infinite-central",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5099 - auc: 0.7995 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "l2_model = load_model(mc_path)\n",
    "eval = l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-checklist",
   "metadata": {},
   "source": [
    "# 10.3. Regularización L1+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "persistent-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1+L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "favorite-letter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 144ms/step - loss: 0.7301 - auc: 0.5565 - accuracy: 0.5732 - val_loss: 0.6605 - val_auc: 0.6425 - val_accuracy: 0.6324\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6248 - auc: 0.6923 - accuracy: 0.6720 - val_loss: 0.5839 - val_auc: 0.7455 - val_accuracy: 0.6811\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5481 - auc: 0.7881 - accuracy: 0.7107 - val_loss: 0.5380 - val_auc: 0.8003 - val_accuracy: 0.7135\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5596 - auc: 0.7826 - accuracy: 0.7085 - val_loss: 0.5087 - val_auc: 0.8266 - val_accuracy: 0.7297\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5237 - auc: 0.8076 - accuracy: 0.7277 - val_loss: 0.4908 - val_auc: 0.8426 - val_accuracy: 0.7622\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5127 - auc: 0.8181 - accuracy: 0.7382 - val_loss: 0.4783 - val_auc: 0.8524 - val_accuracy: 0.7568\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5275 - auc: 0.8139 - accuracy: 0.71 - 0s 3ms/step - loss: 0.4998 - auc: 0.8240 - accuracy: 0.7488 - val_loss: 0.4708 - val_auc: 0.8555 - val_accuracy: 0.7730\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4838 - auc: 0.8338 - accuracy: 0.7478 - val_loss: 0.4648 - val_auc: 0.8598 - val_accuracy: 0.7892\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4653 - auc: 0.8513 - accuracy: 0.7929 - val_loss: 0.4610 - val_auc: 0.8619 - val_accuracy: 0.7892\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4932 - auc: 0.8305 - accuracy: 0.7573 - val_loss: 0.4580 - val_auc: 0.8632 - val_accuracy: 0.7946\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5071 - auc: 0.8143 - accuracy: 0.7651 - val_loss: 0.4556 - val_auc: 0.8629 - val_accuracy: 0.7946\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4661 - auc: 0.8507 - accuracy: 0.7817 - val_loss: 0.4542 - val_auc: 0.8650 - val_accuracy: 0.7946\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4627 - auc: 0.8612 - accuracy: 0.7737 - val_loss: 0.4523 - val_auc: 0.8649 - val_accuracy: 0.7946\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4660 - auc: 0.8447 - accuracy: 0.7824 - val_loss: 0.4511 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4790 - auc: 0.8260 - accuracy: 0.7835 - val_loss: 0.4502 - val_auc: 0.8656 - val_accuracy: 0.8000\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5253 - auc: 0.8148 - accuracy: 0.7272 - val_loss: 0.4494 - val_auc: 0.8654 - val_accuracy: 0.7946\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8312 - accuracy: 0.7764 - val_loss: 0.4488 - val_auc: 0.8660 - val_accuracy: 0.8000\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4520 - auc: 0.8499 - accuracy: 0.7913 - val_loss: 0.4489 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4573 - auc: 0.8538 - accuracy: 0.7985 - val_loss: 0.4490 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4816 - auc: 0.8313 - accuracy: 0.7763 - val_loss: 0.4490 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4561 - auc: 0.8449 - accuracy: 0.7898 - val_loss: 0.4494 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4667 - auc: 0.8434 - accuracy: 0.7791 - val_loss: 0.4490 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4744 - auc: 0.8443 - accuracy: 0.7682 - val_loss: 0.4488 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4293 - auc: 0.8694 - accuracy: 0.8025 - val_loss: 0.4484 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4961 - auc: 0.8155 - accuracy: 0.7677 - val_loss: 0.4483 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4840 - auc: 0.8355 - accuracy: 0.7625 - val_loss: 0.4480 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4558 - auc: 0.8506 - accuracy: 0.7943 - val_loss: 0.4483 - val_auc: 0.8656 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9ff571af0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 and L2 Regularization\n",
    "l1l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l1l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "finished-identity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 1ms/step - loss: 0.5046 - auc: 0.8022 - accuracy: 0.7338\n"
     ]
    }
   ],
   "source": [
    "l1l2_model = load_model(mc_path)\n",
    "eval = l1l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-chance",
   "metadata": {},
   "source": [
    "En este caso, se nota una leve mejora en la métrica empleando regularización L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-material",
   "metadata": {},
   "source": [
    "# 11. Dropout\n",
    "Se emplea una capa extra de dropout para minimizar el overfitting. Este regularizador funciona ignorando a neuronas de forma aleatoria. Se realiza dropout **solo en la etapa de entrenamiento**. **En teoría, no se lleva muy bien con la normalización por capas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "developing-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "diagnostic-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/dropout_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "lonely-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4461 - auc: 0.8675 - accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44613930583000183, 0.8675214052200317, 0.7891891598701477]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model\n",
    "do_model = Sequential()\n",
    "# Adding dropout layer to network\n",
    "do_model.add(Dropout(0))\n",
    "# Adding Dense layer\n",
    "do_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "do_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "do_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "do_model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "packed-detective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4464 - auc: 0.8649 - accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4463537633419037, 0.8648818135261536, 0.7837837934494019]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-scout",
   "metadata": {},
   "source": [
    "# 12. Feature Engineering. Features Polinomiales\n",
    "El objertivo de esta sección es el de agregar variables de entrada al modelo, que surgen de combinar las variables originales. El grado del polinomio determina la cantidad de nuevas variables que se suman al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "japanese-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/poly2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "played-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 98ms/step - loss: 0.8307 - auc: 0.5132 - accuracy: 0.5522 - val_loss: 0.6360 - val_auc: 0.6907 - val_accuracy: 0.6649\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5942 - auc: 0.7201 - accuracy: 0.6942 - val_loss: 0.5561 - val_auc: 0.7600 - val_accuracy: 0.6919\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5643 - auc: 0.7597 - accuracy: 0.7174 - val_loss: 0.5243 - val_auc: 0.7900 - val_accuracy: 0.7135\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4851 - auc: 0.8261 - accuracy: 0.7568 - val_loss: 0.5107 - val_auc: 0.8067 - val_accuracy: 0.7351\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4593 - auc: 0.8503 - accuracy: 0.7619 - val_loss: 0.5097 - val_auc: 0.8104 - val_accuracy: 0.7514\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4311 - auc: 0.8654 - accuracy: 0.7998 - val_loss: 0.4877 - val_auc: 0.8347 - val_accuracy: 0.7676\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4394 - auc: 0.8623 - accuracy: 0.7778 - val_loss: 0.4822 - val_auc: 0.8394 - val_accuracy: 0.7622\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4128 - auc: 0.8901 - accuracy: 0.8012 - val_loss: 0.4767 - val_auc: 0.8413 - val_accuracy: 0.7622\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4432 - auc: 0.8580 - accuracy: 0.7766 - val_loss: 0.4828 - val_auc: 0.8378 - val_accuracy: 0.7676\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4160 - auc: 0.8719 - accuracy: 0.7898 - val_loss: 0.4877 - val_auc: 0.8342 - val_accuracy: 0.7622\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4112 - auc: 0.8854 - accuracy: 0.7950 - val_loss: 0.4869 - val_auc: 0.8362 - val_accuracy: 0.7676\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3974 - auc: 0.8839 - accuracy: 0.8067 - val_loss: 0.4842 - val_auc: 0.8415 - val_accuracy: 0.7784\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4536 - auc: 0.8529 - accuracy: 0.7476 - val_loss: 0.4864 - val_auc: 0.8389 - val_accuracy: 0.7568\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4190 - auc: 0.8702 - accuracy: 0.7858 - val_loss: 0.4827 - val_auc: 0.8451 - val_accuracy: 0.7459\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4126 - auc: 0.8880 - accuracy: 0.7879 - val_loss: 0.4868 - val_auc: 0.8377 - val_accuracy: 0.7514\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4315 - auc: 0.8579 - accuracy: 0.7801 - val_loss: 0.4841 - val_auc: 0.8429 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4364 - auc: 0.8660 - accuracy: 0.7785 - val_loss: 0.4871 - val_auc: 0.8406 - val_accuracy: 0.7568\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3887 - auc: 0.8952 - accuracy: 0.8055 - val_loss: 0.4839 - val_auc: 0.8454 - val_accuracy: 0.7568\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4137 - auc: 0.8771 - accuracy: 0.7817 - val_loss: 0.4860 - val_auc: 0.8440 - val_accuracy: 0.7514\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4026 - auc: 0.8831 - accuracy: 0.7872 - val_loss: 0.4866 - val_auc: 0.8426 - val_accuracy: 0.7568\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4113 - auc: 0.8743 - accuracy: 0.7909 - val_loss: 0.4914 - val_auc: 0.8399 - val_accuracy: 0.7459\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4261 - auc: 0.8731 - accuracy: 0.7886 - val_loss: 0.4908 - val_auc: 0.8407 - val_accuracy: 0.7514\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3918 - auc: 0.8850 - accuracy: 0.7879 - val_loss: 0.4917 - val_auc: 0.8395 - val_accuracy: 0.7568\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3845 - auc: 0.8889 - accuracy: 0.8049 - val_loss: 0.4911 - val_auc: 0.8419 - val_accuracy: 0.7514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a985abfd90>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating polynomial features\n",
    "poly2 = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly2.fit(x_train)\n",
    "# Creating model\n",
    "p2_model  = Sequential()\n",
    "p2_model.add(Dense(1, input_shape=(poly2.n_output_features_,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "p2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Fit model\n",
    "p2_model.fit(poly2.transform(x_train), y_train, validation_data=(poly2.transform(x_valid), y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "structured-yesterday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5972 - auc: 0.7520 - accuracy: 0.7143\n"
     ]
    }
   ],
   "source": [
    "p2_model = load_model(mc_path)\n",
    "eval = p2_model.evaluate(x=poly2.transform(x_test), y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-hopkins",
   "metadata": {},
   "source": [
    "A continuación se observa la progresión de la métrica en **train** y **valid** en función del grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "agreed-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-sessions",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: Realizar la normalización de los datos **después** de aplicar el feature polinomial, sino se rompe todo :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "coupled-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polynomial degrees to train and compute metrics\n",
    "poly_degrees = np.arange(1, 11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "composed-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LRS callback\n",
    "# Define learning rate at start\n",
    "ilr = 0.1 # ilr=0.5, ds = 100000, dr=0.8, stc=False\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=1000, decay_rate=0.8, staircase=True) # Decay every (decay_steps) steps with a base of (decay_rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fabulous-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model Checkpoint callback.\n",
    "mc_path = 'model_checkpoints/get_best_poly_deg_checkpoint.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-motor",
   "metadata": {},
   "source": [
    "En este punto cabe aclarar que se probó el parámetro *interaction_only* del preprocesador de polinomios y se llegó a la conclusión de que el desempeño mejora con este valor en *True*. Esto es así dado que, al activarlo, se logra un número mucho menor de variables en cada orden. Esto contribuye ampliamente a **reducir el overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ordered-array",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Polynomial order = 1 ---\n",
      "Input count = 8\n",
      "AUC for TRAIN subset is 0.8401\n",
      "AUC for VALID subset is 0.8671\n",
      "--- Polynomial order = 2 ---\n",
      "Input count = 36\n",
      "AUC for TRAIN subset is 0.8536\n",
      "AUC for VALID subset is 0.8484\n",
      "--- Polynomial order = 3 ---\n",
      "Input count = 92\n",
      "AUC for TRAIN subset is 0.8438\n",
      "AUC for VALID subset is 0.8355\n",
      "--- Polynomial order = 4 ---\n",
      "Input count = 162\n",
      "AUC for TRAIN subset is 0.9185\n",
      "AUC for VALID subset is 0.8101\n",
      "--- Polynomial order = 5 ---\n",
      "Input count = 218\n",
      "AUC for TRAIN subset is 0.8819\n",
      "AUC for VALID subset is 0.8073\n",
      "--- Polynomial order = 6 ---\n",
      "Input count = 246\n",
      "AUC for TRAIN subset is 0.8911\n",
      "AUC for VALID subset is 0.7925\n",
      "--- Polynomial order = 7 ---\n",
      "Input count = 254\n",
      "AUC for TRAIN subset is 0.8367\n",
      "AUC for VALID subset is 0.7972\n",
      "--- Polynomial order = 8 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.8530\n",
      "AUC for VALID subset is 0.8304\n",
      "--- Polynomial order = 9 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9409\n",
      "AUC for VALID subset is 0.7809\n",
      "--- Polynomial order = 10 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9192\n",
      "AUC for VALID subset is 0.8095\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=True)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    \n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "painted-gossip",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
