{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beginning-grace",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 2 - Ejercicio 1 - Regresión Logística\n",
    "# Notebook #2: Implementación de una Regresión Lineal\n",
    "En esta notebook se busca implementar una regresión logística para poder estimar la condición de diabético de un paciente, perteneciente al Pima Indians Dataset analizado en la notebook anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-impression",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "* Chequear correcto reemplazo de NaN por mean.\n",
    "* Meter el z-score en scripts comunes a ambos ejercicios. Chequear StandardScaler **correctamente inicializado**. **¿Errores de discretización?**\n",
    "    * ¿Dónde meto el área bajo la curva ROC y el F2? -> Respondido por Luqui y Karina.\n",
    "* Añadir **tensorboard** para log entre epochs. Migrar **TODOS LOS GRÁFICOS** a TensorBoard.\n",
    "    * Agregar evolución de f2-score sobre train en selección del umbral.\n",
    "* Graficar **learning rate**.\n",
    "* Sacar los evaluate con **test**, para evitar malas interpretaciones.\n",
    "* PRIMERA PRUEBA DE POLY (2) ESTÁ MAL! **Falta normalizar despues del poly**\n",
    "* Informar métricas secundarias\n",
    "* ¿Kernel/Activity regulariizer? -> **kernel regularizer** afecta a los pesos, **activity regularizer** a las salidas.\n",
    "\n",
    "# ¿Qué cosas puedo variar?\n",
    "* Función de activación:\n",
    "    * Sigmoid\n",
    "    * RELU\n",
    "    * ELU\n",
    "    * tanh\n",
    "    * Leaky RELU\n",
    "    \n",
    "* Optimizador:\n",
    "    * SGD\n",
    "    * Adam\n",
    "    \n",
    "* Early Stopping: Para el entrenamiento cuando la **loss** deja de mejorar. Se pasa a través de un **callback**. (https://keras.io/api/callbacks/early_stopping/) (https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "* Kernel Initializer: Esto es, como son los pesos y bias iniciales. (https://keras.io/api/layers/initializers/)\n",
    "* Model Checkpoint: Guarda un checkpoint del modelo. Puede configurarse para elegir el mejor. Se pasa por **callback**. (https://keras.io/api/callbacks/model_checkpoint/)\n",
    "* Scheduling Learning Rate: Se hace variar el **learning rate** con una función. Es un **callback**. (https://keras.io/api/callbacks/learning_rate_scheduler/)\n",
    "* Reg. dropout: Para evitar overfitting, la capa de dropout \"borra\" una entrada de forma aleatoria y escala el resto. Es una **capa**. (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "* Regularización L1 y L2: Limita el espacio de soluciones agregando un término a la **función de costo**. (https://keras.io/api/layers/regularizers/)\n",
    "* Data Augmentation\n",
    "* Batch Normalization: Normaliza las entradas (media=0, dev=1). Es una **capa**. (https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-saskatchewan",
   "metadata": {},
   "source": [
    "# Dudas\n",
    "* Al generar la métrica F2, ¿me devuelve por batch o por epoch? -> Esto finalmente se explica más adelante.\n",
    "* Al evaluar el predict en threshold selection ¿batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-diamond",
   "metadata": {},
   "source": [
    "# ¿Cuáles son los requerimientos para el **clasificador**?\n",
    "* Métrica principal: **Área bajo la curva ROC**\n",
    "* Buscar el **umbral de decisión** para maximizar el **f2 score** \n",
    "* Informar métricas secundarias:\n",
    "    * Especificidad - Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "    * Sensibilidad\n",
    "    * Valor predictivo positivo\n",
    "    * Valor predictivo negativo\n",
    "    \n",
    "* **Pregunta adicional**:\n",
    "Dada la situación en la cual cambia la prevalencia de la enfermedad en la población a ser del 20%. Se desea reutilizar el modelo sin volver a entrenar, ¿Cómo lo harían? ¿Qué métricas se mantienen igual y cuáles cambiarian?. **¿clases desbalanceadas -> class weight?**. Las f-score son buenas para casos no balanceados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-permission",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "forbidden-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "owned-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polished-return",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read database from .csv\n",
    "df = pd.read_csv('../../databases/diabetes.csv', delimiter=',')\n",
    "\n",
    "# Show first rows of data\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-beatles",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-present",
   "metadata": {},
   "source": [
    "## 2.1 Filtrado de valores inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "antique-slovenia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>733.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.153420</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>12.382158</td>\n",
       "      <td>10.476982</td>\n",
       "      <td>118.775855</td>\n",
       "      <td>6.924988</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  763.000000     733.000000     541.000000  394.000000   \n",
       "mean      3.845052  121.686763      72.405184      29.153420  155.548223   \n",
       "std       3.369578   30.535641      12.382158      10.476982  118.775855   \n",
       "min       0.000000   44.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   76.250000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  125.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  190.000000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  757.000000                768.000000  768.000000  768.000000  \n",
       "mean    32.457464                  0.471876   33.240885    0.348958  \n",
       "std      6.924988                  0.331329   11.760232    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.500000                  0.243750   24.000000    0.000000  \n",
       "50%     32.300000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering Glucose values\n",
    "df['Glucose'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Blood Pressure values\n",
    "df['BloodPressure'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Skin Thickness values\n",
    "df['SkinThickness'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Insulin values\n",
    "df['Insulin'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Body Mass Index values\n",
    "df['BMI'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-necessity",
   "metadata": {},
   "source": [
    "## 2.2 Remoción de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fifty-bread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from src.helper import remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "single-poker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>764.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>759.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.786649</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.115438</td>\n",
       "      <td>28.903346</td>\n",
       "      <td>132.610811</td>\n",
       "      <td>32.204005</td>\n",
       "      <td>0.429832</td>\n",
       "      <td>32.805007</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.278714</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>11.239072</td>\n",
       "      <td>9.865480</td>\n",
       "      <td>74.285393</td>\n",
       "      <td>6.491385</td>\n",
       "      <td>0.249684</td>\n",
       "      <td>11.113182</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   764.000000  763.000000     719.000000     538.000000  370.000000   \n",
       "mean      3.786649  121.686763      72.115438      28.903346  132.610811   \n",
       "std       3.278714   30.535641      11.239072       9.865480   74.285393   \n",
       "min       0.000000   44.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   75.000000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  120.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  177.500000   \n",
       "max      13.000000  199.000000     104.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  749.000000                739.000000  759.000000  768.000000  \n",
       "mean    32.204005                  0.429832   32.805007    0.348958  \n",
       "std      6.491385                  0.249684   11.113182    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.400000                  0.238000   24.000000    0.000000  \n",
       "50%     32.000000                  0.356000   29.000000    0.000000  \n",
       "75%     36.500000                  0.587000   40.000000    1.000000  \n",
       "max     50.000000                  1.191000   66.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "y_labels = ['Outcome']\n",
    "\n",
    "for column in x_labels:\n",
    "    remove_outliers(df, column)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-legislation",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alpine-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "remarkable-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "entertaining-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output variables for the model\n",
    "df_x = df[x_labels]\n",
    "df_y = df[y_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "usual-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Split the train_valid sub-dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indonesian-knowing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>425.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.896471</td>\n",
       "      <td>122.360000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.430713</td>\n",
       "      <td>32.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.269876</td>\n",
       "      <td>30.066982</td>\n",
       "      <td>10.805353</td>\n",
       "      <td>9.793471</td>\n",
       "      <td>70.854164</td>\n",
       "      <td>6.341281</td>\n",
       "      <td>0.252835</td>\n",
       "      <td>10.681080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.700000</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   425.000000  425.000000     403.000000     291.000000  205.000000   \n",
       "mean      3.896471  122.360000      71.665012      28.862543  134.414634   \n",
       "std       3.269876   30.066982      10.805353       9.793471   70.854164   \n",
       "min       0.000000   44.000000      40.000000       8.000000   18.000000   \n",
       "25%       1.000000  100.000000      64.000000      21.500000   76.000000   \n",
       "50%       3.000000  118.000000      72.000000      28.000000  125.000000   \n",
       "75%       6.000000  142.000000      78.000000      36.000000  180.000000   \n",
       "max      13.000000  199.000000     104.000000      52.000000  328.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  417.000000                411.000000  425.000000  \n",
       "mean    32.166667                  0.430713   32.494118  \n",
       "std      6.341281                  0.252835   10.681080  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     27.700000                  0.235500   24.000000  \n",
       "50%     31.600000                  0.355000   29.000000  \n",
       "75%     36.300000                  0.600500   39.000000  \n",
       "max     49.600000                  1.189000   66.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set before NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-vaccine",
   "metadata": {},
   "source": [
    "# 4. Reemplazo de valores inválidos\n",
    "Como se destacó en el análisis estadístico de datos, el dataset suministrado posee varios valores faltantes en algunos individuos. Se asume que en la etapa de producción el modelo contará con todas las variables correctamente informadas, no admitiendo el faltante de alguna de ellas. Luego, se decide reemplazar aquellos valores inválidos en **train**, **valid** y **test** por la correspondiente media en el dataset de train. En este caso, se considera a la media como un estimador correcto para la ocasión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "municipal-account",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of training\n",
    "train_means = x_train.mean().to_numpy()\n",
    "\n",
    "# Replacing nan values of the train dataset with training mean values\n",
    "for index, column in enumerate(x_train.columns):\n",
    "    x_train.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "\n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_test.columns):\n",
    "    x_test.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "    \n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_valid.columns):\n",
    "    x_valid.loc[:,column].replace(np.nan, train_means[index], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respective-strength",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.896471</td>\n",
       "      <td>122.360000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.430713</td>\n",
       "      <td>32.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.254560</td>\n",
       "      <td>29.926152</td>\n",
       "      <td>10.472012</td>\n",
       "      <td>8.061461</td>\n",
       "      <td>48.916861</td>\n",
       "      <td>6.251752</td>\n",
       "      <td>0.247461</td>\n",
       "      <td>10.631051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   429.000000  429.000000     429.000000     429.000000  429.000000   \n",
       "mean      3.896471  122.360000      71.665012      28.862543  134.414634   \n",
       "std       3.254560   29.926152      10.472012       8.061461   48.916861   \n",
       "min       0.000000   44.000000      40.000000       8.000000   18.000000   \n",
       "25%       1.000000  100.000000      64.000000      25.000000  129.000000   \n",
       "50%       3.000000  119.000000      71.665012      28.862543  134.414634   \n",
       "75%       6.000000  141.000000      78.000000      32.000000  134.414634   \n",
       "max      13.000000  199.000000     104.000000      52.000000  328.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  429.000000                429.000000  429.000000  \n",
       "mean    32.166667                  0.430713   32.494118  \n",
       "std      6.251752                  0.247461   10.631051  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     27.800000                  0.238000   24.000000  \n",
       "50%     32.000000                  0.371000   29.000000  \n",
       "75%     36.000000                  0.591000   39.000000  \n",
       "max     49.600000                  1.189000   66.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set after NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "massive-hearts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.691892</td>\n",
       "      <td>119.850595</td>\n",
       "      <td>73.313406</td>\n",
       "      <td>28.960620</td>\n",
       "      <td>131.659328</td>\n",
       "      <td>32.292252</td>\n",
       "      <td>0.426025</td>\n",
       "      <td>33.678283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.284878</td>\n",
       "      <td>30.084793</td>\n",
       "      <td>12.045479</td>\n",
       "      <td>8.215413</td>\n",
       "      <td>51.838933</td>\n",
       "      <td>6.738731</td>\n",
       "      <td>0.245125</td>\n",
       "      <td>11.670442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.542000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.159000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   185.000000  185.000000     185.000000     185.000000  185.000000   \n",
       "mean      3.691892  119.850595      73.313406      28.960620  131.659328   \n",
       "std       3.284878   30.084793      12.045479       8.215413   51.838933   \n",
       "min       0.000000   57.000000      44.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      65.000000      26.000000  116.000000   \n",
       "50%       3.000000  114.000000      72.000000      28.862543  134.414634   \n",
       "75%       6.000000  136.000000      82.000000      32.000000  134.414634   \n",
       "max      13.000000  198.000000     100.000000      54.000000  335.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  185.000000                185.000000  185.000000  \n",
       "mean    32.292252                  0.426025   33.678283  \n",
       "std      6.738731                  0.245125   11.670442  \n",
       "min     19.600000                  0.096000   21.000000  \n",
       "25%     26.600000                  0.241000   24.000000  \n",
       "50%     32.500000                  0.365000   30.000000  \n",
       "75%     36.500000                  0.542000   42.000000  \n",
       "max     50.000000                  1.159000   66.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set after NaN replacement\n",
    "x_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "crucial-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.597403</td>\n",
       "      <td>122.038961</td>\n",
       "      <td>71.787761</td>\n",
       "      <td>28.887267</td>\n",
       "      <td>133.390719</td>\n",
       "      <td>32.197403</td>\n",
       "      <td>0.432119</td>\n",
       "      <td>32.603820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.304818</td>\n",
       "      <td>32.320876</td>\n",
       "      <td>10.448535</td>\n",
       "      <td>8.867500</td>\n",
       "      <td>58.146512</td>\n",
       "      <td>6.484578</td>\n",
       "      <td>0.238998</td>\n",
       "      <td>11.431621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>26.925000</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>142.750000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>40.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>49.300000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   154.000000  154.000000     154.000000     154.000000  154.000000   \n",
       "mean      3.597403  122.038961      71.787761      28.887267  133.390719   \n",
       "std       3.304818   32.320876      10.448535       8.867500   58.146512   \n",
       "min       0.000000   61.000000      44.000000       7.000000   23.000000   \n",
       "25%       1.000000   95.250000      64.000000      23.250000  108.250000   \n",
       "50%       3.000000  117.000000      71.665012      28.862543  134.414634   \n",
       "75%       5.750000  142.750000      79.500000      33.000000  134.414634   \n",
       "max      13.000000  197.000000      94.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  154.000000                154.000000  154.000000  \n",
       "mean    32.197403                  0.432119   32.603820  \n",
       "std      6.484578                  0.238998   11.431621  \n",
       "min     18.400000                  0.078000   21.000000  \n",
       "25%     26.925000                  0.254000   24.000000  \n",
       "50%     32.166667                  0.376500   28.000000  \n",
       "75%     36.625000                  0.567000   40.750000  \n",
       "max     49.300000                  1.191000   66.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set after NaN replacement\n",
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-norway",
   "metadata": {},
   "source": [
    "# 5. Normalización de datos de entrada. Z Score. \n",
    "Dado que todas las variables en juego son numéricas, se puede aplicar z-score a todo el dataset. Esta operación se hace con el objetivo de poder obtener mayor información de los pesos calculados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "nasty-charge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! Backup unnormalized subsets for further utilization\n",
    "x_train_un = x_train\n",
    "x_valid_un = x_valid\n",
    "x_test_un = x_test\n",
    "\n",
    "# Apply z-score to all sub-datasets\n",
    "scalable_variables = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])\n",
    "    x_valid.loc[:, scalable_variables] = scaler.transform(x_valid.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "reverse-watts",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.140692e-17</td>\n",
       "      <td>2.484415e-17</td>\n",
       "      <td>-3.498885e-16</td>\n",
       "      <td>3.685216e-16</td>\n",
       "      <td>-5.010237e-16</td>\n",
       "      <td>3.457478e-16</td>\n",
       "      <td>1.242208e-16</td>\n",
       "      <td>1.780498e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.198632e+00</td>\n",
       "      <td>-2.621503e+00</td>\n",
       "      <td>-3.027306e+00</td>\n",
       "      <td>-2.590957e+00</td>\n",
       "      <td>-2.382625e+00</td>\n",
       "      <td>-2.236649e+00</td>\n",
       "      <td>-1.402716e+00</td>\n",
       "      <td>-1.082446e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.910121e-01</td>\n",
       "      <td>-7.480449e-01</td>\n",
       "      <td>-7.328068e-01</td>\n",
       "      <td>-4.796963e-01</td>\n",
       "      <td>-1.108198e-01</td>\n",
       "      <td>-6.992863e-01</td>\n",
       "      <td>-7.796693e-01</td>\n",
       "      <td>-7.999242e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.757722e-01</td>\n",
       "      <td>-1.124075e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.412180e-16</td>\n",
       "      <td>-5.816991e-16</td>\n",
       "      <td>-2.669032e-02</td>\n",
       "      <td>-2.415838e-01</td>\n",
       "      <td>-3.290547e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.470876e-01</td>\n",
       "      <td>6.235938e-01</td>\n",
       "      <td>6.056510e-01</td>\n",
       "      <td>3.896465e-01</td>\n",
       "      <td>-5.816991e-16</td>\n",
       "      <td>6.138773e-01</td>\n",
       "      <td>6.484824e-01</td>\n",
       "      <td>6.126843e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.800427e+00</td>\n",
       "      <td>2.563961e+00</td>\n",
       "      <td>3.091358e+00</td>\n",
       "      <td>2.873483e+00</td>\n",
       "      <td>3.962057e+00</td>\n",
       "      <td>2.791807e+00</td>\n",
       "      <td>3.067844e+00</td>\n",
       "      <td>3.155380e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pregnancies       Glucose  BloodPressure  SkinThickness       Insulin  \\\n",
       "count  4.290000e+02  4.290000e+02   4.290000e+02   4.290000e+02  4.290000e+02   \n",
       "mean  -4.140692e-17  2.484415e-17  -3.498885e-16   3.685216e-16 -5.010237e-16   \n",
       "std    1.001168e+00  1.001168e+00   1.001168e+00   1.001168e+00  1.001168e+00   \n",
       "min   -1.198632e+00 -2.621503e+00  -3.027306e+00  -2.590957e+00 -2.382625e+00   \n",
       "25%   -8.910121e-01 -7.480449e-01  -7.328068e-01  -4.796963e-01 -1.108198e-01   \n",
       "50%   -2.757722e-01 -1.124075e-01   0.000000e+00   4.412180e-16 -5.816991e-16   \n",
       "75%    6.470876e-01  6.235938e-01   6.056510e-01   3.896465e-01 -5.816991e-16   \n",
       "max    2.800427e+00  2.563961e+00   3.091358e+00   2.873483e+00  3.962057e+00   \n",
       "\n",
       "                BMI  DiabetesPedigreeFunction           Age  \n",
       "count  4.290000e+02              4.290000e+02  4.290000e+02  \n",
       "mean   3.457478e-16              1.242208e-16  1.780498e-16  \n",
       "std    1.001168e+00              1.001168e+00  1.001168e+00  \n",
       "min   -2.236649e+00             -1.402716e+00 -1.082446e+00  \n",
       "25%   -6.992863e-01             -7.796693e-01 -7.999242e-01  \n",
       "50%   -2.669032e-02             -2.415838e-01 -3.290547e-01  \n",
       "75%    6.138773e-01              6.484824e-01  6.126843e-01  \n",
       "max    2.791807e+00              3.067844e+00  3.155380e+00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-fence",
   "metadata": {},
   "source": [
    "# 6. Regresión Logística - Test #1\n",
    "Primera prueba de regresión logística. Se usa SGD y AUC como métrica principal. Se emplea la Binary Cross-Entropy como loss subrogada, dado que **la AUC no es diferenciable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "affiliated-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TensorBoard for learning logging\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "loving-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hungarian-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import SensitivityAtSpecificity\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "owned-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/logistic_regression_first_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unavailable-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Get model brief\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "convertible-lease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics definition\n",
    "metrics = ['AUC', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "prescription-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 15s 142ms/step - loss: 0.7474 - auc: 0.6151 - accuracy: 0.5630 - val_loss: 0.7001 - val_auc: 0.6186 - val_accuracy: 0.5892\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7306 - auc: 0.5910 - accuracy: 0.5479 - val_loss: 0.6778 - val_auc: 0.6399 - val_accuracy: 0.5946\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7480 - auc: 0.5906 - accuracy: 0.5356 - val_loss: 0.6579 - val_auc: 0.6588 - val_accuracy: 0.6054\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.7083 - auc: 0.6212 - accuracy: 0.5788 - val_loss: 0.6410 - val_auc: 0.6749 - val_accuracy: 0.6378\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6870 - auc: 0.6382 - accuracy: 0.6093 - val_loss: 0.6251 - val_auc: 0.6922 - val_accuracy: 0.6432\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6588 - auc: 0.6707 - accuracy: 0.6145 - val_loss: 0.6117 - val_auc: 0.7068 - val_accuracy: 0.6595\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6642 - auc: 0.6675 - accuracy: 0.6250 - val_loss: 0.5996 - val_auc: 0.7216 - val_accuracy: 0.6811\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6111 - auc: 0.7230 - accuracy: 0.6711 - val_loss: 0.5894 - val_auc: 0.7327 - val_accuracy: 0.6919\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5918 - auc: 0.7546 - accuracy: 0.6821 - val_loss: 0.5801 - val_auc: 0.7424 - val_accuracy: 0.6919\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5828 - auc: 0.7564 - accuracy: 0.6860 - val_loss: 0.5720 - val_auc: 0.7523 - val_accuracy: 0.6919\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5774 - auc: 0.7604 - accuracy: 0.6926 - val_loss: 0.5647 - val_auc: 0.7607 - val_accuracy: 0.7027\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5910 - auc: 0.7541 - accuracy: 0.6749 - val_loss: 0.5582 - val_auc: 0.7674 - val_accuracy: 0.7189\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5698 - auc: 0.7718 - accuracy: 0.7187 - val_loss: 0.5523 - val_auc: 0.7735 - val_accuracy: 0.7189\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5396 - auc: 0.7996 - accuracy: 0.7456 - val_loss: 0.5470 - val_auc: 0.7794 - val_accuracy: 0.7243\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5595 - auc: 0.7809 - accuracy: 0.7273 - val_loss: 0.5425 - val_auc: 0.7839 - val_accuracy: 0.7243\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5598 - auc: 0.7807 - accuracy: 0.7125 - val_loss: 0.5382 - val_auc: 0.7899 - val_accuracy: 0.7189\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5622 - auc: 0.7692 - accuracy: 0.6935 - val_loss: 0.5344 - val_auc: 0.7935 - val_accuracy: 0.7297\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5574 - auc: 0.7914 - accuracy: 0.7321 - val_loss: 0.5312 - val_auc: 0.7963 - val_accuracy: 0.7405\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5280 - auc: 0.8046 - accuracy: 0.7193 - val_loss: 0.5279 - val_auc: 0.7996 - val_accuracy: 0.7459\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5076 - auc: 0.8236 - accuracy: 0.7273 - val_loss: 0.5252 - val_auc: 0.8022 - val_accuracy: 0.7459\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5131 - auc: 0.8242 - accuracy: 0.7524 - val_loss: 0.5228 - val_auc: 0.8069 - val_accuracy: 0.7459\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5340 - auc: 0.8019 - accuracy: 0.7305 - val_loss: 0.5204 - val_auc: 0.8087 - val_accuracy: 0.7514\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.4735 - auc: 0.8597 - accuracy: 0.7748 - val_loss: 0.5182 - val_auc: 0.8108 - val_accuracy: 0.7568\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5410 - auc: 0.7971 - accuracy: 0.7196 - val_loss: 0.5163 - val_auc: 0.8118 - val_accuracy: 0.7622\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5112 - auc: 0.8212 - accuracy: 0.7333 - val_loss: 0.5145 - val_auc: 0.8125 - val_accuracy: 0.7568\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5423 - auc: 0.7890 - accuracy: 0.6983 - val_loss: 0.5130 - val_auc: 0.8140 - val_accuracy: 0.7459\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5272 - auc: 0.8048 - accuracy: 0.7271 - val_loss: 0.5114 - val_auc: 0.8151 - val_accuracy: 0.7514\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5215 - auc: 0.8111 - accuracy: 0.7237 - val_loss: 0.5099 - val_auc: 0.8161 - val_accuracy: 0.7514\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4844 - auc: 0.8424 - accuracy: 0.7547 - val_loss: 0.5088 - val_auc: 0.8174 - val_accuracy: 0.7514\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5070 - auc: 0.8237 - accuracy: 0.7319 - val_loss: 0.5075 - val_auc: 0.8186 - val_accuracy: 0.7514\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4941 - auc: 0.8336 - accuracy: 0.7355 - val_loss: 0.5065 - val_auc: 0.8203 - val_accuracy: 0.7514\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4777 - auc: 0.8517 - accuracy: 0.7742 - val_loss: 0.5056 - val_auc: 0.8217 - val_accuracy: 0.7514\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4995 - auc: 0.8245 - accuracy: 0.7435 - val_loss: 0.5047 - val_auc: 0.8225 - val_accuracy: 0.7514\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5049 - auc: 0.8196 - accuracy: 0.7410 - val_loss: 0.5039 - val_auc: 0.8233 - val_accuracy: 0.7514\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4896 - auc: 0.8317 - accuracy: 0.7396 - val_loss: 0.5029 - val_auc: 0.8236 - val_accuracy: 0.7676\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4694 - auc: 0.8570 - accuracy: 0.7756 - val_loss: 0.5022 - val_auc: 0.8242 - val_accuracy: 0.7676\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5039 - auc: 0.8225 - accuracy: 0.7400 - val_loss: 0.5015 - val_auc: 0.8248 - val_accuracy: 0.7676\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4692 - auc: 0.8502 - accuracy: 0.7803 - val_loss: 0.5008 - val_auc: 0.8254 - val_accuracy: 0.7676\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5024 - auc: 0.8205 - accuracy: 0.7416 - val_loss: 0.5001 - val_auc: 0.8252 - val_accuracy: 0.7730\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4999 - auc: 0.8255 - accuracy: 0.7442 - val_loss: 0.4996 - val_auc: 0.8255 - val_accuracy: 0.7730\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4914 - auc: 0.8307 - accuracy: 0.7420 - val_loss: 0.4991 - val_auc: 0.8258 - val_accuracy: 0.7730\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4663 - auc: 0.8513 - accuracy: 0.7702 - val_loss: 0.4985 - val_auc: 0.8263 - val_accuracy: 0.7730\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5084 - auc: 0.8117 - accuracy: 0.7345 - val_loss: 0.4980 - val_auc: 0.8260 - val_accuracy: 0.7730\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4874 - auc: 0.8248 - accuracy: 0.7420 - val_loss: 0.4975 - val_auc: 0.8258 - val_accuracy: 0.7730\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5240 - auc: 0.8023 - accuracy: 0.7273 - val_loss: 0.4970 - val_auc: 0.8264 - val_accuracy: 0.7838\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4847 - auc: 0.8455 - accuracy: 0.7596 - val_loss: 0.4966 - val_auc: 0.8272 - val_accuracy: 0.7838\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4907 - auc: 0.8299 - accuracy: 0.7732 - val_loss: 0.4962 - val_auc: 0.8268 - val_accuracy: 0.7838\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5051 - auc: 0.8214 - accuracy: 0.7565 - val_loss: 0.4958 - val_auc: 0.8266 - val_accuracy: 0.7838\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4885 - auc: 0.8349 - accuracy: 0.7666 - val_loss: 0.4954 - val_auc: 0.8272 - val_accuracy: 0.7838\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4683 - auc: 0.8482 - accuracy: 0.7691 - val_loss: 0.4951 - val_auc: 0.8277 - val_accuracy: 0.7838\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4816 - auc: 0.8411 - accuracy: 0.7704 - val_loss: 0.4947 - val_auc: 0.8277 - val_accuracy: 0.7838\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4829 - auc: 0.8377 - accuracy: 0.7721 - val_loss: 0.4943 - val_auc: 0.8274 - val_accuracy: 0.7838\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4670 - auc: 0.8557 - accuracy: 0.7993 - val_loss: 0.4940 - val_auc: 0.8281 - val_accuracy: 0.7838\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4821 - auc: 0.8408 - accuracy: 0.7508 - val_loss: 0.4938 - val_auc: 0.8283 - val_accuracy: 0.7838\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4439 - auc: 0.8733 - accuracy: 0.8156 - val_loss: 0.4935 - val_auc: 0.8290 - val_accuracy: 0.7784\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4757 - auc: 0.8374 - accuracy: 0.7586 - val_loss: 0.4933 - val_auc: 0.8288 - val_accuracy: 0.7784\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4466 - auc: 0.8647 - accuracy: 0.7948 - val_loss: 0.4932 - val_auc: 0.8296 - val_accuracy: 0.7784\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5035 - auc: 0.8212 - accuracy: 0.7460 - val_loss: 0.4928 - val_auc: 0.8294 - val_accuracy: 0.7784\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4454 - auc: 0.8689 - accuracy: 0.8029 - val_loss: 0.4926 - val_auc: 0.8295 - val_accuracy: 0.7784\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4845 - auc: 0.8417 - accuracy: 0.7675 - val_loss: 0.4924 - val_auc: 0.8296 - val_accuracy: 0.7784\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4896 - auc: 0.8340 - accuracy: 0.7666 - val_loss: 0.4922 - val_auc: 0.8296 - val_accuracy: 0.7784\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4965 - auc: 0.8237 - accuracy: 0.7441 - val_loss: 0.4920 - val_auc: 0.8301 - val_accuracy: 0.7730\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4881 - auc: 0.8358 - accuracy: 0.7494 - val_loss: 0.4918 - val_auc: 0.8309 - val_accuracy: 0.7730\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5024 - auc: 0.8202 - accuracy: 0.7637 - val_loss: 0.4915 - val_auc: 0.8309 - val_accuracy: 0.7730\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5014 - auc: 0.8303 - accuracy: 0.7360 - val_loss: 0.4912 - val_auc: 0.8307 - val_accuracy: 0.7730\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4917 - auc: 0.8296 - accuracy: 0.7617 - val_loss: 0.4911 - val_auc: 0.8304 - val_accuracy: 0.7730\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4662 - auc: 0.8579 - accuracy: 0.7787 - val_loss: 0.4909 - val_auc: 0.8306 - val_accuracy: 0.7730\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4830 - auc: 0.8349 - accuracy: 0.7534 - val_loss: 0.4908 - val_auc: 0.8304 - val_accuracy: 0.7730\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4736 - auc: 0.8412 - accuracy: 0.7581 - val_loss: 0.4907 - val_auc: 0.8307 - val_accuracy: 0.7730\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5072 - auc: 0.8141 - accuracy: 0.7439 - val_loss: 0.4906 - val_auc: 0.8311 - val_accuracy: 0.7730\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4370 - auc: 0.8677 - accuracy: 0.7825 - val_loss: 0.4905 - val_auc: 0.8307 - val_accuracy: 0.7784\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4649 - auc: 0.8561 - accuracy: 0.7841 - val_loss: 0.4902 - val_auc: 0.8307 - val_accuracy: 0.7784\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4648 - auc: 0.8497 - accuracy: 0.7692 - val_loss: 0.4901 - val_auc: 0.8307 - val_accuracy: 0.7784\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4648 - auc: 0.8510 - accuracy: 0.7874 - val_loss: 0.4900 - val_auc: 0.8308 - val_accuracy: 0.7784\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4640 - auc: 0.8499 - accuracy: 0.7725 - val_loss: 0.4898 - val_auc: 0.8311 - val_accuracy: 0.7784\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4486 - auc: 0.8583 - accuracy: 0.7843 - val_loss: 0.4897 - val_auc: 0.8307 - val_accuracy: 0.7730\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4648 - auc: 0.8470 - accuracy: 0.7856 - val_loss: 0.4895 - val_auc: 0.8318 - val_accuracy: 0.7730\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4461 - auc: 0.8568 - accuracy: 0.7749 - val_loss: 0.4894 - val_auc: 0.8321 - val_accuracy: 0.7730\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5076 - auc: 0.8220 - accuracy: 0.7377 - val_loss: 0.4891 - val_auc: 0.8320 - val_accuracy: 0.7730\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4597 - auc: 0.8471 - accuracy: 0.7751 - val_loss: 0.4889 - val_auc: 0.8314 - val_accuracy: 0.7784\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4467 - auc: 0.8596 - accuracy: 0.7846 - val_loss: 0.4888 - val_auc: 0.8316 - val_accuracy: 0.7784\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.4391 - auc: 0.8634 - accuracy: 0.7843 - val_loss: 0.4886 - val_auc: 0.8317 - val_accuracy: 0.7784\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4722 - auc: 0.8416 - accuracy: 0.7838 - val_loss: 0.4885 - val_auc: 0.8318 - val_accuracy: 0.7784\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4676 - auc: 0.8456 - accuracy: 0.7609 - val_loss: 0.4884 - val_auc: 0.8316 - val_accuracy: 0.7784\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4678 - auc: 0.8450 - accuracy: 0.7786 - val_loss: 0.4883 - val_auc: 0.8319 - val_accuracy: 0.7784\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4482 - auc: 0.8554 - accuracy: 0.7782 - val_loss: 0.4881 - val_auc: 0.8320 - val_accuracy: 0.7784\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4763 - auc: 0.8367 - accuracy: 0.7617 - val_loss: 0.4880 - val_auc: 0.8326 - val_accuracy: 0.7784\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4359 - auc: 0.8669 - accuracy: 0.7847 - val_loss: 0.4877 - val_auc: 0.8329 - val_accuracy: 0.7730\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4570 - auc: 0.8503 - accuracy: 0.7730 - val_loss: 0.4876 - val_auc: 0.8333 - val_accuracy: 0.7730\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4877 - auc: 0.8252 - accuracy: 0.7456 - val_loss: 0.4875 - val_auc: 0.8333 - val_accuracy: 0.7730\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4444 - auc: 0.8568 - accuracy: 0.7803 - val_loss: 0.4873 - val_auc: 0.8332 - val_accuracy: 0.7730\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4925 - auc: 0.8297 - accuracy: 0.7393 - val_loss: 0.4872 - val_auc: 0.8331 - val_accuracy: 0.7730\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4492 - auc: 0.8564 - accuracy: 0.7644 - val_loss: 0.4870 - val_auc: 0.8330 - val_accuracy: 0.7730\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4973 - auc: 0.8259 - accuracy: 0.7439 - val_loss: 0.4869 - val_auc: 0.8331 - val_accuracy: 0.7730\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4419 - auc: 0.8699 - accuracy: 0.7748 - val_loss: 0.4867 - val_auc: 0.8332 - val_accuracy: 0.7730\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4243 - auc: 0.8771 - accuracy: 0.7917 - val_loss: 0.4866 - val_auc: 0.8330 - val_accuracy: 0.7730\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4510 - auc: 0.8528 - accuracy: 0.7616 - val_loss: 0.4865 - val_auc: 0.8331 - val_accuracy: 0.7730\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4417 - auc: 0.8562 - accuracy: 0.7868 - val_loss: 0.4864 - val_auc: 0.8332 - val_accuracy: 0.7730\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4649 - auc: 0.8454 - accuracy: 0.7656 - val_loss: 0.4863 - val_auc: 0.8331 - val_accuracy: 0.7730\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4428 - auc: 0.8608 - accuracy: 0.7756 - val_loss: 0.4863 - val_auc: 0.8333 - val_accuracy: 0.7730\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4539 - auc: 0.8525 - accuracy: 0.7738 - val_loss: 0.4862 - val_auc: 0.8335 - val_accuracy: 0.7730\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4950 - auc: 0.8174 - accuracy: 0.7463 - val_loss: 0.4862 - val_auc: 0.8339 - val_accuracy: 0.7730\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4883 - auc: 0.8259 - accuracy: 0.7383 - val_loss: 0.4861 - val_auc: 0.8340 - val_accuracy: 0.7730\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4579 - auc: 0.8481 - accuracy: 0.7647 - val_loss: 0.4859 - val_auc: 0.8340 - val_accuracy: 0.7730\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4916 - auc: 0.8217 - accuracy: 0.7173 - val_loss: 0.4858 - val_auc: 0.8340 - val_accuracy: 0.7730\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4424 - auc: 0.8650 - accuracy: 0.7869 - val_loss: 0.4858 - val_auc: 0.8340 - val_accuracy: 0.7730\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4328 - auc: 0.8717 - accuracy: 0.7916 - val_loss: 0.4856 - val_auc: 0.8344 - val_accuracy: 0.7730\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4776 - auc: 0.8456 - accuracy: 0.7538 - val_loss: 0.4855 - val_auc: 0.8348 - val_accuracy: 0.7730\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4250 - auc: 0.8721 - accuracy: 0.7956 - val_loss: 0.4853 - val_auc: 0.8344 - val_accuracy: 0.7784\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4342 - auc: 0.8660 - accuracy: 0.7776 - val_loss: 0.4852 - val_auc: 0.8342 - val_accuracy: 0.7784\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4468 - auc: 0.8626 - accuracy: 0.7883 - val_loss: 0.4852 - val_auc: 0.8341 - val_accuracy: 0.7784\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4361 - auc: 0.8698 - accuracy: 0.7838 - val_loss: 0.4852 - val_auc: 0.8346 - val_accuracy: 0.7784\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8543 - accuracy: 0.7731 - val_loss: 0.4849 - val_auc: 0.8344 - val_accuracy: 0.7784\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4587 - auc: 0.8379 - accuracy: 0.7551 - val_loss: 0.4849 - val_auc: 0.8346 - val_accuracy: 0.7784\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4702 - auc: 0.8490 - accuracy: 0.7605 - val_loss: 0.4847 - val_auc: 0.8346 - val_accuracy: 0.7784\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4045 - auc: 0.8888 - accuracy: 0.8074 - val_loss: 0.4845 - val_auc: 0.8351 - val_accuracy: 0.7784\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4774 - auc: 0.8285 - accuracy: 0.7503 - val_loss: 0.4844 - val_auc: 0.8346 - val_accuracy: 0.7784\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4454 - auc: 0.8625 - accuracy: 0.7806 - val_loss: 0.4843 - val_auc: 0.8354 - val_accuracy: 0.7784\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4499 - auc: 0.8583 - accuracy: 0.7741 - val_loss: 0.4843 - val_auc: 0.8348 - val_accuracy: 0.7784\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4769 - auc: 0.8395 - accuracy: 0.7522 - val_loss: 0.4842 - val_auc: 0.8351 - val_accuracy: 0.7784\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4621 - auc: 0.8524 - accuracy: 0.7695 - val_loss: 0.4843 - val_auc: 0.8344 - val_accuracy: 0.7784\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4828 - auc: 0.8283 - accuracy: 0.7555 - val_loss: 0.4842 - val_auc: 0.8348 - val_accuracy: 0.7784\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4697 - auc: 0.8368 - accuracy: 0.7733 - val_loss: 0.4840 - val_auc: 0.8347 - val_accuracy: 0.7784\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4390 - auc: 0.8649 - accuracy: 0.7892 - val_loss: 0.4839 - val_auc: 0.8342 - val_accuracy: 0.7784\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4531 - auc: 0.8591 - accuracy: 0.7777 - val_loss: 0.4839 - val_auc: 0.8348 - val_accuracy: 0.7784\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4532 - auc: 0.8589 - accuracy: 0.7758 - val_loss: 0.4839 - val_auc: 0.8346 - val_accuracy: 0.7784\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4770 - auc: 0.8398 - accuracy: 0.7743 - val_loss: 0.4839 - val_auc: 0.8346 - val_accuracy: 0.7784\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4447 - auc: 0.8587 - accuracy: 0.7974 - val_loss: 0.4838 - val_auc: 0.8344 - val_accuracy: 0.7784\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4831 - auc: 0.8301 - accuracy: 0.7521 - val_loss: 0.4837 - val_auc: 0.8341 - val_accuracy: 0.7784\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4698 - auc: 0.8352 - accuracy: 0.7645 - val_loss: 0.4837 - val_auc: 0.8345 - val_accuracy: 0.7784\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4379 - auc: 0.8584 - accuracy: 0.7822 - val_loss: 0.4837 - val_auc: 0.8343 - val_accuracy: 0.7784\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4763 - auc: 0.8360 - accuracy: 0.7699 - val_loss: 0.4836 - val_auc: 0.8346 - val_accuracy: 0.7784\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4108 - auc: 0.8896 - accuracy: 0.8061 - val_loss: 0.4835 - val_auc: 0.8344 - val_accuracy: 0.7784\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4746 - auc: 0.8459 - accuracy: 0.7697 - val_loss: 0.4834 - val_auc: 0.8347 - val_accuracy: 0.7784\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4868 - auc: 0.8251 - accuracy: 0.7638 - val_loss: 0.4833 - val_auc: 0.8347 - val_accuracy: 0.7784\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4698 - auc: 0.8438 - accuracy: 0.7564 - val_loss: 0.4832 - val_auc: 0.8348 - val_accuracy: 0.7784\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4476 - auc: 0.8561 - accuracy: 0.7655 - val_loss: 0.4832 - val_auc: 0.8349 - val_accuracy: 0.7784\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4819 - auc: 0.8285 - accuracy: 0.7516 - val_loss: 0.4832 - val_auc: 0.8350 - val_accuracy: 0.7784\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4391 - auc: 0.8701 - accuracy: 0.7859 - val_loss: 0.4832 - val_auc: 0.8352 - val_accuracy: 0.7784\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4294 - auc: 0.8695 - accuracy: 0.7964 - val_loss: 0.4833 - val_auc: 0.8351 - val_accuracy: 0.7784\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4733 - auc: 0.8390 - accuracy: 0.7657 - val_loss: 0.4832 - val_auc: 0.8352 - val_accuracy: 0.7784\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4402 - auc: 0.8594 - accuracy: 0.7798 - val_loss: 0.4832 - val_auc: 0.8352 - val_accuracy: 0.7784\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4363 - auc: 0.8709 - accuracy: 0.7897 - val_loss: 0.4831 - val_auc: 0.8354 - val_accuracy: 0.7838\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4877 - auc: 0.8261 - accuracy: 0.7438 - val_loss: 0.4830 - val_auc: 0.8359 - val_accuracy: 0.7784\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4672 - auc: 0.8479 - accuracy: 0.7673 - val_loss: 0.4827 - val_auc: 0.8357 - val_accuracy: 0.7784\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4421 - auc: 0.8676 - accuracy: 0.7709 - val_loss: 0.4827 - val_auc: 0.8357 - val_accuracy: 0.7784\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4524 - auc: 0.8556 - accuracy: 0.7841 - val_loss: 0.4827 - val_auc: 0.8357 - val_accuracy: 0.7784\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8535 - accuracy: 0.7637 - val_loss: 0.4827 - val_auc: 0.8358 - val_accuracy: 0.7784\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4365 - auc: 0.8644 - accuracy: 0.7923 - val_loss: 0.4827 - val_auc: 0.8359 - val_accuracy: 0.7784\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4739 - auc: 0.8410 - accuracy: 0.7602 - val_loss: 0.4828 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4512 - auc: 0.8557 - accuracy: 0.7790 - val_loss: 0.4826 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4278 - auc: 0.8705 - accuracy: 0.7877 - val_loss: 0.4825 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8502 - accuracy: 0.7698 - val_loss: 0.4826 - val_auc: 0.8364 - val_accuracy: 0.7784\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4877 - auc: 0.8210 - accuracy: 0.7597 - val_loss: 0.4826 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4666 - auc: 0.8449 - accuracy: 0.7629 - val_loss: 0.4825 - val_auc: 0.8366 - val_accuracy: 0.7784\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4729 - auc: 0.8364 - accuracy: 0.7432 - val_loss: 0.4823 - val_auc: 0.8359 - val_accuracy: 0.7784\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8463 - accuracy: 0.7796 - val_loss: 0.4824 - val_auc: 0.8360 - val_accuracy: 0.7784\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4926 - auc: 0.8278 - accuracy: 0.7549 - val_loss: 0.4824 - val_auc: 0.8362 - val_accuracy: 0.7784\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4790 - auc: 0.8361 - accuracy: 0.7840 - val_loss: 0.4825 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4559 - auc: 0.8623 - accuracy: 0.7711 - val_loss: 0.4824 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4477 - auc: 0.8626 - accuracy: 0.7963 - val_loss: 0.4824 - val_auc: 0.8363 - val_accuracy: 0.7784\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4441 - auc: 0.8571 - accuracy: 0.7884 - val_loss: 0.4823 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4921 - auc: 0.8238 - accuracy: 0.7431 - val_loss: 0.4823 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4567 - auc: 0.8579 - accuracy: 0.7936 - val_loss: 0.4821 - val_auc: 0.8367 - val_accuracy: 0.7784\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8511 - accuracy: 0.7884 - val_loss: 0.4820 - val_auc: 0.8369 - val_accuracy: 0.7784\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4382 - auc: 0.8498 - accuracy: 0.7935 - val_loss: 0.4820 - val_auc: 0.8370 - val_accuracy: 0.7784\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4821 - auc: 0.8372 - accuracy: 0.7646 - val_loss: 0.4819 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4902 - auc: 0.8276 - accuracy: 0.7633 - val_loss: 0.4818 - val_auc: 0.8365 - val_accuracy: 0.7784\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4722 - auc: 0.8472 - accuracy: 0.7799 - val_loss: 0.4819 - val_auc: 0.8364 - val_accuracy: 0.7730\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4820 - auc: 0.8347 - accuracy: 0.7610 - val_loss: 0.4818 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8432 - accuracy: 0.7781 - val_loss: 0.4817 - val_auc: 0.8364 - val_accuracy: 0.7730\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4622 - auc: 0.8446 - accuracy: 0.7795 - val_loss: 0.4816 - val_auc: 0.8362 - val_accuracy: 0.7730\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4531 - auc: 0.8563 - accuracy: 0.7759 - val_loss: 0.4816 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5130 - auc: 0.8216 - accuracy: 0.7535 - val_loss: 0.4815 - val_auc: 0.8361 - val_accuracy: 0.7730\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4559 - auc: 0.8524 - accuracy: 0.7878 - val_loss: 0.4816 - val_auc: 0.8361 - val_accuracy: 0.7730\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4406 - auc: 0.8631 - accuracy: 0.7904 - val_loss: 0.4816 - val_auc: 0.8362 - val_accuracy: 0.7730\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5077 - auc: 0.8218 - accuracy: 0.7503 - val_loss: 0.4816 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4574 - auc: 0.8554 - accuracy: 0.7829 - val_loss: 0.4815 - val_auc: 0.8364 - val_accuracy: 0.7730\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4568 - auc: 0.8612 - accuracy: 0.7861 - val_loss: 0.4814 - val_auc: 0.8371 - val_accuracy: 0.7730\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4471 - auc: 0.8556 - accuracy: 0.7824 - val_loss: 0.4815 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4379 - auc: 0.8641 - accuracy: 0.7774 - val_loss: 0.4815 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7841 - val_loss: 0.4815 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4602 - auc: 0.8570 - accuracy: 0.7810 - val_loss: 0.4814 - val_auc: 0.8373 - val_accuracy: 0.7730\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4644 - auc: 0.8544 - accuracy: 0.7773 - val_loss: 0.4814 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4540 - auc: 0.8558 - accuracy: 0.7823 - val_loss: 0.4814 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4248 - auc: 0.8788 - accuracy: 0.7903 - val_loss: 0.4813 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4927 - auc: 0.8322 - accuracy: 0.7610 - val_loss: 0.4814 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4794 - auc: 0.8402 - accuracy: 0.7665 - val_loss: 0.4815 - val_auc: 0.8366 - val_accuracy: 0.7676\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4792 - auc: 0.8368 - accuracy: 0.7634 - val_loss: 0.4813 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4814 - auc: 0.8394 - accuracy: 0.7501 - val_loss: 0.4813 - val_auc: 0.8370 - val_accuracy: 0.7676\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4720 - auc: 0.8403 - accuracy: 0.7730 - val_loss: 0.4812 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4626 - auc: 0.8500 - accuracy: 0.7657 - val_loss: 0.4811 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4829 - auc: 0.8343 - accuracy: 0.7522 - val_loss: 0.4811 - val_auc: 0.8370 - val_accuracy: 0.7676\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4787 - auc: 0.8394 - accuracy: 0.7621 - val_loss: 0.4811 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4434 - auc: 0.8647 - accuracy: 0.7770 - val_loss: 0.4810 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4531 - auc: 0.8530 - accuracy: 0.7894 - val_loss: 0.4810 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4656 - auc: 0.8549 - accuracy: 0.7785 - val_loss: 0.4810 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4847 - auc: 0.8382 - accuracy: 0.7494 - val_loss: 0.4810 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4505 - auc: 0.8507 - accuracy: 0.7859 - val_loss: 0.4809 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4590 - auc: 0.8493 - accuracy: 0.7874 - val_loss: 0.4808 - val_auc: 0.8376 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230ee76c3a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=1, callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "animated-soldier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 26156), started 0:30:54 ago. (Use '!kill 26156' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ac3c8240d203fc1c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ac3c8240d203fc1c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard launch\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sorted-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 1ms/step - loss: 0.5042 - auc: 0.8071 - accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "model = load_model(mc_path)\n",
    "eval = model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-bearing",
   "metadata": {},
   "source": [
    "# 7. Elección del umbral usando f2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-survey",
   "metadata": {},
   "source": [
    "A la prueba anterior, se suma la selección del umbral (o **threshold**) con el cual el clasificador discrimina entre clases. El mejor umbral de clasificación se calcula para todos los modelos, después del correspondiente entrenamiento. Para esta elección se elije el mejor valor del f2-score sobre el subset de **valid**. También se muestra la evolución de esta métrica respecto al umbral en el subset de **train**. En teoría, este umbral **no modifica la mérica principal del modelo, que es el área bajo la curva ROC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "similar-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def round_threshold(vector, threshold=0.5):\n",
    "    rounded_vector = []\n",
    "    for element in vector:\n",
    "        if element >= threshold:\n",
    "            rounded_vector.append(1)\n",
    "        else:\n",
    "            rounded_vector.append(0)\n",
    "            \n",
    "    return np.array(rounded_vector)\n",
    "        \n",
    "def f2_threshold_selection(y_probs_valid, y_true_valid, y_probs_train, y_true_train, steps=100, plot=True):\n",
    "    # Thresholds and f2-score vectors\n",
    "    thresholds = np.linspace(0, 1, steps)\n",
    "    f2_score_valid = []\n",
    "    f2_score_train = []\n",
    "    \n",
    "    for thld in thresholds:\n",
    "        # Generate predictions with current threshold\n",
    "        y_pred_valid = round_threshold(vector=y_probs_valid, threshold=thld)\n",
    "        y_pred_train = round_threshold(vector=y_probs_train, threshold=thld)\n",
    "        # Compute f2 score for that threshold and append\n",
    "        score_valid = fbeta_score(y_true=y_true_valid, y_pred=y_pred_valid, beta=2)\n",
    "        score_train = fbeta_score(y_true=y_true_train, y_pred=y_pred_train, beta=2)\n",
    "        f2_score_valid.append(score_valid)\n",
    "        f2_score_train.append(score_train)\n",
    "    \n",
    "    idx = np.argmax(f2_score_valid)\n",
    "    if plot == True:\n",
    "        plt.plot(thresholds, f2_score_valid, label='valid')\n",
    "        plt.plot(thresholds, f2_score_train, label='train')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('F2 score')\n",
    "        plt.axvline(thresholds[idx], color='black', linestyle='--')\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "        plt.grid(b=True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return thresholds, f2_score_valid, idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "attempted-apparatus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.26262626262626265"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get binary-class probability from model\n",
    "y_probs_valid = model.predict(x_valid)\n",
    "y_probs_train = model.predict(x_train)\n",
    "thresholds, f2_score, idx = f2_threshold_selection(y_probs_valid, y_valid, y_probs_train, y_train, steps=100)\n",
    "thresholds[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-onion",
   "metadata": {},
   "source": [
    "# 8. Early Stopping\n",
    "Habiendo concluido el test #1, se cree necesario agregar un callback de early stopping al modelo. Este callback deberá detener el proceso de aprendizaje en el momento en el que la **métrica principal** del modelo **deje de aumentar**. Posteriormente, se recupera el modelo con mejor performance en cuanto a esta métrica (AUC). Cabe aclarar que esta técnica es especialmente útil cuando la métrica principal no es diferenciable, y por ende se debe emplear una **loss subrogada** (en este caso, la binary cross entropy). De esta forma, el número de epochs que recorra el proceso de entrenamiento se verá limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "artificial-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Early Stopping callback from keras.\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "intended-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Early Stopping callback\n",
    "es_callback = EarlyStopping(monitor='val_auc', mode='max', min_delta=0.001, patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "valued-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Checkpoint callback\n",
    "mc_path = 'model_checkpoints/early_stopping_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "global-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "es_model = Sequential()\n",
    "es_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "latin-period",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "14/14 [==============================] - 4s 255ms/step - loss: 0.6059 - auc: 0.7569 - accuracy: 0.6770 - val_loss: 0.6644 - val_auc: 0.7047 - val_accuracy: 0.6324\n",
      "Epoch 2/1000\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5772 - auc: 0.7919 - accuracy: 0.7230 - val_loss: 0.6584 - val_auc: 0.7091 - val_accuracy: 0.6324\n",
      "Epoch 3/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5643 - auc: 0.8074 - accuracy: 0.7437 - val_loss: 0.6527 - val_auc: 0.7125 - val_accuracy: 0.6324\n",
      "Epoch 4/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5800 - auc: 0.7917 - accuracy: 0.6944 - val_loss: 0.6476 - val_auc: 0.7164 - val_accuracy: 0.6324\n",
      "Epoch 5/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5502 - auc: 0.8016 - accuracy: 0.7010 - val_loss: 0.6426 - val_auc: 0.7198 - val_accuracy: 0.6378\n",
      "Epoch 6/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5698 - auc: 0.7954 - accuracy: 0.7211 - val_loss: 0.6380 - val_auc: 0.7239 - val_accuracy: 0.6378\n",
      "Epoch 7/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5464 - auc: 0.8156 - accuracy: 0.7509 - val_loss: 0.6335 - val_auc: 0.7270 - val_accuracy: 0.6432\n",
      "Epoch 8/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5567 - auc: 0.8045 - accuracy: 0.7259 - val_loss: 0.6289 - val_auc: 0.7292 - val_accuracy: 0.6432\n",
      "Epoch 9/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5485 - auc: 0.8086 - accuracy: 0.7397 - val_loss: 0.6247 - val_auc: 0.7330 - val_accuracy: 0.6432\n",
      "Epoch 10/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5413 - auc: 0.8162 - accuracy: 0.7485 - val_loss: 0.6208 - val_auc: 0.7361 - val_accuracy: 0.6432\n",
      "Epoch 11/1000\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5305 - auc: 0.8295 - accuracy: 0.7633 - val_loss: 0.6169 - val_auc: 0.7383 - val_accuracy: 0.6486\n",
      "Epoch 12/1000\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5337 - auc: 0.8210 - accuracy: 0.7511 - val_loss: 0.6132 - val_auc: 0.7408 - val_accuracy: 0.6486\n",
      "Epoch 13/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5584 - auc: 0.7994 - accuracy: 0.7379 - val_loss: 0.6095 - val_auc: 0.7430 - val_accuracy: 0.6595\n",
      "Epoch 14/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5140 - auc: 0.8371 - accuracy: 0.7816 - val_loss: 0.6060 - val_auc: 0.7446 - val_accuracy: 0.6595\n",
      "Epoch 15/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5274 - auc: 0.8145 - accuracy: 0.7413 - val_loss: 0.6026 - val_auc: 0.7462 - val_accuracy: 0.6595\n",
      "Epoch 16/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5283 - auc: 0.8195 - accuracy: 0.7669 - val_loss: 0.5993 - val_auc: 0.7484 - val_accuracy: 0.6541\n",
      "Epoch 17/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5158 - auc: 0.8326 - accuracy: 0.7665 - val_loss: 0.5964 - val_auc: 0.7508 - val_accuracy: 0.6595\n",
      "Epoch 18/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5141 - auc: 0.8274 - accuracy: 0.7421 - val_loss: 0.5937 - val_auc: 0.7528 - val_accuracy: 0.6595\n",
      "Epoch 19/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5410 - auc: 0.8097 - accuracy: 0.7341 - val_loss: 0.5908 - val_auc: 0.7551 - val_accuracy: 0.6541\n",
      "Epoch 20/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5057 - auc: 0.8386 - accuracy: 0.7626 - val_loss: 0.5882 - val_auc: 0.7574 - val_accuracy: 0.6541\n",
      "Epoch 21/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5326 - auc: 0.8145 - accuracy: 0.7587 - val_loss: 0.5855 - val_auc: 0.7589 - val_accuracy: 0.6649\n",
      "Epoch 22/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5334 - auc: 0.8070 - accuracy: 0.7468 - val_loss: 0.5827 - val_auc: 0.7607 - val_accuracy: 0.6649\n",
      "Epoch 23/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4990 - auc: 0.8407 - accuracy: 0.7645 - val_loss: 0.5803 - val_auc: 0.7616 - val_accuracy: 0.6649\n",
      "Epoch 24/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5165 - auc: 0.8251 - accuracy: 0.7430 - val_loss: 0.5779 - val_auc: 0.7637 - val_accuracy: 0.6703\n",
      "Epoch 25/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4970 - auc: 0.8397 - accuracy: 0.7641 - val_loss: 0.5756 - val_auc: 0.7648 - val_accuracy: 0.6703\n",
      "Epoch 26/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4952 - auc: 0.8447 - accuracy: 0.7721 - val_loss: 0.5733 - val_auc: 0.7665 - val_accuracy: 0.6703\n",
      "Epoch 27/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4905 - auc: 0.8424 - accuracy: 0.7680 - val_loss: 0.5712 - val_auc: 0.7682 - val_accuracy: 0.6757\n",
      "Epoch 28/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5291 - auc: 0.7994 - accuracy: 0.7399 - val_loss: 0.5690 - val_auc: 0.7695 - val_accuracy: 0.6865\n",
      "Epoch 29/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5045 - auc: 0.8285 - accuracy: 0.7598 - val_loss: 0.5670 - val_auc: 0.7706 - val_accuracy: 0.6919\n",
      "Epoch 30/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5093 - auc: 0.8310 - accuracy: 0.7516 - val_loss: 0.5651 - val_auc: 0.7724 - val_accuracy: 0.6865\n",
      "Epoch 31/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5128 - auc: 0.8250 - accuracy: 0.7458 - val_loss: 0.5632 - val_auc: 0.7734 - val_accuracy: 0.6973\n",
      "Epoch 32/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4820 - auc: 0.8494 - accuracy: 0.7630 - val_loss: 0.5611 - val_auc: 0.7749 - val_accuracy: 0.7081\n",
      "Epoch 33/1000\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4952 - auc: 0.8365 - accuracy: 0.7451 - val_loss: 0.5592 - val_auc: 0.7758 - val_accuracy: 0.7081\n",
      "Epoch 34/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5080 - auc: 0.8250 - accuracy: 0.7525 - val_loss: 0.5574 - val_auc: 0.7767 - val_accuracy: 0.7081\n",
      "Epoch 35/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4985 - auc: 0.8436 - accuracy: 0.7702 - val_loss: 0.5556 - val_auc: 0.7779 - val_accuracy: 0.7081\n",
      "Epoch 36/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4820 - auc: 0.8485 - accuracy: 0.7734 - val_loss: 0.5539 - val_auc: 0.7795 - val_accuracy: 0.7081\n",
      "Epoch 37/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4803 - auc: 0.8492 - accuracy: 0.7777 - val_loss: 0.5521 - val_auc: 0.7804 - val_accuracy: 0.7135\n",
      "Epoch 38/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5030 - auc: 0.8299 - accuracy: 0.7597 - val_loss: 0.5505 - val_auc: 0.7811 - val_accuracy: 0.7135\n",
      "Epoch 39/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5077 - auc: 0.8227 - accuracy: 0.7475 - val_loss: 0.5491 - val_auc: 0.7823 - val_accuracy: 0.7081\n",
      "Epoch 40/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4773 - auc: 0.8542 - accuracy: 0.7823 - val_loss: 0.5476 - val_auc: 0.7837 - val_accuracy: 0.7081\n",
      "Epoch 41/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4799 - auc: 0.8434 - accuracy: 0.7908 - val_loss: 0.5461 - val_auc: 0.7846 - val_accuracy: 0.7081\n",
      "Epoch 42/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4794 - auc: 0.8348 - accuracy: 0.7655 - val_loss: 0.5447 - val_auc: 0.7859 - val_accuracy: 0.7081\n",
      "Epoch 43/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4988 - auc: 0.8318 - accuracy: 0.7704 - val_loss: 0.5434 - val_auc: 0.7864 - val_accuracy: 0.7189\n",
      "Epoch 44/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5012 - auc: 0.8179 - accuracy: 0.7512 - val_loss: 0.5421 - val_auc: 0.7873 - val_accuracy: 0.7189\n",
      "Epoch 45/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4600 - auc: 0.8588 - accuracy: 0.7773 - val_loss: 0.5409 - val_auc: 0.7881 - val_accuracy: 0.7243\n",
      "Epoch 46/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4541 - auc: 0.8613 - accuracy: 0.8039 - val_loss: 0.5393 - val_auc: 0.7893 - val_accuracy: 0.7243\n",
      "Epoch 47/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5005 - auc: 0.8217 - accuracy: 0.7609 - val_loss: 0.5381 - val_auc: 0.7900 - val_accuracy: 0.7243\n",
      "Epoch 48/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4742 - auc: 0.8511 - accuracy: 0.7649 - val_loss: 0.5368 - val_auc: 0.7909 - val_accuracy: 0.7243\n",
      "Epoch 49/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4908 - auc: 0.8345 - accuracy: 0.7644 - val_loss: 0.5357 - val_auc: 0.7918 - val_accuracy: 0.7243\n",
      "Epoch 50/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5161 - auc: 0.8166 - accuracy: 0.7424 - val_loss: 0.5348 - val_auc: 0.7919 - val_accuracy: 0.7243\n",
      "Epoch 51/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4835 - auc: 0.8390 - accuracy: 0.7511 - val_loss: 0.5335 - val_auc: 0.7929 - val_accuracy: 0.7243\n",
      "Epoch 52/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4720 - auc: 0.8494 - accuracy: 0.7723 - val_loss: 0.5324 - val_auc: 0.7937 - val_accuracy: 0.7243\n",
      "Epoch 53/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5062 - auc: 0.8195 - accuracy: 0.7408 - val_loss: 0.5314 - val_auc: 0.7951 - val_accuracy: 0.7297\n",
      "Epoch 54/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4784 - auc: 0.8421 - accuracy: 0.7585 - val_loss: 0.5302 - val_auc: 0.7970 - val_accuracy: 0.7297\n",
      "Epoch 55/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4856 - auc: 0.8295 - accuracy: 0.7554 - val_loss: 0.5292 - val_auc: 0.7976 - val_accuracy: 0.7297\n",
      "Epoch 56/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8592 - accuracy: 0.7704 - val_loss: 0.5279 - val_auc: 0.7980 - val_accuracy: 0.7297\n",
      "Epoch 57/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4865 - auc: 0.8318 - accuracy: 0.7792 - val_loss: 0.5270 - val_auc: 0.7989 - val_accuracy: 0.7297\n",
      "Epoch 58/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8498 - accuracy: 0.7641 - val_loss: 0.5262 - val_auc: 0.7996 - val_accuracy: 0.7297\n",
      "Epoch 59/1000\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4556 - auc: 0.8561 - accuracy: 0.7784 - val_loss: 0.5252 - val_auc: 0.8005 - val_accuracy: 0.7297\n",
      "Epoch 60/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4830 - auc: 0.8405 - accuracy: 0.7747 - val_loss: 0.5242 - val_auc: 0.8011 - val_accuracy: 0.7297\n",
      "Epoch 61/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4754 - auc: 0.8470 - accuracy: 0.7671 - val_loss: 0.5233 - val_auc: 0.8013 - val_accuracy: 0.7243\n",
      "Epoch 62/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4870 - auc: 0.8352 - accuracy: 0.7614 - val_loss: 0.5224 - val_auc: 0.8023 - val_accuracy: 0.7243\n",
      "Epoch 63/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4851 - auc: 0.8399 - accuracy: 0.7780 - val_loss: 0.5216 - val_auc: 0.8031 - val_accuracy: 0.7243\n",
      "Epoch 64/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4317 - auc: 0.8736 - accuracy: 0.8066 - val_loss: 0.5208 - val_auc: 0.8035 - val_accuracy: 0.7243\n",
      "Epoch 65/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4485 - auc: 0.8616 - accuracy: 0.8011 - val_loss: 0.5200 - val_auc: 0.8042 - val_accuracy: 0.7243\n",
      "Epoch 66/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4554 - auc: 0.8588 - accuracy: 0.7936 - val_loss: 0.5194 - val_auc: 0.8048 - val_accuracy: 0.7243\n",
      "Epoch 67/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4538 - auc: 0.8607 - accuracy: 0.7821 - val_loss: 0.5187 - val_auc: 0.8049 - val_accuracy: 0.7243\n",
      "Epoch 68/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4902 - auc: 0.8307 - accuracy: 0.7687 - val_loss: 0.5180 - val_auc: 0.8055 - val_accuracy: 0.7243\n",
      "Epoch 69/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4776 - auc: 0.8417 - accuracy: 0.7704 - val_loss: 0.5172 - val_auc: 0.8061 - val_accuracy: 0.7243\n",
      "Epoch 70/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4485 - auc: 0.8682 - accuracy: 0.7794 - val_loss: 0.5166 - val_auc: 0.8071 - val_accuracy: 0.7243\n",
      "Epoch 71/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4754 - auc: 0.8442 - accuracy: 0.7585 - val_loss: 0.5160 - val_auc: 0.8078 - val_accuracy: 0.7243\n",
      "Epoch 72/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4920 - auc: 0.8290 - accuracy: 0.7480 - val_loss: 0.5155 - val_auc: 0.8078 - val_accuracy: 0.7243\n",
      "Epoch 73/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5057 - auc: 0.8130 - accuracy: 0.7494 - val_loss: 0.5148 - val_auc: 0.8084 - val_accuracy: 0.7297\n",
      "Epoch 74/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4759 - auc: 0.8412 - accuracy: 0.7781 - val_loss: 0.5142 - val_auc: 0.8087 - val_accuracy: 0.7297\n",
      "Epoch 75/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4779 - auc: 0.8438 - accuracy: 0.7712 - val_loss: 0.5135 - val_auc: 0.8085 - val_accuracy: 0.7297\n",
      "Epoch 76/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8493 - accuracy: 0.7859 - val_loss: 0.5129 - val_auc: 0.8085 - val_accuracy: 0.7351\n",
      "Epoch 77/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4886 - auc: 0.8349 - accuracy: 0.7538 - val_loss: 0.5124 - val_auc: 0.8091 - val_accuracy: 0.7405\n",
      "Epoch 78/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4755 - auc: 0.8427 - accuracy: 0.7646 - val_loss: 0.5118 - val_auc: 0.8095 - val_accuracy: 0.7405\n",
      "Epoch 79/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4692 - auc: 0.8444 - accuracy: 0.7874 - val_loss: 0.5112 - val_auc: 0.8106 - val_accuracy: 0.7405\n",
      "Epoch 80/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5004 - auc: 0.8270 - accuracy: 0.7545 - val_loss: 0.5107 - val_auc: 0.8101 - val_accuracy: 0.7405\n",
      "Epoch 81/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4802 - auc: 0.8414 - accuracy: 0.7585 - val_loss: 0.5102 - val_auc: 0.8112 - val_accuracy: 0.7405\n",
      "Epoch 82/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4815 - auc: 0.8411 - accuracy: 0.7543 - val_loss: 0.5096 - val_auc: 0.8117 - val_accuracy: 0.7405\n",
      "Epoch 83/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4514 - auc: 0.8586 - accuracy: 0.7973 - val_loss: 0.5089 - val_auc: 0.8120 - val_accuracy: 0.7459\n",
      "Epoch 84/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8488 - accuracy: 0.7810 - val_loss: 0.5082 - val_auc: 0.8123 - val_accuracy: 0.7459\n",
      "Epoch 85/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4710 - auc: 0.8459 - accuracy: 0.7703 - val_loss: 0.5077 - val_auc: 0.8130 - val_accuracy: 0.7459\n",
      "Epoch 86/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4630 - auc: 0.8548 - accuracy: 0.7853 - val_loss: 0.5073 - val_auc: 0.8130 - val_accuracy: 0.7459\n",
      "Epoch 87/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4869 - auc: 0.8380 - accuracy: 0.7516 - val_loss: 0.5067 - val_auc: 0.8136 - val_accuracy: 0.7459\n",
      "Epoch 88/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4707 - auc: 0.8447 - accuracy: 0.7688 - val_loss: 0.5063 - val_auc: 0.8140 - val_accuracy: 0.7459\n",
      "Epoch 89/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4805 - auc: 0.8388 - accuracy: 0.7698 - val_loss: 0.5058 - val_auc: 0.8141 - val_accuracy: 0.7459\n",
      "Epoch 90/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4944 - auc: 0.8338 - accuracy: 0.7542 - val_loss: 0.5053 - val_auc: 0.8150 - val_accuracy: 0.7459\n",
      "Epoch 91/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4777 - auc: 0.8383 - accuracy: 0.7653 - val_loss: 0.5049 - val_auc: 0.8154 - val_accuracy: 0.7459\n",
      "Epoch 92/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4534 - auc: 0.8567 - accuracy: 0.7789 - val_loss: 0.5044 - val_auc: 0.8158 - val_accuracy: 0.7459\n",
      "Epoch 93/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4356 - auc: 0.8749 - accuracy: 0.7882 - val_loss: 0.5041 - val_auc: 0.8160 - val_accuracy: 0.7459\n",
      "Epoch 94/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4509 - auc: 0.8515 - accuracy: 0.7745 - val_loss: 0.5036 - val_auc: 0.8175 - val_accuracy: 0.7405\n",
      "Epoch 95/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4695 - auc: 0.8431 - accuracy: 0.7745 - val_loss: 0.5033 - val_auc: 0.8176 - val_accuracy: 0.7459\n",
      "Epoch 96/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5033 - auc: 0.8238 - accuracy: 0.7449 - val_loss: 0.5029 - val_auc: 0.8168 - val_accuracy: 0.7459\n",
      "Epoch 97/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4703 - auc: 0.8488 - accuracy: 0.7684 - val_loss: 0.5027 - val_auc: 0.8174 - val_accuracy: 0.7459\n",
      "Epoch 98/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4680 - auc: 0.8509 - accuracy: 0.7747 - val_loss: 0.5023 - val_auc: 0.8171 - val_accuracy: 0.7459\n",
      "Epoch 99/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8530 - accuracy: 0.7724 - val_loss: 0.5018 - val_auc: 0.8181 - val_accuracy: 0.7459\n",
      "Epoch 100/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4529 - auc: 0.8643 - accuracy: 0.7770 - val_loss: 0.5015 - val_auc: 0.8183 - val_accuracy: 0.7405\n",
      "Epoch 101/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4673 - auc: 0.8458 - accuracy: 0.7623 - val_loss: 0.5012 - val_auc: 0.8189 - val_accuracy: 0.7459\n",
      "Epoch 102/1000\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.4502 - auc: 0.8531 - accuracy: 0.7756 - val_loss: 0.5009 - val_auc: 0.8192 - val_accuracy: 0.7459\n",
      "Epoch 103/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4267 - auc: 0.8807 - accuracy: 0.8065 - val_loss: 0.5006 - val_auc: 0.8193 - val_accuracy: 0.7459\n",
      "Epoch 104/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4695 - auc: 0.8483 - accuracy: 0.7624 - val_loss: 0.5002 - val_auc: 0.8199 - val_accuracy: 0.7459\n",
      "Epoch 105/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4729 - auc: 0.8424 - accuracy: 0.7726 - val_loss: 0.4999 - val_auc: 0.8203 - val_accuracy: 0.7459\n",
      "Epoch 106/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4702 - auc: 0.8482 - accuracy: 0.7831 - val_loss: 0.4995 - val_auc: 0.8206 - val_accuracy: 0.7459\n",
      "Epoch 107/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4433 - auc: 0.8697 - accuracy: 0.7923 - val_loss: 0.4993 - val_auc: 0.8211 - val_accuracy: 0.7459\n",
      "Epoch 108/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8470 - accuracy: 0.7712 - val_loss: 0.4989 - val_auc: 0.8214 - val_accuracy: 0.7459\n",
      "Epoch 109/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4359 - auc: 0.8743 - accuracy: 0.7888 - val_loss: 0.4986 - val_auc: 0.8217 - val_accuracy: 0.7459\n",
      "Epoch 110/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4466 - auc: 0.8608 - accuracy: 0.7850 - val_loss: 0.4982 - val_auc: 0.8223 - val_accuracy: 0.7459\n",
      "Epoch 111/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8416 - accuracy: 0.7570 - val_loss: 0.4979 - val_auc: 0.8222 - val_accuracy: 0.7514\n",
      "Epoch 112/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4467 - auc: 0.8605 - accuracy: 0.7804 - val_loss: 0.4976 - val_auc: 0.8223 - val_accuracy: 0.7568\n",
      "Epoch 113/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8445 - accuracy: 0.7782 - val_loss: 0.4973 - val_auc: 0.8222 - val_accuracy: 0.7568\n",
      "Epoch 114/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4570 - auc: 0.8512 - accuracy: 0.7794 - val_loss: 0.4970 - val_auc: 0.8223 - val_accuracy: 0.7568\n",
      "Epoch 115/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4719 - auc: 0.8428 - accuracy: 0.7613 - val_loss: 0.4967 - val_auc: 0.8226 - val_accuracy: 0.7568\n",
      "Epoch 116/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4738 - auc: 0.8390 - accuracy: 0.7600 - val_loss: 0.4964 - val_auc: 0.8229 - val_accuracy: 0.7568\n",
      "Epoch 117/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4681 - auc: 0.8460 - accuracy: 0.7628 - val_loss: 0.4962 - val_auc: 0.8231 - val_accuracy: 0.7568\n",
      "Epoch 118/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4678 - auc: 0.8454 - accuracy: 0.7664 - val_loss: 0.4957 - val_auc: 0.8236 - val_accuracy: 0.7568\n",
      "Epoch 119/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4471 - auc: 0.8609 - accuracy: 0.7983 - val_loss: 0.4955 - val_auc: 0.8234 - val_accuracy: 0.7568\n",
      "Epoch 120/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4731 - auc: 0.8391 - accuracy: 0.7799 - val_loss: 0.4952 - val_auc: 0.8243 - val_accuracy: 0.7568\n",
      "Epoch 121/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8464 - accuracy: 0.7765 - val_loss: 0.4950 - val_auc: 0.8242 - val_accuracy: 0.7568\n",
      "Epoch 122/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4625 - auc: 0.8539 - accuracy: 0.7816 - val_loss: 0.4947 - val_auc: 0.8244 - val_accuracy: 0.7568\n",
      "Epoch 123/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8486 - accuracy: 0.7800 - val_loss: 0.4944 - val_auc: 0.8244 - val_accuracy: 0.7568\n",
      "Epoch 124/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4363 - auc: 0.8646 - accuracy: 0.7964 - val_loss: 0.4942 - val_auc: 0.8248 - val_accuracy: 0.7568\n",
      "Epoch 125/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4528 - auc: 0.8559 - accuracy: 0.7777 - val_loss: 0.4941 - val_auc: 0.8247 - val_accuracy: 0.7568\n",
      "Epoch 126/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4788 - auc: 0.8282 - accuracy: 0.7572 - val_loss: 0.4939 - val_auc: 0.8251 - val_accuracy: 0.7568\n",
      "Epoch 127/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4763 - auc: 0.8421 - accuracy: 0.7618 - val_loss: 0.4937 - val_auc: 0.8254 - val_accuracy: 0.7568\n",
      "Epoch 128/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4779 - auc: 0.8435 - accuracy: 0.7616 - val_loss: 0.4934 - val_auc: 0.8256 - val_accuracy: 0.7568\n",
      "Epoch 129/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8525 - accuracy: 0.7692 - val_loss: 0.4932 - val_auc: 0.8257 - val_accuracy: 0.7568\n",
      "Epoch 130/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4661 - auc: 0.8474 - accuracy: 0.7733 - val_loss: 0.4929 - val_auc: 0.8260 - val_accuracy: 0.7568\n",
      "Epoch 131/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4496 - auc: 0.8593 - accuracy: 0.7692 - val_loss: 0.4925 - val_auc: 0.8265 - val_accuracy: 0.7568\n",
      "Epoch 132/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4727 - auc: 0.8447 - accuracy: 0.7504 - val_loss: 0.4924 - val_auc: 0.8268 - val_accuracy: 0.7568\n",
      "Epoch 133/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4798 - auc: 0.8370 - accuracy: 0.7620 - val_loss: 0.4923 - val_auc: 0.8269 - val_accuracy: 0.7514\n",
      "Epoch 134/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8409 - accuracy: 0.7775 - val_loss: 0.4920 - val_auc: 0.8278 - val_accuracy: 0.7514\n",
      "Epoch 135/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8547 - accuracy: 0.7752 - val_loss: 0.4918 - val_auc: 0.8278 - val_accuracy: 0.7514\n",
      "Epoch 136/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4909 - auc: 0.8239 - accuracy: 0.7637 - val_loss: 0.4917 - val_auc: 0.8278 - val_accuracy: 0.7514\n",
      "Epoch 137/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4454 - auc: 0.8623 - accuracy: 0.7919 - val_loss: 0.4916 - val_auc: 0.8279 - val_accuracy: 0.7514\n",
      "Epoch 138/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4555 - auc: 0.8398 - accuracy: 0.7797 - val_loss: 0.4914 - val_auc: 0.8278 - val_accuracy: 0.7514\n",
      "Epoch 139/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4764 - auc: 0.8312 - accuracy: 0.7624 - val_loss: 0.4912 - val_auc: 0.8286 - val_accuracy: 0.7514\n",
      "Epoch 140/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4314 - auc: 0.8649 - accuracy: 0.7965 - val_loss: 0.4909 - val_auc: 0.8290 - val_accuracy: 0.7514\n",
      "Epoch 141/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4665 - auc: 0.8469 - accuracy: 0.7774 - val_loss: 0.4908 - val_auc: 0.8288 - val_accuracy: 0.7514\n",
      "Epoch 142/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4300 - auc: 0.8452 - accuracy: 0.8028 - val_loss: 0.4906 - val_auc: 0.8291 - val_accuracy: 0.7514\n",
      "Epoch 143/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4525 - auc: 0.8467 - accuracy: 0.7682 - val_loss: 0.4903 - val_auc: 0.8300 - val_accuracy: 0.7514\n",
      "Epoch 144/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4481 - auc: 0.8517 - accuracy: 0.7726 - val_loss: 0.4900 - val_auc: 0.8299 - val_accuracy: 0.7568\n",
      "Epoch 145/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4635 - auc: 0.8309 - accuracy: 0.7485 - val_loss: 0.4897 - val_auc: 0.8301 - val_accuracy: 0.7568\n",
      "Epoch 146/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4409 - auc: 0.8676 - accuracy: 0.8010 - val_loss: 0.4896 - val_auc: 0.8296 - val_accuracy: 0.7568\n",
      "Epoch 147/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4448 - auc: 0.8680 - accuracy: 0.7764 - val_loss: 0.4894 - val_auc: 0.8299 - val_accuracy: 0.7568\n",
      "Epoch 148/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4453 - auc: 0.8652 - accuracy: 0.7814 - val_loss: 0.4893 - val_auc: 0.8303 - val_accuracy: 0.7568\n",
      "Epoch 149/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8564 - accuracy: 0.7898 - val_loss: 0.4893 - val_auc: 0.8305 - val_accuracy: 0.7568\n",
      "Epoch 150/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4602 - auc: 0.8461 - accuracy: 0.7631 - val_loss: 0.4891 - val_auc: 0.8307 - val_accuracy: 0.7568\n",
      "Epoch 151/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4386 - auc: 0.8628 - accuracy: 0.8060 - val_loss: 0.4887 - val_auc: 0.8309 - val_accuracy: 0.7676\n",
      "Epoch 152/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4500 - auc: 0.8595 - accuracy: 0.7837 - val_loss: 0.4886 - val_auc: 0.8307 - val_accuracy: 0.7622\n",
      "Epoch 153/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4413 - auc: 0.8695 - accuracy: 0.7828 - val_loss: 0.4883 - val_auc: 0.8307 - val_accuracy: 0.7676\n",
      "Epoch 154/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4381 - auc: 0.8597 - accuracy: 0.8005 - val_loss: 0.4883 - val_auc: 0.8306 - val_accuracy: 0.7622\n",
      "Epoch 155/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8230 - accuracy: 0.7579 - val_loss: 0.4882 - val_auc: 0.8308 - val_accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230f87e8580>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "es_model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/ES/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "electric-healing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5000 - auc: 0.8103 - accuracy: 0.7273\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test subset and predict.\n",
    "es_model = load_model(mc_path)\n",
    "eval = es_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-hours",
   "metadata": {},
   "source": [
    "# 9. Learning Rate Scheduling\n",
    "En este apartado se prueba la opción de Learning Rate Scheduling. Esta se encarga de aplicarle una función al Learning Rate entre epochs, de forma tal de encontrar el mínimo de la loss de forma más rápida, y apuntando a evitar mínimos locales y, por ende, overfitting. Se sigue aplicando el concepto de **early stopping** para la AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "occupied-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay, PolynomialDecay # API in https://keras.io/api/optimizers/learning_rate_schedules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "upset-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/lrs_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bizarre-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "lrs_model = Sequential()\n",
    "lrs_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "accepting-slave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 175ms/step - loss: 1.1095 - auc: 0.3254 - accuracy: 0.3955 - val_loss: 0.7586 - val_auc: 0.4773 - val_accuracy: 0.5081\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7652 - auc: 0.5179 - accuracy: 0.5425 - val_loss: 0.6330 - val_auc: 0.6633 - val_accuracy: 0.6270\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6352 - auc: 0.6795 - accuracy: 0.6596 - val_loss: 0.5843 - val_auc: 0.7330 - val_accuracy: 0.6865\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5612 - auc: 0.7784 - accuracy: 0.75 - 0s 3ms/step - loss: 0.5621 - auc: 0.7632 - accuracy: 0.7088 - val_loss: 0.5562 - val_auc: 0.7614 - val_accuracy: 0.6919\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5112 - auc: 0.8141 - accuracy: 0.7467 - val_loss: 0.5377 - val_auc: 0.7790 - val_accuracy: 0.7081\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4890 - auc: 0.8342 - accuracy: 0.7692 - val_loss: 0.5244 - val_auc: 0.7920 - val_accuracy: 0.7135\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4998 - auc: 0.8204 - accuracy: 0.7425 - val_loss: 0.5157 - val_auc: 0.8001 - val_accuracy: 0.7189\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4896 - auc: 0.8295 - accuracy: 0.7764 - val_loss: 0.5096 - val_auc: 0.8052 - val_accuracy: 0.7243\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4856 - auc: 0.8319 - accuracy: 0.7574 - val_loss: 0.5038 - val_auc: 0.8113 - val_accuracy: 0.7459\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4542 - auc: 0.8536 - accuracy: 0.7737 - val_loss: 0.4984 - val_auc: 0.8156 - val_accuracy: 0.7405\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4767 - auc: 0.8398 - accuracy: 0.7620 - val_loss: 0.4945 - val_auc: 0.8199 - val_accuracy: 0.7297\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4750 - auc: 0.8409 - accuracy: 0.7615 - val_loss: 0.4919 - val_auc: 0.8223 - val_accuracy: 0.7351\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4526 - auc: 0.8658 - accuracy: 0.7668 - val_loss: 0.4899 - val_auc: 0.8249 - val_accuracy: 0.7351\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4816 - auc: 0.8295 - accuracy: 0.7640 - val_loss: 0.4885 - val_auc: 0.8263 - val_accuracy: 0.7405\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4664 - auc: 0.8542 - accuracy: 0.7580 - val_loss: 0.4875 - val_auc: 0.8265 - val_accuracy: 0.7514\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4568 - auc: 0.8577 - accuracy: 0.7635 - val_loss: 0.4861 - val_auc: 0.8285 - val_accuracy: 0.7514\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4545 - auc: 0.8541 - accuracy: 0.7702 - val_loss: 0.4850 - val_auc: 0.8291 - val_accuracy: 0.7459\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4771 - auc: 0.8432 - accuracy: 0.7481 - val_loss: 0.4834 - val_auc: 0.8309 - val_accuracy: 0.7459\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4412 - auc: 0.8590 - accuracy: 0.7789 - val_loss: 0.4839 - val_auc: 0.8308 - val_accuracy: 0.7568\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4378 - auc: 0.8630 - accuracy: 0.7907 - val_loss: 0.4840 - val_auc: 0.8308 - val_accuracy: 0.7459\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4597 - auc: 0.8501 - accuracy: 0.7788 - val_loss: 0.4840 - val_auc: 0.8321 - val_accuracy: 0.7514\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4644 - auc: 0.8478 - accuracy: 0.7798 - val_loss: 0.4822 - val_auc: 0.8324 - val_accuracy: 0.7514\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4302 - auc: 0.8711 - accuracy: 0.8004 - val_loss: 0.4815 - val_auc: 0.8330 - val_accuracy: 0.7514\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4215 - auc: 0.8681 - accuracy: 0.8196 - val_loss: 0.4810 - val_auc: 0.8332 - val_accuracy: 0.7514\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4395 - auc: 0.8626 - accuracy: 0.7886 - val_loss: 0.4808 - val_auc: 0.8343 - val_accuracy: 0.7514\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4550 - auc: 0.8521 - accuracy: 0.7842 - val_loss: 0.4791 - val_auc: 0.8340 - val_accuracy: 0.7622\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4640 - auc: 0.8526 - accuracy: 0.7704 - val_loss: 0.4789 - val_auc: 0.8348 - val_accuracy: 0.7622\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4724 - auc: 0.8427 - accuracy: 0.7771 - val_loss: 0.4787 - val_auc: 0.8353 - val_accuracy: 0.7622\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4628 - auc: 0.8456 - accuracy: 0.7786 - val_loss: 0.4790 - val_auc: 0.8351 - val_accuracy: 0.7676\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4640 - auc: 0.8507 - accuracy: 0.7642 - val_loss: 0.4780 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4780 - auc: 0.8415 - accuracy: 0.7708 - val_loss: 0.4769 - val_auc: 0.8359 - val_accuracy: 0.7730\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4383 - auc: 0.8602 - accuracy: 0.7848 - val_loss: 0.4761 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8476 - accuracy: 0.7615 - val_loss: 0.4763 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4536 - auc: 0.8595 - accuracy: 0.7709 - val_loss: 0.4779 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4539 - auc: 0.8520 - accuracy: 0.7780 - val_loss: 0.4784 - val_auc: 0.8366 - val_accuracy: 0.7676\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4728 - auc: 0.8434 - accuracy: 0.7564 - val_loss: 0.4789 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4219 - auc: 0.8727 - accuracy: 0.7988 - val_loss: 0.4786 - val_auc: 0.8379 - val_accuracy: 0.7676\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4839 - auc: 0.8267 - accuracy: 0.7779 - val_loss: 0.4789 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4739 - auc: 0.8302 - accuracy: 0.7653 - val_loss: 0.4797 - val_auc: 0.8367 - val_accuracy: 0.7838\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4127 - auc: 0.8798 - accuracy: 0.8120 - val_loss: 0.4805 - val_auc: 0.8363 - val_accuracy: 0.7838\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4663 - auc: 0.8401 - accuracy: 0.7837 - val_loss: 0.4810 - val_auc: 0.8368 - val_accuracy: 0.7838\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4690 - auc: 0.8485 - accuracy: 0.7563 - val_loss: 0.4791 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4500 - auc: 0.8594 - accuracy: 0.7911 - val_loss: 0.4786 - val_auc: 0.8381 - val_accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230f9cd0130>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define learning rate at start\n",
    "ilr = 0.1\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=100000, decay_rate=0.96, staircase=False) # Decay every (decay_steps) steps with a base of (decay_rate).\n",
    "# Compiling model\n",
    "lrs_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "subtle-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.4996 - auc: 0.8060 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-wrist",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Exponential Decay se lleva bien con Early Stopping?, ya que si reduzco el learning rate \"me muevo menos\", con lo cual el callback de Early Stopping cortaría prematuramente. Ahora probamos sin Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "civic-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 118ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7739 - val_loss: 0.4770 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4766 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8492 - accuracy: 0.7832 - val_loss: 0.4778 - val_auc: 0.8361 - val_accuracy: 0.7622\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4786 - val_auc: 0.8355 - val_accuracy: 0.7730\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8490 - accuracy: 0.7692 - val_loss: 0.4776 - val_auc: 0.8359 - val_accuracy: 0.7676\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4786 - val_auc: 0.8357 - val_accuracy: 0.7676\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4617 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4770 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8500 - accuracy: 0.7786 - val_loss: 0.4777 - val_auc: 0.8362 - val_accuracy: 0.7676\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8499 - accuracy: 0.7809 - val_loss: 0.4776 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8498 - accuracy: 0.7855 - val_loss: 0.4778 - val_auc: 0.8361 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8492 - accuracy: 0.7716 - val_loss: 0.4786 - val_auc: 0.8366 - val_accuracy: 0.7676\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8498 - accuracy: 0.7762 - val_loss: 0.4787 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8500 - accuracy: 0.7832 - val_loss: 0.4785 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8496 - accuracy: 0.7762 - val_loss: 0.4784 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8489 - accuracy: 0.7739 - val_loss: 0.4796 - val_auc: 0.8371 - val_accuracy: 0.7784\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4801 - val_auc: 0.8365 - val_accuracy: 0.7838\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4791 - val_auc: 0.8374 - val_accuracy: 0.7838\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8500 - accuracy: 0.7786 - val_loss: 0.4783 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4778 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4607 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4780 - val_auc: 0.8371 - val_accuracy: 0.7676\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7762 - val_loss: 0.4781 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8489 - accuracy: 0.7762 - val_loss: 0.4786 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4790 - val_auc: 0.8371 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4797 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8495 - accuracy: 0.7739 - val_loss: 0.4788 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8492 - accuracy: 0.7832 - val_loss: 0.4790 - val_auc: 0.8381 - val_accuracy: 0.7838\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8490 - accuracy: 0.7716 - val_loss: 0.4795 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4792 - val_auc: 0.8377 - val_accuracy: 0.7838\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4782 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4793 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4789 - val_auc: 0.8373 - val_accuracy: 0.7838\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8493 - accuracy: 0.7739 - val_loss: 0.4789 - val_auc: 0.8378 - val_accuracy: 0.7838\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4774 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4767 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8501 - accuracy: 0.7809 - val_loss: 0.4779 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8500 - accuracy: 0.7762 - val_loss: 0.4775 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4775 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4766 - val_auc: 0.8391 - val_accuracy: 0.7784\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4763 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8486 - accuracy: 0.7786 - val_loss: 0.4757 - val_auc: 0.8383 - val_accuracy: 0.7730\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8489 - accuracy: 0.7832 - val_loss: 0.4763 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4758 - val_auc: 0.8396 - val_accuracy: 0.7784\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4761 - val_auc: 0.8397 - val_accuracy: 0.7784\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8487 - accuracy: 0.7832 - val_loss: 0.4765 - val_auc: 0.8390 - val_accuracy: 0.7838\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8486 - accuracy: 0.7832 - val_loss: 0.4765 - val_auc: 0.8382 - val_accuracy: 0.7838\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8488 - accuracy: 0.7855 - val_loss: 0.4768 - val_auc: 0.8386 - val_accuracy: 0.7946\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8496 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8379 - val_accuracy: 0.7838\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8489 - accuracy: 0.7786 - val_loss: 0.4778 - val_auc: 0.8382 - val_accuracy: 0.7784\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4784 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4790 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7716 - val_loss: 0.4785 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8366 - val_accuracy: 0.7676\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4790 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8487 - accuracy: 0.7739 - val_loss: 0.4793 - val_auc: 0.8364 - val_accuracy: 0.7676\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4799 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4804 - val_auc: 0.8364 - val_accuracy: 0.7730\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8486 - accuracy: 0.7809 - val_loss: 0.4810 - val_auc: 0.8359 - val_accuracy: 0.7784\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4815 - val_auc: 0.8366 - val_accuracy: 0.7784\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4810 - val_auc: 0.8367 - val_accuracy: 0.7784\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8488 - accuracy: 0.7739 - val_loss: 0.4798 - val_auc: 0.8364 - val_accuracy: 0.7730\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4809 - val_auc: 0.8360 - val_accuracy: 0.7784\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4802 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8494 - accuracy: 0.7855 - val_loss: 0.4800 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8485 - accuracy: 0.7739 - val_loss: 0.4785 - val_auc: 0.8377 - val_accuracy: 0.7676\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8490 - accuracy: 0.7762 - val_loss: 0.4792 - val_auc: 0.8370 - val_accuracy: 0.7676\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8487 - accuracy: 0.7762 - val_loss: 0.4794 - val_auc: 0.8378 - val_accuracy: 0.7676\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4615 - auc: 0.8478 - accuracy: 0.7739 - val_loss: 0.4795 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7809 - val_loss: 0.4788 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4787 - val_auc: 0.8371 - val_accuracy: 0.7676\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4787 - val_auc: 0.8388 - val_accuracy: 0.7730\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4796 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4804 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7809 - val_loss: 0.4796 - val_auc: 0.8378 - val_accuracy: 0.7676\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4793 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4613 - auc: 0.8490 - accuracy: 0.7762 - val_loss: 0.4794 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8489 - accuracy: 0.7855 - val_loss: 0.4777 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4792 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4798 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8487 - accuracy: 0.7832 - val_loss: 0.4788 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4781 - val_auc: 0.8373 - val_accuracy: 0.7730\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4783 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8485 - accuracy: 0.7832 - val_loss: 0.4789 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8495 - accuracy: 0.7716 - val_loss: 0.4795 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7739 - val_loss: 0.4785 - val_auc: 0.8369 - val_accuracy: 0.7676\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4784 - val_auc: 0.8373 - val_accuracy: 0.7676\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8496 - accuracy: 0.7809 - val_loss: 0.4795 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4792 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4786 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8496 - accuracy: 0.7762 - val_loss: 0.4785 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4768 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8383 - val_accuracy: 0.7838\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7762 - val_loss: 0.4776 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4768 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4609 - auc: 0.8500 - accuracy: 0.7786 - val_loss: 0.4768 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4770 - val_auc: 0.8373 - val_accuracy: 0.7730\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7832 - val_loss: 0.4761 - val_auc: 0.8384 - val_accuracy: 0.7676\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4754 - val_auc: 0.8387 - val_accuracy: 0.7730\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4754 - val_auc: 0.8385 - val_accuracy: 0.7730\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8497 - accuracy: 0.7716 - val_loss: 0.4753 - val_auc: 0.8386 - val_accuracy: 0.7784\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4760 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8483 - accuracy: 0.7786 - val_loss: 0.4767 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4773 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7739 - val_loss: 0.4760 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4775 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4781 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4611 - auc: 0.8496 - accuracy: 0.7809 - val_loss: 0.4787 - val_auc: 0.8366 - val_accuracy: 0.7730\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4777 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4775 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4773 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4774 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8484 - accuracy: 0.7809 - val_loss: 0.4764 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8484 - accuracy: 0.7809 - val_loss: 0.4765 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8496 - accuracy: 0.7809 - val_loss: 0.4780 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8494 - accuracy: 0.7739 - val_loss: 0.4779 - val_auc: 0.8383 - val_accuracy: 0.7838\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7832 - val_loss: 0.4778 - val_auc: 0.8384 - val_accuracy: 0.7838\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8496 - accuracy: 0.7762 - val_loss: 0.4781 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4797 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8486 - accuracy: 0.7786 - val_loss: 0.4795 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8486 - accuracy: 0.7762 - val_loss: 0.4806 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4817 - val_auc: 0.8361 - val_accuracy: 0.7838\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7739 - val_loss: 0.4816 - val_auc: 0.8361 - val_accuracy: 0.7838\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8498 - accuracy: 0.7786 - val_loss: 0.4791 - val_auc: 0.8373 - val_accuracy: 0.7838\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4785 - val_auc: 0.8370 - val_accuracy: 0.7784\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4789 - val_auc: 0.8376 - val_accuracy: 0.7838\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4782 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8495 - accuracy: 0.7832 - val_loss: 0.4783 - val_auc: 0.8362 - val_accuracy: 0.7730\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4792 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4785 - val_auc: 0.8360 - val_accuracy: 0.7730\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4803 - val_auc: 0.8359 - val_accuracy: 0.7784\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8501 - accuracy: 0.7786 - val_loss: 0.4799 - val_auc: 0.8363 - val_accuracy: 0.7784\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8496 - accuracy: 0.7692 - val_loss: 0.4785 - val_auc: 0.8368 - val_accuracy: 0.7838\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8496 - accuracy: 0.7762 - val_loss: 0.4781 - val_auc: 0.8374 - val_accuracy: 0.7838\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4785 - val_auc: 0.8363 - val_accuracy: 0.7838\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8494 - accuracy: 0.7716 - val_loss: 0.4795 - val_auc: 0.8364 - val_accuracy: 0.7892\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4797 - val_auc: 0.8359 - val_accuracy: 0.7892\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8489 - accuracy: 0.7786 - val_loss: 0.4789 - val_auc: 0.8366 - val_accuracy: 0.7892\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8495 - accuracy: 0.7739 - val_loss: 0.4787 - val_auc: 0.8364 - val_accuracy: 0.7784\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4786 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8500 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8369 - val_accuracy: 0.7676\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8498 - accuracy: 0.7786 - val_loss: 0.4784 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7716 - val_loss: 0.4768 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4761 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8497 - accuracy: 0.7786 - val_loss: 0.4765 - val_auc: 0.8380 - val_accuracy: 0.7838\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4759 - val_auc: 0.8386 - val_accuracy: 0.7730\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7739 - val_loss: 0.4764 - val_auc: 0.8390 - val_accuracy: 0.7730\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8488 - accuracy: 0.7855 - val_loss: 0.4761 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8500 - accuracy: 0.7762 - val_loss: 0.4770 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7762 - val_loss: 0.4773 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7855 - val_loss: 0.4781 - val_auc: 0.8379 - val_accuracy: 0.7676\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4782 - val_auc: 0.8376 - val_accuracy: 0.7838\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7716 - val_loss: 0.4782 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8487 - accuracy: 0.7762 - val_loss: 0.4785 - val_auc: 0.8384 - val_accuracy: 0.7838\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4782 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8486 - accuracy: 0.7762 - val_loss: 0.4776 - val_auc: 0.8382 - val_accuracy: 0.7838\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4754 - val_auc: 0.8389 - val_accuracy: 0.7784\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8486 - accuracy: 0.7832 - val_loss: 0.4753 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8487 - accuracy: 0.7786 - val_loss: 0.4773 - val_auc: 0.8376 - val_accuracy: 0.7676\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4789 - val_auc: 0.8375 - val_accuracy: 0.7838\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8489 - accuracy: 0.7832 - val_loss: 0.4787 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8498 - accuracy: 0.7786 - val_loss: 0.4774 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4785 - val_auc: 0.8371 - val_accuracy: 0.7784\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4794 - val_auc: 0.8372 - val_accuracy: 0.7838\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7716 - val_loss: 0.4774 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8481 - accuracy: 0.7809 - val_loss: 0.4774 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4781 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8493 - accuracy: 0.7716 - val_loss: 0.4775 - val_auc: 0.8380 - val_accuracy: 0.7892\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4769 - val_auc: 0.8382 - val_accuracy: 0.7730\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4770 - val_auc: 0.8377 - val_accuracy: 0.7676\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8497 - accuracy: 0.7739 - val_loss: 0.4777 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8496 - accuracy: 0.7855 - val_loss: 0.4783 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4795 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7832 - val_loss: 0.4775 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8485 - accuracy: 0.7739 - val_loss: 0.4782 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4770 - val_auc: 0.8388 - val_accuracy: 0.7784\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7762 - val_loss: 0.4769 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8495 - accuracy: 0.7855 - val_loss: 0.4771 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4773 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4780 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4776 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7855 - val_loss: 0.4783 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4781 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7832 - val_loss: 0.4776 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8484 - accuracy: 0.7809 - val_loss: 0.4759 - val_auc: 0.8389 - val_accuracy: 0.7784\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4766 - val_auc: 0.8384 - val_accuracy: 0.7784\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7832 - val_loss: 0.4779 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7832 - val_loss: 0.4779 - val_auc: 0.8379 - val_accuracy: 0.7838\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4777 - val_auc: 0.8386 - val_accuracy: 0.7784\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8485 - accuracy: 0.7809 - val_loss: 0.4780 - val_auc: 0.8386 - val_accuracy: 0.7784\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4620 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4804 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4617 - auc: 0.8485 - accuracy: 0.7832 - val_loss: 0.4780 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4772 - val_auc: 0.8385 - val_accuracy: 0.7730\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8492 - accuracy: 0.7832 - val_loss: 0.4784 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8490 - accuracy: 0.7739 - val_loss: 0.4788 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4784 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8494 - accuracy: 0.7739 - val_loss: 0.4785 - val_auc: 0.8374 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230fab93bb0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "handed-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5066 - auc: 0.8034 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-convertible",
   "metadata": {},
   "source": [
    "# 10. Regularización\n",
    "La idea de la regularización es la de limitar aquellos pesos que son altos. De esta forma, se agrega una capa previa a la capa densa que contiene la capa de regularización. Se probarán dos regularizaciones distintas: L1 y L2 (donde el número significa el grado del término adicional que se suma a la función de costo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-platinum",
   "metadata": {},
   "source": [
    "# 10.1. Regularización L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "comfortable-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "simple-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "driven-electric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 4s 167ms/step - loss: 0.6201 - auc: 0.7419 - accuracy: 0.6711 - val_loss: 0.6180 - val_auc: 0.7409 - val_accuracy: 0.6865\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5508 - auc: 0.8026 - accuracy: 0.7234 - val_loss: 0.5748 - val_auc: 0.7719 - val_accuracy: 0.7243\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5414 - auc: 0.7976 - accuracy: 0.7349 - val_loss: 0.5507 - val_auc: 0.7869 - val_accuracy: 0.7297\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4954 - auc: 0.8321 - accuracy: 0.7551 - val_loss: 0.5342 - val_auc: 0.7945 - val_accuracy: 0.7459\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5141 - auc: 0.8208 - accuracy: 0.7381 - val_loss: 0.5232 - val_auc: 0.8036 - val_accuracy: 0.7514\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4748 - auc: 0.8444 - accuracy: 0.7418 - val_loss: 0.5132 - val_auc: 0.8093 - val_accuracy: 0.7622\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4675 - auc: 0.8480 - accuracy: 0.7554 - val_loss: 0.5084 - val_auc: 0.8122 - val_accuracy: 0.7568\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4723 - auc: 0.8450 - accuracy: 0.7620 - val_loss: 0.5040 - val_auc: 0.8179 - val_accuracy: 0.7514\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4871 - auc: 0.8378 - accuracy: 0.7588 - val_loss: 0.4984 - val_auc: 0.8215 - val_accuracy: 0.7622\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4862 - auc: 0.8302 - accuracy: 0.7517 - val_loss: 0.4957 - val_auc: 0.8240 - val_accuracy: 0.7459\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4758 - auc: 0.8463 - accuracy: 0.7671 - val_loss: 0.4945 - val_auc: 0.8248 - val_accuracy: 0.7514\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4630 - auc: 0.8469 - accuracy: 0.7864 - val_loss: 0.4918 - val_auc: 0.8257 - val_accuracy: 0.7514\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4790 - auc: 0.8507 - accuracy: 0.7489 - val_loss: 0.4907 - val_auc: 0.8266 - val_accuracy: 0.7459\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4668 - auc: 0.8524 - accuracy: 0.7647 - val_loss: 0.4876 - val_auc: 0.8300 - val_accuracy: 0.7514\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8485 - accuracy: 0.7668 - val_loss: 0.4862 - val_auc: 0.8305 - val_accuracy: 0.7568\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4736 - auc: 0.8383 - accuracy: 0.7565 - val_loss: 0.4844 - val_auc: 0.8311 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4576 - auc: 0.8478 - accuracy: 0.7818 - val_loss: 0.4833 - val_auc: 0.8337 - val_accuracy: 0.7622\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4439 - auc: 0.8611 - accuracy: 0.7805 - val_loss: 0.4832 - val_auc: 0.8335 - val_accuracy: 0.7568\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4678 - auc: 0.8400 - accuracy: 0.7695 - val_loss: 0.4841 - val_auc: 0.8333 - val_accuracy: 0.7514\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4547 - auc: 0.8514 - accuracy: 0.7874 - val_loss: 0.4834 - val_auc: 0.8343 - val_accuracy: 0.7568\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4522 - auc: 0.8548 - accuracy: 0.7865 - val_loss: 0.4838 - val_auc: 0.8348 - val_accuracy: 0.7676\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4638 - auc: 0.8431 - accuracy: 0.7659 - val_loss: 0.4840 - val_auc: 0.8353 - val_accuracy: 0.7676\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4904 - auc: 0.8406 - accuracy: 0.7460 - val_loss: 0.4826 - val_auc: 0.8357 - val_accuracy: 0.7730\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4530 - auc: 0.8594 - accuracy: 0.7662 - val_loss: 0.4814 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8347 - accuracy: 0.7791 - val_loss: 0.4822 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4669 - auc: 0.8418 - accuracy: 0.7775 - val_loss: 0.4824 - val_auc: 0.8365 - val_accuracy: 0.7622\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4771 - auc: 0.8424 - accuracy: 0.7711 - val_loss: 0.4819 - val_auc: 0.8367 - val_accuracy: 0.7676\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4433 - auc: 0.8660 - accuracy: 0.7855 - val_loss: 0.4816 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4943 - auc: 0.8214 - accuracy: 0.7620 - val_loss: 0.4820 - val_auc: 0.8362 - val_accuracy: 0.7676\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4331 - auc: 0.8721 - accuracy: 0.7987 - val_loss: 0.4814 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4646 - auc: 0.8528 - accuracy: 0.7738 - val_loss: 0.4820 - val_auc: 0.8366 - val_accuracy: 0.7730\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4541 - auc: 0.8534 - accuracy: 0.7771 - val_loss: 0.4829 - val_auc: 0.8357 - val_accuracy: 0.7622\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4688 - auc: 0.8466 - accuracy: 0.7790 - val_loss: 0.4827 - val_auc: 0.8359 - val_accuracy: 0.7676\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4475 - auc: 0.8668 - accuracy: 0.7830 - val_loss: 0.4823 - val_auc: 0.8367 - val_accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230faeb8e50>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 Regularization\n",
    "l1_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l1(0.01)))\n",
    "# Compiling model\n",
    "l1_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "precise-green",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5066 - auc: 0.8105 - accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "l1_model = load_model(mc_path)\n",
    "eval = l1_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-kidney",
   "metadata": {},
   "source": [
    "# 10.2. Regularización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "controversial-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "overhead-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 140ms/step - loss: 0.7352 - auc: 0.5639 - accuracy: 0.5537 - val_loss: 0.6543 - val_auc: 0.6827 - val_accuracy: 0.6541\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5937 - auc: 0.7635 - accuracy: 0.7051 - val_loss: 0.5901 - val_auc: 0.7542 - val_accuracy: 0.7297\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5169 - auc: 0.8254 - accuracy: 0.7661 - val_loss: 0.5570 - val_auc: 0.7847 - val_accuracy: 0.7189\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5102 - auc: 0.8320 - accuracy: 0.7676 - val_loss: 0.5358 - val_auc: 0.7974 - val_accuracy: 0.7189\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4971 - auc: 0.8339 - accuracy: 0.7589 - val_loss: 0.5224 - val_auc: 0.8058 - val_accuracy: 0.7405\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4887 - auc: 0.8374 - accuracy: 0.7680 - val_loss: 0.5134 - val_auc: 0.8126 - val_accuracy: 0.7459\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4767 - auc: 0.8398 - accuracy: 0.7880 - val_loss: 0.5088 - val_auc: 0.8158 - val_accuracy: 0.7568\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4826 - auc: 0.8398 - accuracy: 0.7744 - val_loss: 0.5050 - val_auc: 0.8187 - val_accuracy: 0.7514\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4377 - auc: 0.8716 - accuracy: 0.8115 - val_loss: 0.5012 - val_auc: 0.8206 - val_accuracy: 0.7514\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4760 - auc: 0.8439 - accuracy: 0.7724 - val_loss: 0.4981 - val_auc: 0.8233 - val_accuracy: 0.7568\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4907 - auc: 0.8301 - accuracy: 0.7654 - val_loss: 0.4939 - val_auc: 0.8267 - val_accuracy: 0.7676\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4358 - auc: 0.8696 - accuracy: 0.8026 - val_loss: 0.4910 - val_auc: 0.8286 - val_accuracy: 0.7676\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5048 - auc: 0.8196 - accuracy: 0.7434 - val_loss: 0.4903 - val_auc: 0.8309 - val_accuracy: 0.7730\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8454 - accuracy: 0.7598 - val_loss: 0.4887 - val_auc: 0.8301 - val_accuracy: 0.7676\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4641 - auc: 0.8440 - accuracy: 0.7750 - val_loss: 0.4875 - val_auc: 0.8320 - val_accuracy: 0.7676\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4637 - auc: 0.8403 - accuracy: 0.7714 - val_loss: 0.4875 - val_auc: 0.8333 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4943 - auc: 0.8321 - accuracy: 0.7544 - val_loss: 0.4871 - val_auc: 0.8330 - val_accuracy: 0.7676\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8451 - accuracy: 0.7645 - val_loss: 0.4859 - val_auc: 0.8343 - val_accuracy: 0.7676\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4837 - auc: 0.8313 - accuracy: 0.7784 - val_loss: 0.4856 - val_auc: 0.8342 - val_accuracy: 0.7784\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4688 - auc: 0.8488 - accuracy: 0.7682 - val_loss: 0.4860 - val_auc: 0.8347 - val_accuracy: 0.7838\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4619 - auc: 0.8446 - accuracy: 0.7602 - val_loss: 0.4846 - val_auc: 0.8361 - val_accuracy: 0.7838\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8501 - accuracy: 0.7760 - val_loss: 0.4831 - val_auc: 0.8367 - val_accuracy: 0.7838\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4521 - auc: 0.8582 - accuracy: 0.7820 - val_loss: 0.4827 - val_auc: 0.8375 - val_accuracy: 0.7838\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4847 - auc: 0.8315 - accuracy: 0.7842 - val_loss: 0.4820 - val_auc: 0.8368 - val_accuracy: 0.7838\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5049 - auc: 0.8186 - accuracy: 0.7406 - val_loss: 0.4828 - val_auc: 0.8364 - val_accuracy: 0.7838\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4769 - auc: 0.8388 - accuracy: 0.7733 - val_loss: 0.4830 - val_auc: 0.8360 - val_accuracy: 0.7892\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4724 - auc: 0.8438 - accuracy: 0.7767 - val_loss: 0.4835 - val_auc: 0.8361 - val_accuracy: 0.7838\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4275 - auc: 0.8789 - accuracy: 0.7986 - val_loss: 0.4824 - val_auc: 0.8354 - val_accuracy: 0.7730\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4598 - auc: 0.8471 - accuracy: 0.8000 - val_loss: 0.4829 - val_auc: 0.8361 - val_accuracy: 0.7730\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4707 - auc: 0.8373 - accuracy: 0.7525 - val_loss: 0.4828 - val_auc: 0.8364 - val_accuracy: 0.7838\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4210 - auc: 0.8751 - accuracy: 0.8004 - val_loss: 0.4834 - val_auc: 0.8359 - val_accuracy: 0.7838\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4823 - auc: 0.8372 - accuracy: 0.7735 - val_loss: 0.4815 - val_auc: 0.8366 - val_accuracy: 0.7838\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4984 - auc: 0.8196 - accuracy: 0.7563 - val_loss: 0.4815 - val_auc: 0.8367 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230fd395370>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L2 Regularization\n",
    "l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "burning-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5053 - auc: 0.8068 - accuracy: 0.7338\n"
     ]
    }
   ],
   "source": [
    "l2_model = load_model(mc_path)\n",
    "eval = l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-mobile",
   "metadata": {},
   "source": [
    "# 10.3. Regularización L1+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "continent-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1+L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "satisfactory-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 4s 208ms/step - loss: 0.7727 - auc: 0.4977 - accuracy: 0.4991 - val_loss: 0.6240 - val_auc: 0.7002 - val_accuracy: 0.7081\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6091 - auc: 0.7283 - accuracy: 0.6924 - val_loss: 0.5672 - val_auc: 0.7642 - val_accuracy: 0.7135\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5413 - auc: 0.8051 - accuracy: 0.7536 - val_loss: 0.5407 - val_auc: 0.7890 - val_accuracy: 0.7243\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5353 - auc: 0.7914 - accuracy: 0.7343 - val_loss: 0.5260 - val_auc: 0.8001 - val_accuracy: 0.7297\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4991 - auc: 0.8340 - accuracy: 0.7480 - val_loss: 0.5150 - val_auc: 0.8083 - val_accuracy: 0.7297\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4754 - auc: 0.8494 - accuracy: 0.7604 - val_loss: 0.5074 - val_auc: 0.8151 - val_accuracy: 0.7459\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4830 - auc: 0.8399 - accuracy: 0.7720 - val_loss: 0.5007 - val_auc: 0.8209 - val_accuracy: 0.7459\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4930 - auc: 0.8326 - accuracy: 0.7569 - val_loss: 0.4961 - val_auc: 0.8226 - val_accuracy: 0.7514\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8529 - accuracy: 0.7736 - val_loss: 0.4919 - val_auc: 0.8267 - val_accuracy: 0.7514\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4457 - auc: 0.8678 - accuracy: 0.7860 - val_loss: 0.4899 - val_auc: 0.8277 - val_accuracy: 0.7514\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4732 - auc: 0.8485 - accuracy: 0.7591 - val_loss: 0.4882 - val_auc: 0.8297 - val_accuracy: 0.7514\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4998 - auc: 0.8193 - accuracy: 0.7300 - val_loss: 0.4873 - val_auc: 0.8296 - val_accuracy: 0.7568\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4505 - auc: 0.8574 - accuracy: 0.7592 - val_loss: 0.4845 - val_auc: 0.8313 - val_accuracy: 0.7568\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8335 - accuracy: 0.7497 - val_loss: 0.4855 - val_auc: 0.8305 - val_accuracy: 0.7514\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8502 - accuracy: 0.7666 - val_loss: 0.4853 - val_auc: 0.8314 - val_accuracy: 0.7514\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4435 - auc: 0.8730 - accuracy: 0.7874 - val_loss: 0.4841 - val_auc: 0.8316 - val_accuracy: 0.7459\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4380 - auc: 0.8643 - accuracy: 0.7979 - val_loss: 0.4838 - val_auc: 0.8326 - val_accuracy: 0.7514\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4654 - auc: 0.8497 - accuracy: 0.7698 - val_loss: 0.4832 - val_auc: 0.8331 - val_accuracy: 0.7514\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4791 - auc: 0.8313 - accuracy: 0.7662 - val_loss: 0.4836 - val_auc: 0.8339 - val_accuracy: 0.7514\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4982 - auc: 0.8230 - accuracy: 0.7432 - val_loss: 0.4821 - val_auc: 0.8340 - val_accuracy: 0.7514\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4544 - auc: 0.8529 - accuracy: 0.7876 - val_loss: 0.4824 - val_auc: 0.8351 - val_accuracy: 0.7514\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4560 - auc: 0.8588 - accuracy: 0.7727 - val_loss: 0.4828 - val_auc: 0.8345 - val_accuracy: 0.7568\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4908 - auc: 0.8236 - accuracy: 0.7498 - val_loss: 0.4833 - val_auc: 0.8346 - val_accuracy: 0.7622\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4476 - auc: 0.8632 - accuracy: 0.7564 - val_loss: 0.4830 - val_auc: 0.8348 - val_accuracy: 0.7568\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4835 - auc: 0.8345 - accuracy: 0.7598 - val_loss: 0.4834 - val_auc: 0.8348 - val_accuracy: 0.7514\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4670 - auc: 0.8497 - accuracy: 0.7668 - val_loss: 0.4823 - val_auc: 0.8353 - val_accuracy: 0.7622\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4564 - auc: 0.8484 - accuracy: 0.7697 - val_loss: 0.4827 - val_auc: 0.8353 - val_accuracy: 0.7622\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4531 - auc: 0.8546 - accuracy: 0.7745 - val_loss: 0.4828 - val_auc: 0.8349 - val_accuracy: 0.7676\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4574 - auc: 0.8531 - accuracy: 0.7828 - val_loss: 0.4808 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8364 - accuracy: 0.7516 - val_loss: 0.4818 - val_auc: 0.8364 - val_accuracy: 0.7676\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4875 - auc: 0.8329 - accuracy: 0.7571 - val_loss: 0.4809 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4405 - auc: 0.8716 - accuracy: 0.7884 - val_loss: 0.4811 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4772 - auc: 0.8352 - accuracy: 0.7655 - val_loss: 0.4816 - val_auc: 0.8364 - val_accuracy: 0.7676\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4459 - auc: 0.8627 - accuracy: 0.7851 - val_loss: 0.4812 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4536 - auc: 0.8572 - accuracy: 0.7903 - val_loss: 0.4809 - val_auc: 0.8366 - val_accuracy: 0.7730\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4580 - auc: 0.8408 - accuracy: 0.7677 - val_loss: 0.4811 - val_auc: 0.8373 - val_accuracy: 0.7676\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4750 - auc: 0.8485 - accuracy: 0.7657 - val_loss: 0.4826 - val_auc: 0.8359 - val_accuracy: 0.7676\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4485 - auc: 0.8528 - accuracy: 0.7857 - val_loss: 0.4830 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8470 - accuracy: 0.7809 - val_loss: 0.4823 - val_auc: 0.8368 - val_accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230fd500a30>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 and L2 Regularization\n",
    "l1l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l1l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "placed-paraguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5057 - auc: 0.8078 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "l1l2_model = load_model(mc_path)\n",
    "eval = l1l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-innocent",
   "metadata": {},
   "source": [
    "En este caso, se nota una leve mejora en la métrica empleando regularización L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-tenant",
   "metadata": {},
   "source": [
    "# 11. Dropout\n",
    "Se emplea una capa extra de dropout para minimizar el overfitting. Este regularizador funciona ignorando a neuronas de forma aleatoria. Se realiza dropout **solo en la etapa de entrenamiento**. **En teoría, no se lleva muy bien con la normalización por capas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "female-stretch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "similar-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/dropout_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "expanded-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4807 - auc: 0.8356 - accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4806887209415436, 0.8356101512908936, 0.7621621489524841]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model\n",
    "do_model = Sequential()\n",
    "# Adding dropout layer to network\n",
    "do_model.add(Dropout(0))\n",
    "# Adding Dense layer\n",
    "do_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "do_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "do_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "do_model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "nutritional-receipt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4804 - auc: 0.8379 - accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.480373352766037, 0.8378870487213135, 0.7783783674240112]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-flour",
   "metadata": {},
   "source": [
    "# 12. Feature Engineering. Features Polinomiales\n",
    "El objertivo de esta sección es el de agregar variables de entrada al modelo, que surgen de combinar las variables originales. El grado del polinomio determina la cantidad de nuevas variables que se suman al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "prepared-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/poly2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "tough-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 120ms/step - loss: 0.8060 - auc: 0.5846 - accuracy: 0.5907 - val_loss: 0.6144 - val_auc: 0.7154 - val_accuracy: 0.6865\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6004 - auc: 0.7192 - accuracy: 0.7175 - val_loss: 0.5454 - val_auc: 0.7689 - val_accuracy: 0.7027\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5227 - auc: 0.8230 - accuracy: 0.7444 - val_loss: 0.5025 - val_auc: 0.8031 - val_accuracy: 0.7405\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4879 - auc: 0.8403 - accuracy: 0.7599 - val_loss: 0.4909 - val_auc: 0.8130 - val_accuracy: 0.7514\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4700 - auc: 0.8498 - accuracy: 0.7643 - val_loss: 0.4855 - val_auc: 0.8206 - val_accuracy: 0.7514\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4769 - auc: 0.8524 - accuracy: 0.7851 - val_loss: 0.4777 - val_auc: 0.8306 - val_accuracy: 0.7405\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4546 - auc: 0.8616 - accuracy: 0.7944 - val_loss: 0.4778 - val_auc: 0.8322 - val_accuracy: 0.7514\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4107 - auc: 0.8923 - accuracy: 0.8092 - val_loss: 0.4834 - val_auc: 0.8340 - val_accuracy: 0.7514\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4442 - auc: 0.8789 - accuracy: 0.7759 - val_loss: 0.4852 - val_auc: 0.8314 - val_accuracy: 0.7459\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3851 - auc: 0.9017 - accuracy: 0.8099 - val_loss: 0.4842 - val_auc: 0.8326 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4112 - auc: 0.8862 - accuracy: 0.7813 - val_loss: 0.4861 - val_auc: 0.8333 - val_accuracy: 0.7514\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4317 - auc: 0.8818 - accuracy: 0.7683 - val_loss: 0.4906 - val_auc: 0.8315 - val_accuracy: 0.7622\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4090 - auc: 0.8866 - accuracy: 0.7951 - val_loss: 0.4932 - val_auc: 0.8309 - val_accuracy: 0.7568\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4258 - auc: 0.8770 - accuracy: 0.7689 - val_loss: 0.4934 - val_auc: 0.8326 - val_accuracy: 0.7622\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4289 - auc: 0.8719 - accuracy: 0.7783 - val_loss: 0.4937 - val_auc: 0.8336 - val_accuracy: 0.7730\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4359 - auc: 0.8731 - accuracy: 0.7640 - val_loss: 0.5020 - val_auc: 0.8264 - val_accuracy: 0.7459\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4377 - auc: 0.8622 - accuracy: 0.7637 - val_loss: 0.5009 - val_auc: 0.8277 - val_accuracy: 0.7568\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4026 - auc: 0.8931 - accuracy: 0.7931 - val_loss: 0.5030 - val_auc: 0.8274 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230844210a0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating polynomial features\n",
    "poly2 = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly2.fit(x_train)\n",
    "# Creating model\n",
    "p2_model  = Sequential()\n",
    "p2_model.add(Dense(1, input_shape=(poly2.n_output_features_,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "p2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Fit model\n",
    "p2_model.fit(poly2.transform(x_train), y_train, validation_data=(poly2.transform(x_valid), y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "liberal-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5157 - auc: 0.8048 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "p2_model = load_model(mc_path)\n",
    "eval = p2_model.evaluate(x=poly2.transform(x_test), y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-grammar",
   "metadata": {},
   "source": [
    "A continuación se observa la progresión de la métrica en **train** y **valid** en función del grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "controversial-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-spelling",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: Realizar la normalización de los datos **después** de aplicar el feature polinomial, sino se rompe todo :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "gothic-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polynomial degrees to train and compute metrics\n",
    "poly_degrees = np.arange(1, 11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "human-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LRS callback\n",
    "# Define learning rate at start\n",
    "ilr = 0.2 # ilr=0.5, ds = 100000, dr=0.8, stc=False\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=1000, decay_rate=0.8, staircase=True) # Decay every (decay_steps) steps with a base of (decay_rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "mechanical-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model Checkpoint callback.\n",
    "mc_path = 'model_checkpoints/get_best_poly_deg_checkpoint.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-interpretation",
   "metadata": {},
   "source": [
    "En este punto cabe aclarar que se probó el parámetro *interaction_only* del preprocesador de polinomios y se llegó a la conclusión de que el desempeño mejora con este valor en *True*. Esto es así dado que, al activarlo, se logra un número mucho menor de variables en cada orden. Esto contribuye ampliamente a **reducir el overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "wrapped-beijing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Polynomial order = 1 ---\n",
      "Input count = 8\n",
      "AUC for TRAIN subset is 0.8496\n",
      "AUC for VALID subset is 0.8402\n",
      "--- Polynomial order = 2 ---\n",
      "Input count = 36\n",
      "AUC for TRAIN subset is 0.8763\n",
      "AUC for VALID subset is 0.8411\n",
      "--- Polynomial order = 3 ---\n",
      "Input count = 92\n",
      "AUC for TRAIN subset is 0.9070\n",
      "AUC for VALID subset is 0.8176\n",
      "--- Polynomial order = 4 ---\n",
      "Input count = 162\n",
      "AUC for TRAIN subset is 0.8843\n",
      "AUC for VALID subset is 0.8054\n",
      "--- Polynomial order = 5 ---\n",
      "Input count = 218\n",
      "AUC for TRAIN subset is 0.9191\n",
      "AUC for VALID subset is 0.8209\n",
      "--- Polynomial order = 6 ---\n",
      "Input count = 246\n",
      "AUC for TRAIN subset is 0.9237\n",
      "AUC for VALID subset is 0.8298\n",
      "--- Polynomial order = 7 ---\n",
      "Input count = 254\n",
      "AUC for TRAIN subset is 0.9072\n",
      "AUC for VALID subset is 0.8308\n",
      "--- Polynomial order = 8 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9195\n",
      "AUC for VALID subset is 0.8304\n",
      "--- Polynomial order = 9 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9199\n",
      "AUC for VALID subset is 0.8123\n",
      "--- Polynomial order = 10 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9205\n",
      "AUC for VALID subset is 0.8062\n",
      "Wall time: 28.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=True)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    \n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(1e-4)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "focal-demographic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "opened-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KFold from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_crossval(model, x, y, metrics, n_splits=2, shuffle=False, random_state=None, epochs=1, verbose='auto', callbacks=None, batch_size=32):\n",
    "    x = x.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    # Initialize kfold splitter\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    fold_num = 1\n",
    "    \n",
    "    # Create arrays for metrics\n",
    "    auc_valid = []\n",
    "    auc_train = []\n",
    "    \n",
    "    for train_index, valid_index in kf.split(x):\n",
    "        print(f'----- Fold N = {fold_num} -----')\n",
    "        # Get train and valid splits\n",
    "        x_train, y_train = x[train_index], y[train_index]\n",
    "        x_valid, y_valid = x[valid_index], y[valid_index]\n",
    "        \n",
    "        # Fit model\n",
    "        history = model.fit(x=x_train, \n",
    "                  y=y_train, \n",
    "                  validation_data=(x_valid, y_valid), \n",
    "                  shuffle=shuffle, \n",
    "                  epochs=epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  verbose=verbose, \n",
    "                  callbacks=callbacks\n",
    "                 )\n",
    "        \n",
    "        # Get metrics\n",
    "        eval_train = model.evaluate(x=x_train, y=y_train, return_dict=True, verbose=0)\n",
    "        eval_valid = model.evaluate(x=x_valid, y=y_valid, return_dict=True, verbose=0)\n",
    "        \n",
    "        tscore = eval_train['auc']\n",
    "        vscore = eval_valid['auc']\n",
    "        \n",
    "        tsize = x_train.shape[0]\n",
    "        vsize = x_valid.shape[0]\n",
    "        \n",
    "        # Append metrics\n",
    "        auc_train.append(tscore)\n",
    "        auc_valid.append(vscore)\n",
    "        \n",
    "        print(f'Size for TRAIN is {tsize}')\n",
    "        print(f'Size for VALID is {vsize}')\n",
    "        \n",
    "        print(f'AUC for TRAIN is {tscore:.4f}')\n",
    "        print(f'AUC for VALID is {vscore:.4f}')\n",
    "        \n",
    "        # Increase fold number\n",
    "        fold_num = fold_num + 1\n",
    "        \n",
    "    return auc_train, auc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "offensive-rating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold N = 1 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8045\n",
      "AUC for VALID is 0.9381\n",
      "----- Fold N = 2 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8586\n",
      "AUC for VALID is 0.7780\n",
      "----- Fold N = 3 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8558\n",
      "AUC for VALID is 0.7894\n",
      "----- Fold N = 4 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8441\n",
      "AUC for VALID is 0.8399\n",
      "----- Fold N = 5 -----\n",
      "Size for TRAIN is 344\n",
      "Size for VALID is 85\n",
      "AUC for TRAIN is 0.8417\n",
      "AUC for VALID is 0.8429\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "auct, aucv = kfold_crossval(model=model, x=x_train, y=y_train, metrics=metrics,\n",
    "                            n_splits=5, shuffle=False, random_state=None, epochs=1000, verbose=0, \n",
    "                            callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "#es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "vocal-beijing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC for TRAIN is 0.8409\n",
      "Average AUC for VALID is 0.8376\n"
     ]
    }
   ],
   "source": [
    "aucv_mean = np.mean(aucv)\n",
    "auct_mean = np.mean(auct)\n",
    "print(f'Average AUC for TRAIN is {auct_mean:.4f}')\n",
    "print(f'Average AUC for VALID is {aucv_mean:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
