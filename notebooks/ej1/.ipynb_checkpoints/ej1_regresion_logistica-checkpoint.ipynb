{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daily-tumor",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 2 - Ejercicio 1 - Regresión Logística\n",
    "# Notebook #2: Implementación de una Regresión Lineal\n",
    "En esta notebook se busca implementar una regresión logística para poder estimar la condición de diabético de un paciente, perteneciente al Pima Indians Dataset analizado en la notebook anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-banks",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "* Chequear correcto reemplazo de NaN por mean.\n",
    "* Meter el z-score en scripts comunes a ambos ejercicios. Chequear StandardScaler **correctamente inicializado**. **¿Errores de discretización?**\n",
    "    * ¿Dónde meto el área bajo la curva ROC y el F2? -> Respondido por Luqui y Karina.\n",
    "* Añadir **tensorboard** para log entre epochs. Migrar **TODOS LOS GRÁFICOS** a TensorBoard.\n",
    "    * Agregar evolución de f2-score sobre train en selección del umbral.\n",
    "* Graficar **learning rate**.\n",
    "* Sacar los evaluate con **test**, para evitar malas interpretaciones.\n",
    "* PRIMERA PRUEBA DE POLY (2) ESTÁ MAL! **Falta normalizar despues del poly**\n",
    "* Informar métricas secundarias\n",
    "\n",
    "# ¿Qué cosas puedo variar?\n",
    "* Función de activación:\n",
    "    * Sigmoid\n",
    "    * RELU\n",
    "    * ELU\n",
    "    * tanh\n",
    "    * Leaky RELU\n",
    "    \n",
    "* Optimizador:\n",
    "    * SGD\n",
    "    * Adam\n",
    "    \n",
    "* Early Stopping: Para el entrenamiento cuando la **loss** deja de mejorar. Se pasa a través de un **callback**. (https://keras.io/api/callbacks/early_stopping/) (https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "* Kernel Initializer: Esto es, como son los pesos y bias iniciales. (https://keras.io/api/layers/initializers/)\n",
    "* Model Checkpoint: Guarda un checkpoint del modelo. Puede configurarse para elegir el mejor. Se pasa por **callback**. (https://keras.io/api/callbacks/model_checkpoint/)\n",
    "* Scheduling Learning Rate: Se hace variar el **learning rate** con una función. Es un **callback**. (https://keras.io/api/callbacks/learning_rate_scheduler/)\n",
    "* Reg. dropout: Para evitar overfitting, la capa de dropout \"borra\" una entrada de forma aleatoria y escala el resto. Es una **capa**. (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "* Regularización L1 y L2: Limita el espacio de soluciones agregando un término a la **función de costo**. (https://keras.io/api/layers/regularizers/)\n",
    "* Data Augmentation\n",
    "* Batch Normalization: Normaliza las entradas (media=0, dev=1). Es una **capa**. (https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-figure",
   "metadata": {},
   "source": [
    "# Dudas\n",
    "* Al generar la métrica F2, ¿me devuelve por batch o por epoch? -> Esto finalmente se explica más adelante.\n",
    "* Al evaluar el predict en threshold selection ¿batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-basin",
   "metadata": {},
   "source": [
    "# ¿Cuáles son los requerimientos para el **clasificador**?\n",
    "* Métrica principal: **Área bajo la curva ROC**\n",
    "* Buscar el **umbral de decisión** para maximizar el **f2 score** \n",
    "* Informar métricas secundarias:\n",
    "    * Especificidad - Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "    * Sensibilidad\n",
    "    * Valor predictivo positivo\n",
    "    * Valor predictivo negativo\n",
    "    \n",
    "* **Pregunta adicional**:\n",
    "Dada la situación en la cual cambia la prevalencia de la enfermedad en la población a ser del 20%. Se desea reutilizar el modelo sin volver a entrenar, ¿Cómo lo harían? ¿Qué métricas se mantienen igual y cuáles cambiarian?. **¿clases desbalanceadas -> class weight?**. Las f-score son buenas para casos no balanceados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-replication",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indirect-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "terminal-antarctica",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "permanent-bristol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read database from .csv\n",
    "df = pd.read_csv('../../databases/diabetes.csv', delimiter=',')\n",
    "\n",
    "# Show first rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-olive",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-cartoon",
   "metadata": {},
   "source": [
    "## 2.1 Filtrado de valores inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ranging-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Glucose values\n",
    "df['Glucose'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Blood Pressure values\n",
    "df['BloodPressure'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Skin Thickness values\n",
    "df['SkinThickness'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Insulin values\n",
    "df['Insulin'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Body Mass Index values\n",
    "df['BMI'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-price",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naughty-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acting-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alive-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output variables for the model\n",
    "x_labels = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "y_labels = ['Outcome']\n",
    "\n",
    "df_x = df[x_labels]\n",
    "df_y = df[y_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "operating-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Split the train_valid sub-dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "russian-sodium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>426.000000</td>\n",
       "      <td>406.000000</td>\n",
       "      <td>287.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.906760</td>\n",
       "      <td>120.514085</td>\n",
       "      <td>72.325123</td>\n",
       "      <td>28.930314</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>32.247857</td>\n",
       "      <td>0.469189</td>\n",
       "      <td>33.207459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.350363</td>\n",
       "      <td>29.742282</td>\n",
       "      <td>12.611486</td>\n",
       "      <td>10.041280</td>\n",
       "      <td>107.966521</td>\n",
       "      <td>7.030966</td>\n",
       "      <td>0.330389</td>\n",
       "      <td>11.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>27.175000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>31.750000</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>185.750000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   429.000000  426.000000     406.000000     287.000000  216.000000   \n",
       "mean      3.906760  120.514085      72.325123      28.930314  152.740741   \n",
       "std       3.350363   29.742282      12.611486      10.041280  107.966521   \n",
       "min       0.000000   56.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000  100.000000      64.000000      22.000000   82.000000   \n",
       "50%       3.000000  115.000000      74.000000      29.000000  128.000000   \n",
       "75%       6.000000  138.000000      80.000000      36.000000  185.750000   \n",
       "max      14.000000  198.000000     122.000000      63.000000  680.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  420.000000                429.000000  429.000000  \n",
       "mean    32.247857                  0.469189   33.207459  \n",
       "std      7.030966                  0.330389   11.602100  \n",
       "min     18.200000                  0.085000   21.000000  \n",
       "25%     27.175000                  0.240000   24.000000  \n",
       "50%     31.750000                  0.351000   29.000000  \n",
       "75%     36.300000                  0.646000   41.000000  \n",
       "max     67.100000                  2.420000   72.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set before NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-cedar",
   "metadata": {},
   "source": [
    "# 4. Reemplazo de valores inválidos\n",
    "Como se destacó en el análisis estadístico de datos, el dataset suministrado posee varios valores faltantes en algunos individuos. Se asume que en la etapa de producción el modelo contará con todas las variables correctamente informadas, no admitiendo el faltante de alguna de ellas. Luego, se decide reemplazar aquellos valores inválidos en **train**, **valid** y **test** por la correspondiente media en el dataset de train. En este caso, se considera a la media como un estimador correcto para la ocasión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "composed-conjunction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Select columns to perform non-valid values replacement\n",
    "replace_labels = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "\n",
    "# Perform NaN replacement\n",
    "for label in replace_labels:\n",
    "    # Compute mean for particular column and replace in train, valid and test\n",
    "    mean = np.nanmean(x_train[label])\n",
    "    x_train[label].replace(np.nan, mean, inplace=True)\n",
    "    x_valid[label].replace(np.nan, mean, inplace=True)\n",
    "    x_test[label].replace(np.nan, mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "protected-colorado",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.906760</td>\n",
       "      <td>120.514085</td>\n",
       "      <td>72.325123</td>\n",
       "      <td>28.930314</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>32.247857</td>\n",
       "      <td>0.469189</td>\n",
       "      <td>33.207459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.350363</td>\n",
       "      <td>29.637862</td>\n",
       "      <td>12.267947</td>\n",
       "      <td>8.208243</td>\n",
       "      <td>76.522025</td>\n",
       "      <td>6.956649</td>\n",
       "      <td>0.330389</td>\n",
       "      <td>11.602100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>72.325123</td>\n",
       "      <td>28.930314</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>36.100000</td>\n",
       "      <td>0.646000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   429.000000  429.000000     429.000000     429.000000  429.000000   \n",
       "mean      3.906760  120.514085      72.325123      28.930314  152.740741   \n",
       "std       3.350363   29.637862      12.267947       8.208243   76.522025   \n",
       "min       0.000000   56.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000  100.000000      64.000000      26.000000  126.000000   \n",
       "50%       3.000000  116.000000      72.325123      28.930314  152.740741   \n",
       "75%       6.000000  138.000000      80.000000      32.000000  152.740741   \n",
       "max      14.000000  198.000000     122.000000      63.000000  680.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  429.000000                429.000000  429.000000  \n",
       "mean    32.247857                  0.469189   33.207459  \n",
       "std      6.956649                  0.330389   11.602100  \n",
       "min     18.200000                  0.085000   21.000000  \n",
       "25%     27.300000                  0.240000   24.000000  \n",
       "50%     32.000000                  0.351000   29.000000  \n",
       "75%     36.100000                  0.646000   41.000000  \n",
       "max     67.100000                  2.420000   72.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set after NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "civilian-jesus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.908108</td>\n",
       "      <td>124.081233</td>\n",
       "      <td>73.325949</td>\n",
       "      <td>29.225539</td>\n",
       "      <td>156.113313</td>\n",
       "      <td>32.909988</td>\n",
       "      <td>0.471708</td>\n",
       "      <td>33.464865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.474627</td>\n",
       "      <td>30.668200</td>\n",
       "      <td>11.926671</td>\n",
       "      <td>8.568083</td>\n",
       "      <td>87.393680</td>\n",
       "      <td>6.636033</td>\n",
       "      <td>0.325015</td>\n",
       "      <td>11.883040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>28.800000</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.930314</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>36.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>57.300000</td>\n",
       "      <td>1.893000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   185.000000  185.000000     185.000000     185.000000  185.000000   \n",
       "mean      3.908108  124.081233      73.325949      29.225539  156.113313   \n",
       "std       3.474627   30.668200      11.926671       8.568083   87.393680   \n",
       "min       0.000000   44.000000      50.000000      11.000000   15.000000   \n",
       "25%       1.000000  101.000000      65.000000      24.000000  115.000000   \n",
       "50%       3.000000  120.000000      72.000000      28.930314  152.740741   \n",
       "75%       6.000000  145.000000      80.000000      34.000000  152.740741   \n",
       "max      17.000000  199.000000     114.000000      60.000000  545.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  185.000000                185.000000  185.000000  \n",
       "mean    32.909988                  0.471708   33.464865  \n",
       "std      6.636033                  0.325015   11.883040  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     28.800000                  0.236000   24.000000  \n",
       "50%     32.500000                  0.389000   30.000000  \n",
       "75%     36.900000                  0.600000   40.000000  \n",
       "max     57.300000                  1.893000   81.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set after NaN replacement\n",
    "x_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "published-second",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.597403</td>\n",
       "      <td>122.038961</td>\n",
       "      <td>71.503903</td>\n",
       "      <td>29.359428</td>\n",
       "      <td>155.872054</td>\n",
       "      <td>32.482778</td>\n",
       "      <td>0.479565</td>\n",
       "      <td>33.064935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.304818</td>\n",
       "      <td>32.320876</td>\n",
       "      <td>11.814455</td>\n",
       "      <td>10.513698</td>\n",
       "      <td>103.288567</td>\n",
       "      <td>6.946169</td>\n",
       "      <td>0.343303</td>\n",
       "      <td>12.118519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>26.925000</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.930314</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>32.273929</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>142.750000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>152.740741</td>\n",
       "      <td>36.950000</td>\n",
       "      <td>0.603750</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.329000</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   154.000000  154.000000     154.000000     154.000000  154.000000   \n",
       "mean      3.597403  122.038961      71.503903      29.359428  155.872054   \n",
       "std       3.304818   32.320876      11.814455      10.513698  103.288567   \n",
       "min       0.000000   61.000000      30.000000       7.000000   23.000000   \n",
       "25%       1.000000   95.250000      64.000000      23.250000  108.250000   \n",
       "50%       3.000000  117.000000      72.000000      28.930314  152.740741   \n",
       "75%       5.750000  142.750000      80.000000      33.750000  152.740741   \n",
       "max      13.000000  197.000000     106.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  154.000000                154.000000  154.000000  \n",
       "mean    32.482778                  0.479565   33.064935  \n",
       "std      6.946169                  0.343303   12.118519  \n",
       "min     18.400000                  0.078000   21.000000  \n",
       "25%     26.925000                  0.254000   24.000000  \n",
       "50%     32.273929                  0.376500   28.000000  \n",
       "75%     36.950000                  0.603750   41.000000  \n",
       "max     55.000000                  2.329000   69.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set after NaN replacement\n",
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-israeli",
   "metadata": {},
   "source": [
    "# 5. Normalización de datos de entrada. Z Score. \n",
    "Dado que todas las variables en juego son numéricas, se puede aplicar z-score a todo el dataset. Esta operación se hace con el objetivo de poder obtener mayor información de los pesos calculados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "narrow-snake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! Backup unnormalized subsets for further utilization\n",
    "x_train_un = x_train\n",
    "x_valid_un = x_valid\n",
    "x_test_un = x_test\n",
    "\n",
    "# Apply z-score to all sub-datasets\n",
    "scalable_variables = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])\n",
    "    x_valid.loc[:, scalable_variables] = scaler.transform(x_valid.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "warming-enhancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.140692e-17</td>\n",
       "      <td>-7.349728e-17</td>\n",
       "      <td>-5.341493e-16</td>\n",
       "      <td>-5.300086e-16</td>\n",
       "      <td>4.140692e-17</td>\n",
       "      <td>-6.211038e-18</td>\n",
       "      <td>5.175865e-17</td>\n",
       "      <td>-1.407835e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.167432e+00</td>\n",
       "      <td>-2.179287e+00</td>\n",
       "      <td>-3.943736e+00</td>\n",
       "      <td>-2.674862e+00</td>\n",
       "      <td>-1.815199e+00</td>\n",
       "      <td>-2.021700e+00</td>\n",
       "      <td>-1.164196e+00</td>\n",
       "      <td>-1.053405e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.686085e-01</td>\n",
       "      <td>-6.929662e-01</td>\n",
       "      <td>-6.794000e-01</td>\n",
       "      <td>-3.574133e-01</td>\n",
       "      <td>-3.498596e-01</td>\n",
       "      <td>-7.120718e-01</td>\n",
       "      <td>-6.945041e-01</td>\n",
       "      <td>-7.945294e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.709613e-01</td>\n",
       "      <td>-1.524859e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.333280e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.567041e-02</td>\n",
       "      <td>-3.581441e-01</td>\n",
       "      <td>-3.630697e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.255096e-01</td>\n",
       "      <td>5.906746e-01</td>\n",
       "      <td>6.263344e-01</td>\n",
       "      <td>3.744127e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.543819e-01</td>\n",
       "      <td>5.357857e-01</td>\n",
       "      <td>6.724333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.016099e+00</td>\n",
       "      <td>2.617476e+00</td>\n",
       "      <td>4.053887e+00</td>\n",
       "      <td>4.155514e+00</td>\n",
       "      <td>6.898339e+00</td>\n",
       "      <td>5.015753e+00</td>\n",
       "      <td>5.911486e+00</td>\n",
       "      <td>3.347483e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pregnancies       Glucose  BloodPressure  SkinThickness       Insulin  \\\n",
       "count  4.290000e+02  4.290000e+02   4.290000e+02   4.290000e+02  4.290000e+02   \n",
       "mean  -4.140692e-17 -7.349728e-17  -5.341493e-16  -5.300086e-16  4.140692e-17   \n",
       "std    1.001168e+00  1.001168e+00   1.001168e+00   1.001168e+00  1.001168e+00   \n",
       "min   -1.167432e+00 -2.179287e+00  -3.943736e+00  -2.674862e+00 -1.815199e+00   \n",
       "25%   -8.686085e-01 -6.929662e-01  -6.794000e-01  -3.574133e-01 -3.498596e-01   \n",
       "50%   -2.709613e-01 -1.524859e-01   0.000000e+00  -4.333280e-16  0.000000e+00   \n",
       "75%    6.255096e-01  5.906746e-01   6.263344e-01   3.744127e-01  0.000000e+00   \n",
       "max    3.016099e+00  2.617476e+00   4.053887e+00   4.155514e+00  6.898339e+00   \n",
       "\n",
       "                BMI  DiabetesPedigreeFunction           Age  \n",
       "count  4.290000e+02              4.290000e+02  4.290000e+02  \n",
       "mean  -6.211038e-18              5.175865e-17 -1.407835e-16  \n",
       "std    1.001168e+00              1.001168e+00  1.001168e+00  \n",
       "min   -2.021700e+00             -1.164196e+00 -1.053405e+00  \n",
       "25%   -7.120718e-01             -6.945041e-01 -7.945294e-01  \n",
       "50%   -3.567041e-02             -3.581441e-01 -3.630697e-01  \n",
       "75%    5.543819e-01              5.357857e-01  6.724333e-01  \n",
       "max    5.015753e+00              5.911486e+00  3.347483e+00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-trademark",
   "metadata": {},
   "source": [
    "# 6. Regresión Logística - Test #1\n",
    "Primera prueba de regresión logística. Se usa SGD y AUC como métrica principal. Se emplea la Binary Cross-Entropy como loss subrogada, dado que **la AUC no es diferenciable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "furnished-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TensorBoard for learning logging\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thousand-berlin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "hindu-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import SensitivityAtSpecificity\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tracked-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/logistic_regression_first_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "broadband-colorado",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Get model brief\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "phantom-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics definition\n",
    "metrics = ['AUC', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "specified-denmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 15s 146ms/step - loss: 0.8922 - auc: 0.5191 - accuracy: 0.4975 - val_loss: 0.9092 - val_auc: 0.4580 - val_accuracy: 0.4595\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.8648 - auc: 0.5298 - accuracy: 0.4829 - val_loss: 0.8809 - val_auc: 0.4759 - val_accuracy: 0.4703\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7755 - auc: 0.5928 - accuracy: 0.5323 - val_loss: 0.8522 - val_auc: 0.4914 - val_accuracy: 0.4811\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7370 - auc: 0.6192 - accuracy: 0.5706 - val_loss: 0.8271 - val_auc: 0.5090 - val_accuracy: 0.5081\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7192 - auc: 0.6352 - accuracy: 0.5832 - val_loss: 0.8039 - val_auc: 0.5264 - val_accuracy: 0.5459\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7976 - auc: 0.5423 - accuracy: 0.5397 - val_loss: 0.7808 - val_auc: 0.5474 - val_accuracy: 0.5622\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.7165 - auc: 0.6196 - accuracy: 0.6004 - val_loss: 0.7603 - val_auc: 0.5671 - val_accuracy: 0.5676\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6617 - auc: 0.6696 - accuracy: 0.6249 - val_loss: 0.7416 - val_auc: 0.5844 - val_accuracy: 0.5568\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6704 - auc: 0.6540 - accuracy: 0.6412 - val_loss: 0.7235 - val_auc: 0.6004 - val_accuracy: 0.5676\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6717 - auc: 0.6592 - accuracy: 0.6403 - val_loss: 0.7072 - val_auc: 0.6148 - val_accuracy: 0.5892\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6836 - auc: 0.6558 - accuracy: 0.6203 - val_loss: 0.6923 - val_auc: 0.6319 - val_accuracy: 0.6108\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6360 - auc: 0.6890 - accuracy: 0.6494 - val_loss: 0.6779 - val_auc: 0.6477 - val_accuracy: 0.6162\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6281 - auc: 0.7086 - accuracy: 0.6580 - val_loss: 0.6648 - val_auc: 0.6632 - val_accuracy: 0.6216\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6454 - auc: 0.6720 - accuracy: 0.6180 - val_loss: 0.6523 - val_auc: 0.6745 - val_accuracy: 0.6270\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6159 - auc: 0.7058 - accuracy: 0.6711 - val_loss: 0.6405 - val_auc: 0.6885 - val_accuracy: 0.6324\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6180 - auc: 0.6979 - accuracy: 0.6576 - val_loss: 0.6294 - val_auc: 0.7038 - val_accuracy: 0.6595\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6283 - auc: 0.6977 - accuracy: 0.6661 - val_loss: 0.6194 - val_auc: 0.7150 - val_accuracy: 0.6703\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5814 - auc: 0.7257 - accuracy: 0.6778 - val_loss: 0.6096 - val_auc: 0.7262 - val_accuracy: 0.6757\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5960 - auc: 0.7303 - accuracy: 0.6832 - val_loss: 0.6011 - val_auc: 0.7353 - val_accuracy: 0.6811\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5904 - auc: 0.7209 - accuracy: 0.6826 - val_loss: 0.5930 - val_auc: 0.7436 - val_accuracy: 0.6973\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5641 - auc: 0.7526 - accuracy: 0.6983 - val_loss: 0.5852 - val_auc: 0.7518 - val_accuracy: 0.7135\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5550 - auc: 0.7607 - accuracy: 0.7161 - val_loss: 0.5781 - val_auc: 0.7607 - val_accuracy: 0.7081\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5528 - auc: 0.7671 - accuracy: 0.7233 - val_loss: 0.5713 - val_auc: 0.7684 - val_accuracy: 0.7081\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5460 - auc: 0.7794 - accuracy: 0.7257 - val_loss: 0.5648 - val_auc: 0.7756 - val_accuracy: 0.7135\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5567 - auc: 0.7567 - accuracy: 0.7101 - val_loss: 0.5589 - val_auc: 0.7795 - val_accuracy: 0.7297\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5207 - auc: 0.7995 - accuracy: 0.7444 - val_loss: 0.5533 - val_auc: 0.7866 - val_accuracy: 0.7297\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5129 - auc: 0.7968 - accuracy: 0.7512 - val_loss: 0.5484 - val_auc: 0.7924 - val_accuracy: 0.7351\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5326 - auc: 0.7806 - accuracy: 0.7344 - val_loss: 0.5437 - val_auc: 0.7977 - val_accuracy: 0.7405\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5227 - auc: 0.7989 - accuracy: 0.7552 - val_loss: 0.5393 - val_auc: 0.8029 - val_accuracy: 0.7405\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5536 - auc: 0.7734 - accuracy: 0.7126 - val_loss: 0.5351 - val_auc: 0.8074 - val_accuracy: 0.7351\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4971 - auc: 0.8254 - accuracy: 0.7548 - val_loss: 0.5311 - val_auc: 0.8116 - val_accuracy: 0.7297\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5139 - auc: 0.8041 - accuracy: 0.7575 - val_loss: 0.5273 - val_auc: 0.8155 - val_accuracy: 0.7405\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5135 - auc: 0.8052 - accuracy: 0.7388 - val_loss: 0.5238 - val_auc: 0.8174 - val_accuracy: 0.7514\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5137 - auc: 0.7888 - accuracy: 0.7354 - val_loss: 0.5207 - val_auc: 0.8198 - val_accuracy: 0.7459\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4882 - auc: 0.8229 - accuracy: 0.7676 - val_loss: 0.5175 - val_auc: 0.8236 - val_accuracy: 0.7459\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5229 - auc: 0.8019 - accuracy: 0.7390 - val_loss: 0.5142 - val_auc: 0.8259 - val_accuracy: 0.7459\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5121 - auc: 0.8055 - accuracy: 0.7546 - val_loss: 0.5112 - val_auc: 0.8285 - val_accuracy: 0.7514\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5111 - auc: 0.8032 - accuracy: 0.7529 - val_loss: 0.5085 - val_auc: 0.8296 - val_accuracy: 0.7514\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5188 - auc: 0.7873 - accuracy: 0.7381 - val_loss: 0.5061 - val_auc: 0.8324 - val_accuracy: 0.7514\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4964 - auc: 0.8152 - accuracy: 0.7690 - val_loss: 0.5038 - val_auc: 0.8343 - val_accuracy: 0.7514\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4779 - auc: 0.8409 - accuracy: 0.7749 - val_loss: 0.5017 - val_auc: 0.8355 - val_accuracy: 0.7514\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4798 - auc: 0.8423 - accuracy: 0.7714 - val_loss: 0.4994 - val_auc: 0.8373 - val_accuracy: 0.7568\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4883 - auc: 0.8213 - accuracy: 0.7552 - val_loss: 0.4975 - val_auc: 0.8391 - val_accuracy: 0.7514\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4970 - auc: 0.8191 - accuracy: 0.7632 - val_loss: 0.4956 - val_auc: 0.8402 - val_accuracy: 0.7568\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4732 - auc: 0.8437 - accuracy: 0.7820 - val_loss: 0.4937 - val_auc: 0.8416 - val_accuracy: 0.7622\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5227 - auc: 0.8047 - accuracy: 0.7306 - val_loss: 0.4919 - val_auc: 0.8423 - val_accuracy: 0.7622\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4735 - auc: 0.8408 - accuracy: 0.7781 - val_loss: 0.4902 - val_auc: 0.8435 - val_accuracy: 0.7568\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4689 - auc: 0.8431 - accuracy: 0.7783 - val_loss: 0.4885 - val_auc: 0.8445 - val_accuracy: 0.7568\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4754 - auc: 0.8356 - accuracy: 0.7679 - val_loss: 0.4868 - val_auc: 0.8461 - val_accuracy: 0.7568\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4742 - auc: 0.8305 - accuracy: 0.7836 - val_loss: 0.4852 - val_auc: 0.8472 - val_accuracy: 0.7568\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4620 - auc: 0.8543 - accuracy: 0.8035 - val_loss: 0.4838 - val_auc: 0.8485 - val_accuracy: 0.7514\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4922 - auc: 0.8240 - accuracy: 0.7579 - val_loss: 0.4824 - val_auc: 0.8490 - val_accuracy: 0.7514\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4867 - auc: 0.8283 - accuracy: 0.7694 - val_loss: 0.4810 - val_auc: 0.8500 - val_accuracy: 0.7568\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5152 - auc: 0.8001 - accuracy: 0.7450 - val_loss: 0.4797 - val_auc: 0.8506 - val_accuracy: 0.7622\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4945 - auc: 0.8238 - accuracy: 0.7417 - val_loss: 0.4786 - val_auc: 0.8514 - val_accuracy: 0.7622\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4876 - auc: 0.8253 - accuracy: 0.7698 - val_loss: 0.4774 - val_auc: 0.8519 - val_accuracy: 0.7622\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5025 - auc: 0.8257 - accuracy: 0.7632 - val_loss: 0.4763 - val_auc: 0.8528 - val_accuracy: 0.7622\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4858 - auc: 0.8375 - accuracy: 0.7636 - val_loss: 0.4752 - val_auc: 0.8534 - val_accuracy: 0.7622\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4600 - auc: 0.8538 - accuracy: 0.7832 - val_loss: 0.4742 - val_auc: 0.8540 - val_accuracy: 0.7622\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4672 - auc: 0.8421 - accuracy: 0.7867 - val_loss: 0.4734 - val_auc: 0.8546 - val_accuracy: 0.7622\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4937 - auc: 0.8218 - accuracy: 0.7738 - val_loss: 0.4725 - val_auc: 0.8548 - val_accuracy: 0.7622\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4868 - auc: 0.8311 - accuracy: 0.7835 - val_loss: 0.4715 - val_auc: 0.8561 - val_accuracy: 0.7622\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4632 - auc: 0.8407 - accuracy: 0.7795 - val_loss: 0.4707 - val_auc: 0.8558 - val_accuracy: 0.7622\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8376 - accuracy: 0.7720 - val_loss: 0.4699 - val_auc: 0.8566 - val_accuracy: 0.7622\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4641 - auc: 0.8402 - accuracy: 0.7902 - val_loss: 0.4692 - val_auc: 0.8576 - val_accuracy: 0.7568\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4971 - auc: 0.8196 - accuracy: 0.7559 - val_loss: 0.4685 - val_auc: 0.8574 - val_accuracy: 0.7568\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4409 - auc: 0.8699 - accuracy: 0.7997 - val_loss: 0.4676 - val_auc: 0.8581 - val_accuracy: 0.7568\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4564 - auc: 0.8575 - accuracy: 0.7934 - val_loss: 0.4669 - val_auc: 0.8584 - val_accuracy: 0.7514\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4790 - auc: 0.8381 - accuracy: 0.7584 - val_loss: 0.4662 - val_auc: 0.8582 - val_accuracy: 0.7514\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4521 - auc: 0.8617 - accuracy: 0.7870 - val_loss: 0.4655 - val_auc: 0.8585 - val_accuracy: 0.7514\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4560 - auc: 0.8494 - accuracy: 0.7825 - val_loss: 0.4650 - val_auc: 0.8585 - val_accuracy: 0.7514\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4635 - auc: 0.8479 - accuracy: 0.7884 - val_loss: 0.4643 - val_auc: 0.8588 - val_accuracy: 0.7514\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4504 - auc: 0.8618 - accuracy: 0.7903 - val_loss: 0.4638 - val_auc: 0.8590 - val_accuracy: 0.7514\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4785 - auc: 0.8416 - accuracy: 0.7590 - val_loss: 0.4632 - val_auc: 0.8597 - val_accuracy: 0.7514\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4518 - auc: 0.8542 - accuracy: 0.7991 - val_loss: 0.4626 - val_auc: 0.8594 - val_accuracy: 0.7514\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4698 - auc: 0.8452 - accuracy: 0.7810 - val_loss: 0.4621 - val_auc: 0.8599 - val_accuracy: 0.7514\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4493 - auc: 0.8421 - accuracy: 0.7919 - val_loss: 0.4615 - val_auc: 0.8599 - val_accuracy: 0.7514\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4577 - auc: 0.8485 - accuracy: 0.7648 - val_loss: 0.4610 - val_auc: 0.8594 - val_accuracy: 0.7514\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4998 - auc: 0.8251 - accuracy: 0.7530 - val_loss: 0.4605 - val_auc: 0.8597 - val_accuracy: 0.7514\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4924 - auc: 0.8275 - accuracy: 0.7566 - val_loss: 0.4600 - val_auc: 0.8604 - val_accuracy: 0.7514\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4671 - auc: 0.8424 - accuracy: 0.7889 - val_loss: 0.4595 - val_auc: 0.8602 - val_accuracy: 0.7514\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4774 - auc: 0.8411 - accuracy: 0.7708 - val_loss: 0.4590 - val_auc: 0.8611 - val_accuracy: 0.7514\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4812 - auc: 0.8260 - accuracy: 0.7549 - val_loss: 0.4586 - val_auc: 0.8613 - val_accuracy: 0.7514\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4680 - auc: 0.8365 - accuracy: 0.7885 - val_loss: 0.4582 - val_auc: 0.8609 - val_accuracy: 0.7514\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4404 - auc: 0.8696 - accuracy: 0.8100 - val_loss: 0.4578 - val_auc: 0.8610 - val_accuracy: 0.7514\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4942 - auc: 0.8198 - accuracy: 0.7648 - val_loss: 0.4574 - val_auc: 0.8619 - val_accuracy: 0.7514\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4766 - auc: 0.8393 - accuracy: 0.7759 - val_loss: 0.4570 - val_auc: 0.8621 - val_accuracy: 0.7568\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8415 - accuracy: 0.7808 - val_loss: 0.4566 - val_auc: 0.8619 - val_accuracy: 0.7568\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4866 - auc: 0.8297 - accuracy: 0.7684 - val_loss: 0.4562 - val_auc: 0.8622 - val_accuracy: 0.7568\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4601 - auc: 0.8465 - accuracy: 0.7949 - val_loss: 0.4559 - val_auc: 0.8624 - val_accuracy: 0.7568\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4524 - auc: 0.8372 - accuracy: 0.7871 - val_loss: 0.4556 - val_auc: 0.8624 - val_accuracy: 0.7568\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4508 - auc: 0.8632 - accuracy: 0.8064 - val_loss: 0.4554 - val_auc: 0.8628 - val_accuracy: 0.7568\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4324 - auc: 0.8721 - accuracy: 0.8064 - val_loss: 0.4551 - val_auc: 0.8632 - val_accuracy: 0.7568\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8472 - accuracy: 0.7791 - val_loss: 0.4549 - val_auc: 0.8634 - val_accuracy: 0.7568\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4455 - auc: 0.8614 - accuracy: 0.7939 - val_loss: 0.4546 - val_auc: 0.8628 - val_accuracy: 0.7568\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4810 - auc: 0.8437 - accuracy: 0.7583 - val_loss: 0.4543 - val_auc: 0.8627 - val_accuracy: 0.7568\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3962 - auc: 0.8960 - accuracy: 0.8280 - val_loss: 0.4540 - val_auc: 0.8632 - val_accuracy: 0.7568\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4353 - auc: 0.8645 - accuracy: 0.8004 - val_loss: 0.4537 - val_auc: 0.8633 - val_accuracy: 0.7622\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4452 - auc: 0.8694 - accuracy: 0.7909 - val_loss: 0.4534 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4511 - auc: 0.8559 - accuracy: 0.8096 - val_loss: 0.4531 - val_auc: 0.8631 - val_accuracy: 0.7622\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8467 - accuracy: 0.7668 - val_loss: 0.4529 - val_auc: 0.8630 - val_accuracy: 0.7622\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4545 - auc: 0.8650 - accuracy: 0.8027 - val_loss: 0.4527 - val_auc: 0.8632 - val_accuracy: 0.7622\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4699 - auc: 0.8485 - accuracy: 0.7890 - val_loss: 0.4524 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4559 - auc: 0.8483 - accuracy: 0.7925 - val_loss: 0.4522 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4606 - auc: 0.8484 - accuracy: 0.7960 - val_loss: 0.4520 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4470 - auc: 0.8557 - accuracy: 0.7875 - val_loss: 0.4518 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4542 - auc: 0.8447 - accuracy: 0.8015 - val_loss: 0.4516 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5020 - auc: 0.8292 - accuracy: 0.7502 - val_loss: 0.4514 - val_auc: 0.8632 - val_accuracy: 0.7622\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4683 - auc: 0.8499 - accuracy: 0.7953 - val_loss: 0.4511 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4579 - auc: 0.8581 - accuracy: 0.7844 - val_loss: 0.4509 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4201 - auc: 0.8851 - accuracy: 0.8287 - val_loss: 0.4507 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4312 - auc: 0.8698 - accuracy: 0.8224 - val_loss: 0.4506 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8320 - accuracy: 0.7876 - val_loss: 0.4504 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4820 - auc: 0.8339 - accuracy: 0.7691 - val_loss: 0.4502 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4557 - auc: 0.8477 - accuracy: 0.7934 - val_loss: 0.4501 - val_auc: 0.8633 - val_accuracy: 0.7676\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4732 - auc: 0.8343 - accuracy: 0.7758 - val_loss: 0.4499 - val_auc: 0.8632 - val_accuracy: 0.7676\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4362 - auc: 0.8630 - accuracy: 0.8054 - val_loss: 0.4498 - val_auc: 0.8635 - val_accuracy: 0.7676\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4698 - auc: 0.8480 - accuracy: 0.7983 - val_loss: 0.4497 - val_auc: 0.8632 - val_accuracy: 0.7676\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4911 - auc: 0.8330 - accuracy: 0.7673 - val_loss: 0.4496 - val_auc: 0.8631 - val_accuracy: 0.7676\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4499 - auc: 0.8584 - accuracy: 0.7947 - val_loss: 0.4495 - val_auc: 0.8632 - val_accuracy: 0.7676\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4421 - auc: 0.8584 - accuracy: 0.8055 - val_loss: 0.4493 - val_auc: 0.8632 - val_accuracy: 0.7676\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4471 - auc: 0.8599 - accuracy: 0.7988 - val_loss: 0.4492 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4637 - auc: 0.8427 - accuracy: 0.7883 - val_loss: 0.4490 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4430 - auc: 0.8701 - accuracy: 0.8024 - val_loss: 0.4489 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4386 - auc: 0.8647 - accuracy: 0.8085 - val_loss: 0.4488 - val_auc: 0.8633 - val_accuracy: 0.7676\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8494 - accuracy: 0.7931 - val_loss: 0.4487 - val_auc: 0.8635 - val_accuracy: 0.7676\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8268 - accuracy: 0.7773 - val_loss: 0.4486 - val_auc: 0.8633 - val_accuracy: 0.7676\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4663 - auc: 0.8493 - accuracy: 0.7873 - val_loss: 0.4486 - val_auc: 0.8631 - val_accuracy: 0.7676\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4759 - auc: 0.8398 - accuracy: 0.8001 - val_loss: 0.4484 - val_auc: 0.8627 - val_accuracy: 0.7676\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4986 - auc: 0.8169 - accuracy: 0.7653 - val_loss: 0.4483 - val_auc: 0.8629 - val_accuracy: 0.7676\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4603 - auc: 0.8504 - accuracy: 0.7704 - val_loss: 0.4482 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4807 - auc: 0.8379 - accuracy: 0.7877 - val_loss: 0.4480 - val_auc: 0.8632 - val_accuracy: 0.7676\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4762 - auc: 0.8388 - accuracy: 0.7901 - val_loss: 0.4479 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4371 - auc: 0.8639 - accuracy: 0.8224 - val_loss: 0.4478 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4662 - auc: 0.8469 - accuracy: 0.7784 - val_loss: 0.4477 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4730 - auc: 0.8489 - accuracy: 0.7788 - val_loss: 0.4476 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4736 - auc: 0.8418 - accuracy: 0.7841 - val_loss: 0.4475 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4297 - auc: 0.8593 - accuracy: 0.8229 - val_loss: 0.4474 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4679 - auc: 0.8407 - accuracy: 0.7776 - val_loss: 0.4473 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4680 - auc: 0.8413 - accuracy: 0.7937 - val_loss: 0.4472 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4723 - auc: 0.8449 - accuracy: 0.7819 - val_loss: 0.4471 - val_auc: 0.8638 - val_accuracy: 0.7622\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4938 - auc: 0.8214 - accuracy: 0.7833 - val_loss: 0.4471 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4352 - auc: 0.8678 - accuracy: 0.8238 - val_loss: 0.4470 - val_auc: 0.8640 - val_accuracy: 0.7622\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4756 - auc: 0.8238 - accuracy: 0.7676 - val_loss: 0.4469 - val_auc: 0.8640 - val_accuracy: 0.7622\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4637 - auc: 0.8374 - accuracy: 0.7937 - val_loss: 0.4468 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4453 - auc: 0.8565 - accuracy: 0.7985 - val_loss: 0.4467 - val_auc: 0.8640 - val_accuracy: 0.7622\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4621 - auc: 0.8451 - accuracy: 0.7897 - val_loss: 0.4466 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4398 - auc: 0.8583 - accuracy: 0.8112 - val_loss: 0.4465 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5090 - auc: 0.8225 - accuracy: 0.7747 - val_loss: 0.4465 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4641 - auc: 0.8382 - accuracy: 0.7906 - val_loss: 0.4464 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4760 - auc: 0.8352 - accuracy: 0.7801 - val_loss: 0.4463 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4680 - auc: 0.8428 - accuracy: 0.7837 - val_loss: 0.4463 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4469 - auc: 0.8496 - accuracy: 0.8000 - val_loss: 0.4462 - val_auc: 0.8645 - val_accuracy: 0.7622\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4511 - auc: 0.8542 - accuracy: 0.7925 - val_loss: 0.4461 - val_auc: 0.8646 - val_accuracy: 0.7622\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4759 - auc: 0.8308 - accuracy: 0.7833 - val_loss: 0.4460 - val_auc: 0.8646 - val_accuracy: 0.7622\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4660 - auc: 0.8398 - accuracy: 0.7747 - val_loss: 0.4460 - val_auc: 0.8646 - val_accuracy: 0.7622\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4223 - auc: 0.8790 - accuracy: 0.8174 - val_loss: 0.4460 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4658 - auc: 0.8499 - accuracy: 0.7895 - val_loss: 0.4459 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4543 - auc: 0.8498 - accuracy: 0.7946 - val_loss: 0.4459 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4410 - auc: 0.8641 - accuracy: 0.7908 - val_loss: 0.4459 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4794 - auc: 0.8305 - accuracy: 0.7717 - val_loss: 0.4458 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4303 - auc: 0.8703 - accuracy: 0.8044 - val_loss: 0.4458 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4868 - auc: 0.8318 - accuracy: 0.7716 - val_loss: 0.4457 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4687 - auc: 0.8371 - accuracy: 0.7686 - val_loss: 0.4457 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4626 - auc: 0.8395 - accuracy: 0.7972 - val_loss: 0.4456 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4288 - auc: 0.8722 - accuracy: 0.8253 - val_loss: 0.4456 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4973 - auc: 0.8282 - accuracy: 0.7788 - val_loss: 0.4455 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4405 - auc: 0.8684 - accuracy: 0.7934 - val_loss: 0.4455 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4766 - auc: 0.8407 - accuracy: 0.7777 - val_loss: 0.4454 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4832 - auc: 0.8318 - accuracy: 0.7619 - val_loss: 0.4453 - val_auc: 0.8635 - val_accuracy: 0.7622\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4632 - auc: 0.8440 - accuracy: 0.8028 - val_loss: 0.4452 - val_auc: 0.8638 - val_accuracy: 0.7622\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4330 - auc: 0.8656 - accuracy: 0.8120 - val_loss: 0.4452 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4598 - auc: 0.8356 - accuracy: 0.8040 - val_loss: 0.4451 - val_auc: 0.8638 - val_accuracy: 0.7622\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4490 - auc: 0.8645 - accuracy: 0.7972 - val_loss: 0.4451 - val_auc: 0.8635 - val_accuracy: 0.7622\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4740 - auc: 0.8375 - accuracy: 0.7988 - val_loss: 0.4451 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4505 - auc: 0.8441 - accuracy: 0.7854 - val_loss: 0.4450 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4505 - auc: 0.8528 - accuracy: 0.7990 - val_loss: 0.4449 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4506 - auc: 0.8557 - accuracy: 0.7998 - val_loss: 0.4449 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4408 - auc: 0.8604 - accuracy: 0.8102 - val_loss: 0.4449 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4330 - auc: 0.8677 - accuracy: 0.8219 - val_loss: 0.4448 - val_auc: 0.8637 - val_accuracy: 0.7622\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4577 - auc: 0.8561 - accuracy: 0.7910 - val_loss: 0.4447 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4738 - auc: 0.8327 - accuracy: 0.7818 - val_loss: 0.4447 - val_auc: 0.8633 - val_accuracy: 0.7622\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4412 - auc: 0.8745 - accuracy: 0.8034 - val_loss: 0.4447 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8552 - accuracy: 0.7877 - val_loss: 0.4446 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4545 - auc: 0.8582 - accuracy: 0.7917 - val_loss: 0.4446 - val_auc: 0.8637 - val_accuracy: 0.7622\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4754 - auc: 0.8231 - accuracy: 0.8020 - val_loss: 0.4446 - val_auc: 0.8638 - val_accuracy: 0.7622\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4669 - auc: 0.8448 - accuracy: 0.7935 - val_loss: 0.4446 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4870 - auc: 0.8176 - accuracy: 0.7764 - val_loss: 0.4445 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8313 - accuracy: 0.7961 - val_loss: 0.4445 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8429 - accuracy: 0.7829 - val_loss: 0.4444 - val_auc: 0.8638 - val_accuracy: 0.7622\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4345 - auc: 0.8716 - accuracy: 0.8035 - val_loss: 0.4445 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4440 - auc: 0.8644 - accuracy: 0.7996 - val_loss: 0.4444 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8471 - accuracy: 0.7862 - val_loss: 0.4444 - val_auc: 0.8637 - val_accuracy: 0.7622\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4515 - auc: 0.8494 - accuracy: 0.7891 - val_loss: 0.4444 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4540 - auc: 0.8501 - accuracy: 0.7930 - val_loss: 0.4444 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4677 - auc: 0.8542 - accuracy: 0.7895 - val_loss: 0.4443 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4543 - auc: 0.8501 - accuracy: 0.7896 - val_loss: 0.4443 - val_auc: 0.8638 - val_accuracy: 0.7622\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4394 - auc: 0.8610 - accuracy: 0.8041 - val_loss: 0.4443 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4314 - auc: 0.8727 - accuracy: 0.8247 - val_loss: 0.4443 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4449 - auc: 0.8638 - accuracy: 0.8016 - val_loss: 0.4442 - val_auc: 0.8638 - val_accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd7806e190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=1, callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bulgarian-beaver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b72e4f6ffc5c6668\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b72e4f6ffc5c6668\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard launch\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "optical-gamma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5276 - auc: 0.7984 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "model = load_model(mc_path)\n",
    "eval = model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-blowing",
   "metadata": {},
   "source": [
    "# 7. Elección del umbral usando f2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-invitation",
   "metadata": {},
   "source": [
    "A la prueba anterior, se suma la selección del umbral (o **threshiold**) con el cual el clasificador discrimina entre clases. El mejor umbral de clasificación se calcula para todos los modelos, después del correspondiente entrenamiento. Para esta elección se elije el mejor valor del f2-score sobre el subset de **valid**. También se muestra la evolución de esta métrica respecto al umbral en el subset de **train**. En teoría, este umbral **no modifica la mérica principal del modelo, que es el área bajo la curva ROC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "continental-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def round_threshold(vector, threshold=0.5):\n",
    "    rounded_vector = []\n",
    "    for element in vector:\n",
    "        if element >= threshold:\n",
    "            rounded_vector.append(1)\n",
    "        else:\n",
    "            rounded_vector.append(0)\n",
    "            \n",
    "    return np.array(rounded_vector)\n",
    "        \n",
    "def f2_threshold_selection(y_probs_valid, y_true_valid, y_probs_train, y_true_train, steps=100, plot=True):\n",
    "    # Thresholds and f2-score vectors\n",
    "    thresholds = np.linspace(0, 1, steps)\n",
    "    f2_score_valid = []\n",
    "    f2_score_train = []\n",
    "    \n",
    "    for thld in thresholds:\n",
    "        # Generate predictions with current threshold\n",
    "        y_pred_valid = round_threshold(vector=y_probs_valid, threshold=thld)\n",
    "        y_pred_train = round_threshold(vector=y_probs_train, threshold=thld)\n",
    "        # Compute f2 score for that threshold and append\n",
    "        score_valid = fbeta_score(y_true=y_true_valid, y_pred=y_pred_valid, beta=2)\n",
    "        score_train = fbeta_score(y_true=y_true_train, y_pred=y_pred_train, beta=2)\n",
    "        f2_score_valid.append(score_valid)\n",
    "        f2_score_train.append(score_train)\n",
    "    \n",
    "    idx = np.argmax(f2_score_valid)\n",
    "    if plot == True:\n",
    "        plt.plot(thresholds, f2_score_valid, label='valid')\n",
    "        plt.plot(thresholds, f2_score_train, label='train')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('F2 score')\n",
    "        plt.axvline(thresholds[idx], color='black', linestyle='--')\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "        plt.grid(b=True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return thresholds, f2_score_valid, idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "coastal-information",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.26262626262626265"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get binary-class probability from model\n",
    "y_probs_valid = model.predict(x_valid)\n",
    "y_probs_train = model.predict(x_train)\n",
    "thresholds, f2_score, idx = f2_threshold_selection(y_probs_valid, y_valid, y_probs_train, y_train, steps=100)\n",
    "thresholds[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-secondary",
   "metadata": {},
   "source": [
    "# 8. Early Stopping\n",
    "Habiendo concluido el test #1, se cree necesario agregar un callback de early stopping al modelo. Este callback deberá detener el proceso de aprendizaje en el momento en el que la **métrica principal** del modelo **deje de aumentar**. Posteriormente, se recupera el modelo con mejor performance en cuanto a esta métrica (AUC). Cabe aclarar que esta técnica es especialmente útil cuando la métrica principal no es diferenciable, y por ende se debe emplear una **loss subrogada** (en este caso, la binary cross entropy). De esta forma, el número de epochs que recorra el proceso de entrenamiento se verá limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vulnerable-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Early Stopping callback from keras.\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "documented-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Early Stopping callback\n",
    "es_callback = EarlyStopping(monitor='val_auc', mode='max', min_delta=0.001, patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "facial-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Checkpoint callback\n",
    "mc_path = 'model_checkpoints/early_stopping_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "pointed-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "es_model = Sequential()\n",
    "es_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "municipal-archives",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "14/14 [==============================] - 3s 176ms/step - loss: 0.8009 - auc: 0.5596 - accuracy: 0.5417 - val_loss: 0.7978 - val_auc: 0.5632 - val_accuracy: 0.5405\n",
      "Epoch 2/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.7611 - auc: 0.5977 - accuracy: 0.5891 - val_loss: 0.7813 - val_auc: 0.5742 - val_accuracy: 0.5459\n",
      "Epoch 3/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.7732 - auc: 0.5818 - accuracy: 0.5513 - val_loss: 0.7662 - val_auc: 0.5866 - val_accuracy: 0.5568\n",
      "Epoch 4/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7625 - auc: 0.5792 - accuracy: 0.5600 - val_loss: 0.7516 - val_auc: 0.5987 - val_accuracy: 0.5784\n",
      "Epoch 5/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7425 - auc: 0.6159 - accuracy: 0.5750 - val_loss: 0.7381 - val_auc: 0.6088 - val_accuracy: 0.5784\n",
      "Epoch 6/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7271 - auc: 0.6126 - accuracy: 0.5877 - val_loss: 0.7250 - val_auc: 0.6180 - val_accuracy: 0.5784\n",
      "Epoch 7/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7015 - auc: 0.6278 - accuracy: 0.6046 - val_loss: 0.7124 - val_auc: 0.6288 - val_accuracy: 0.5784\n",
      "Epoch 8/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7051 - auc: 0.6472 - accuracy: 0.6241 - val_loss: 0.7005 - val_auc: 0.6407 - val_accuracy: 0.5892\n",
      "Epoch 9/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6831 - auc: 0.6534 - accuracy: 0.6314 - val_loss: 0.6893 - val_auc: 0.6496 - val_accuracy: 0.5946\n",
      "Epoch 10/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6723 - auc: 0.6640 - accuracy: 0.6320 - val_loss: 0.6787 - val_auc: 0.6584 - val_accuracy: 0.6000\n",
      "Epoch 11/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6245 - auc: 0.7208 - accuracy: 0.6631 - val_loss: 0.6687 - val_auc: 0.6663 - val_accuracy: 0.5946\n",
      "Epoch 12/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6509 - auc: 0.6756 - accuracy: 0.6318 - val_loss: 0.6591 - val_auc: 0.6775 - val_accuracy: 0.6000\n",
      "Epoch 13/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6461 - auc: 0.6885 - accuracy: 0.6292 - val_loss: 0.6499 - val_auc: 0.6858 - val_accuracy: 0.6108\n",
      "Epoch 14/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6770 - auc: 0.6422 - accuracy: 0.5803 - val_loss: 0.6414 - val_auc: 0.6932 - val_accuracy: 0.6270\n",
      "Epoch 15/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6758 - auc: 0.6563 - accuracy: 0.5994 - val_loss: 0.6331 - val_auc: 0.7010 - val_accuracy: 0.6324\n",
      "Epoch 16/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6322 - auc: 0.6829 - accuracy: 0.6206 - val_loss: 0.6253 - val_auc: 0.7083 - val_accuracy: 0.6378\n",
      "Epoch 17/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6212 - auc: 0.7035 - accuracy: 0.6408 - val_loss: 0.6180 - val_auc: 0.7152 - val_accuracy: 0.6324\n",
      "Epoch 18/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6177 - auc: 0.7110 - accuracy: 0.6338 - val_loss: 0.6107 - val_auc: 0.7238 - val_accuracy: 0.6378\n",
      "Epoch 19/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6227 - auc: 0.7089 - accuracy: 0.6420 - val_loss: 0.6039 - val_auc: 0.7299 - val_accuracy: 0.6486\n",
      "Epoch 20/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6328 - auc: 0.6781 - accuracy: 0.6352 - val_loss: 0.5973 - val_auc: 0.7369 - val_accuracy: 0.6486\n",
      "Epoch 21/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5911 - auc: 0.7369 - accuracy: 0.6602 - val_loss: 0.5915 - val_auc: 0.7421 - val_accuracy: 0.6541\n",
      "Epoch 22/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6125 - auc: 0.6950 - accuracy: 0.6273 - val_loss: 0.5853 - val_auc: 0.7479 - val_accuracy: 0.6541\n",
      "Epoch 23/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6016 - auc: 0.7330 - accuracy: 0.6576 - val_loss: 0.5799 - val_auc: 0.7518 - val_accuracy: 0.6649\n",
      "Epoch 24/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5609 - auc: 0.7749 - accuracy: 0.6997 - val_loss: 0.5744 - val_auc: 0.7576 - val_accuracy: 0.6757\n",
      "Epoch 25/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5649 - auc: 0.7541 - accuracy: 0.6908 - val_loss: 0.5694 - val_auc: 0.7633 - val_accuracy: 0.6811\n",
      "Epoch 26/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5924 - auc: 0.7394 - accuracy: 0.6593 - val_loss: 0.5645 - val_auc: 0.7674 - val_accuracy: 0.6865\n",
      "Epoch 27/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6020 - auc: 0.7289 - accuracy: 0.6652 - val_loss: 0.5600 - val_auc: 0.7713 - val_accuracy: 0.6919\n",
      "Epoch 28/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5905 - auc: 0.7340 - accuracy: 0.6553 - val_loss: 0.5556 - val_auc: 0.7756 - val_accuracy: 0.6973\n",
      "Epoch 29/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5716 - auc: 0.7550 - accuracy: 0.6757 - val_loss: 0.5511 - val_auc: 0.7812 - val_accuracy: 0.6973\n",
      "Epoch 30/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5413 - auc: 0.7906 - accuracy: 0.7000 - val_loss: 0.5472 - val_auc: 0.7841 - val_accuracy: 0.7081\n",
      "Epoch 31/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5378 - auc: 0.7931 - accuracy: 0.6975 - val_loss: 0.5435 - val_auc: 0.7882 - val_accuracy: 0.7027\n",
      "Epoch 32/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5509 - auc: 0.7762 - accuracy: 0.6992 - val_loss: 0.5399 - val_auc: 0.7905 - val_accuracy: 0.6973\n",
      "Epoch 33/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5181 - auc: 0.8184 - accuracy: 0.7238 - val_loss: 0.5364 - val_auc: 0.7949 - val_accuracy: 0.7135\n",
      "Epoch 34/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5328 - auc: 0.8003 - accuracy: 0.7132 - val_loss: 0.5331 - val_auc: 0.7985 - val_accuracy: 0.7135\n",
      "Epoch 35/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5296 - auc: 0.7996 - accuracy: 0.6931 - val_loss: 0.5300 - val_auc: 0.8024 - val_accuracy: 0.7189\n",
      "Epoch 36/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5149 - auc: 0.8157 - accuracy: 0.7259 - val_loss: 0.5269 - val_auc: 0.8052 - val_accuracy: 0.7189\n",
      "Epoch 37/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5404 - auc: 0.7835 - accuracy: 0.6924 - val_loss: 0.5240 - val_auc: 0.8088 - val_accuracy: 0.7189\n",
      "Epoch 38/1000\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.5779 - auc: 0.7361 - accuracy: 0.68 - 0s 4ms/step - loss: 0.5271 - auc: 0.7990 - accuracy: 0.7078 - val_loss: 0.5213 - val_auc: 0.8116 - val_accuracy: 0.7297\n",
      "Epoch 39/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5323 - auc: 0.7965 - accuracy: 0.7079 - val_loss: 0.5187 - val_auc: 0.8127 - val_accuracy: 0.7297\n",
      "Epoch 40/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5162 - auc: 0.8242 - accuracy: 0.7066 - val_loss: 0.5163 - val_auc: 0.8146 - val_accuracy: 0.7243\n",
      "Epoch 41/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5128 - auc: 0.8170 - accuracy: 0.7152 - val_loss: 0.5139 - val_auc: 0.8168 - val_accuracy: 0.7351\n",
      "Epoch 42/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5259 - auc: 0.7912 - accuracy: 0.7069 - val_loss: 0.5116 - val_auc: 0.8185 - val_accuracy: 0.7405\n",
      "Epoch 43/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5201 - auc: 0.8221 - accuracy: 0.7306 - val_loss: 0.5093 - val_auc: 0.8209 - val_accuracy: 0.7405\n",
      "Epoch 44/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4994 - auc: 0.8317 - accuracy: 0.7268 - val_loss: 0.5071 - val_auc: 0.8226 - val_accuracy: 0.7405\n",
      "Epoch 45/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4853 - auc: 0.8380 - accuracy: 0.7343 - val_loss: 0.5049 - val_auc: 0.8257 - val_accuracy: 0.7405\n",
      "Epoch 46/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4938 - auc: 0.8206 - accuracy: 0.7424 - val_loss: 0.5028 - val_auc: 0.8285 - val_accuracy: 0.7405\n",
      "Epoch 47/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5079 - auc: 0.8266 - accuracy: 0.7420 - val_loss: 0.5008 - val_auc: 0.8304 - val_accuracy: 0.7459\n",
      "Epoch 48/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4880 - auc: 0.8430 - accuracy: 0.7465 - val_loss: 0.4990 - val_auc: 0.8312 - val_accuracy: 0.7459\n",
      "Epoch 49/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5140 - auc: 0.8167 - accuracy: 0.7135 - val_loss: 0.4972 - val_auc: 0.8324 - val_accuracy: 0.7459\n",
      "Epoch 50/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4852 - auc: 0.8424 - accuracy: 0.7407 - val_loss: 0.4955 - val_auc: 0.8340 - val_accuracy: 0.7405\n",
      "Epoch 51/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4977 - auc: 0.8277 - accuracy: 0.7300 - val_loss: 0.4940 - val_auc: 0.8353 - val_accuracy: 0.7568\n",
      "Epoch 52/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4998 - auc: 0.8280 - accuracy: 0.7267 - val_loss: 0.4922 - val_auc: 0.8365 - val_accuracy: 0.7568\n",
      "Epoch 53/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4943 - auc: 0.8238 - accuracy: 0.7309 - val_loss: 0.4906 - val_auc: 0.8377 - val_accuracy: 0.7568\n",
      "Epoch 54/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4872 - auc: 0.8362 - accuracy: 0.7348 - val_loss: 0.4892 - val_auc: 0.8387 - val_accuracy: 0.7568\n",
      "Epoch 55/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4878 - auc: 0.8316 - accuracy: 0.7470 - val_loss: 0.4878 - val_auc: 0.8391 - val_accuracy: 0.7514\n",
      "Epoch 56/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4728 - auc: 0.8491 - accuracy: 0.7590 - val_loss: 0.4863 - val_auc: 0.8404 - val_accuracy: 0.7514\n",
      "Epoch 57/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5071 - auc: 0.8173 - accuracy: 0.7155 - val_loss: 0.4849 - val_auc: 0.8411 - val_accuracy: 0.7568\n",
      "Epoch 58/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5152 - auc: 0.8156 - accuracy: 0.7311 - val_loss: 0.4836 - val_auc: 0.8421 - val_accuracy: 0.7622\n",
      "Epoch 59/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5168 - auc: 0.8085 - accuracy: 0.7171 - val_loss: 0.4826 - val_auc: 0.8433 - val_accuracy: 0.7622\n",
      "Epoch 60/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5088 - auc: 0.8220 - accuracy: 0.7153 - val_loss: 0.4813 - val_auc: 0.8436 - val_accuracy: 0.7622\n",
      "Epoch 61/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4632 - auc: 0.8544 - accuracy: 0.7642 - val_loss: 0.4800 - val_auc: 0.8445 - val_accuracy: 0.7622\n",
      "Epoch 62/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8316 - accuracy: 0.7574 - val_loss: 0.4789 - val_auc: 0.8462 - val_accuracy: 0.7622\n",
      "Epoch 63/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5206 - auc: 0.7995 - accuracy: 0.7195 - val_loss: 0.4778 - val_auc: 0.8474 - val_accuracy: 0.7568\n",
      "Epoch 64/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8438 - accuracy: 0.7629 - val_loss: 0.4769 - val_auc: 0.8478 - val_accuracy: 0.7568\n",
      "Epoch 65/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4737 - auc: 0.8483 - accuracy: 0.7587 - val_loss: 0.4759 - val_auc: 0.8485 - val_accuracy: 0.7514\n",
      "Epoch 66/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5121 - auc: 0.8251 - accuracy: 0.7293 - val_loss: 0.4750 - val_auc: 0.8493 - val_accuracy: 0.7514\n",
      "Epoch 67/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4825 - auc: 0.8381 - accuracy: 0.7568 - val_loss: 0.4741 - val_auc: 0.8500 - val_accuracy: 0.7568\n",
      "Epoch 68/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4484 - auc: 0.8660 - accuracy: 0.7735 - val_loss: 0.4734 - val_auc: 0.8507 - val_accuracy: 0.7514\n",
      "Epoch 69/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4973 - auc: 0.8187 - accuracy: 0.7322 - val_loss: 0.4725 - val_auc: 0.8519 - val_accuracy: 0.7514\n",
      "Epoch 70/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5288 - auc: 0.7948 - accuracy: 0.7059 - val_loss: 0.4717 - val_auc: 0.8517 - val_accuracy: 0.7514\n",
      "Epoch 71/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4625 - auc: 0.8513 - accuracy: 0.7725 - val_loss: 0.4710 - val_auc: 0.8520 - val_accuracy: 0.7514\n",
      "Epoch 72/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5021 - auc: 0.8239 - accuracy: 0.7392 - val_loss: 0.4702 - val_auc: 0.8530 - val_accuracy: 0.7514\n",
      "Epoch 73/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4657 - auc: 0.8525 - accuracy: 0.7541 - val_loss: 0.4695 - val_auc: 0.8527 - val_accuracy: 0.7459\n",
      "Epoch 74/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4828 - auc: 0.8367 - accuracy: 0.7542 - val_loss: 0.4687 - val_auc: 0.8533 - val_accuracy: 0.7459\n",
      "Epoch 75/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4546 - auc: 0.8540 - accuracy: 0.7790 - val_loss: 0.4680 - val_auc: 0.8538 - val_accuracy: 0.7459\n",
      "Epoch 76/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4443 - auc: 0.8524 - accuracy: 0.7995 - val_loss: 0.4673 - val_auc: 0.8534 - val_accuracy: 0.7459\n",
      "Epoch 77/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4782 - auc: 0.8355 - accuracy: 0.7658 - val_loss: 0.4666 - val_auc: 0.8539 - val_accuracy: 0.7514\n",
      "Epoch 78/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4712 - auc: 0.8463 - accuracy: 0.7696 - val_loss: 0.4660 - val_auc: 0.8541 - val_accuracy: 0.7514\n",
      "Epoch 79/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4904 - auc: 0.8394 - accuracy: 0.7424 - val_loss: 0.4653 - val_auc: 0.8548 - val_accuracy: 0.7514\n",
      "Epoch 80/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4616 - auc: 0.8528 - accuracy: 0.7725 - val_loss: 0.4647 - val_auc: 0.8551 - val_accuracy: 0.7568\n",
      "Epoch 81/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4821 - auc: 0.8387 - accuracy: 0.7697 - val_loss: 0.4641 - val_auc: 0.8544 - val_accuracy: 0.7568\n",
      "Epoch 82/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4833 - auc: 0.8327 - accuracy: 0.7659 - val_loss: 0.4635 - val_auc: 0.8551 - val_accuracy: 0.7568\n",
      "Epoch 83/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4841 - auc: 0.8335 - accuracy: 0.7363 - val_loss: 0.4630 - val_auc: 0.8556 - val_accuracy: 0.7568\n",
      "Epoch 84/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4531 - auc: 0.8582 - accuracy: 0.7856 - val_loss: 0.4625 - val_auc: 0.8557 - val_accuracy: 0.7514\n",
      "Epoch 85/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4739 - auc: 0.8474 - accuracy: 0.7458 - val_loss: 0.4620 - val_auc: 0.8557 - val_accuracy: 0.7514\n",
      "Epoch 86/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4808 - auc: 0.8489 - accuracy: 0.7679 - val_loss: 0.4614 - val_auc: 0.8562 - val_accuracy: 0.7622\n",
      "Epoch 87/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4537 - auc: 0.8543 - accuracy: 0.7731 - val_loss: 0.4610 - val_auc: 0.8566 - val_accuracy: 0.7622\n",
      "Epoch 88/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4939 - auc: 0.8127 - accuracy: 0.7448 - val_loss: 0.4605 - val_auc: 0.8567 - val_accuracy: 0.7568\n",
      "Epoch 89/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4435 - auc: 0.8555 - accuracy: 0.7837 - val_loss: 0.4601 - val_auc: 0.8566 - val_accuracy: 0.7568\n",
      "Epoch 90/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4958 - auc: 0.8239 - accuracy: 0.7284 - val_loss: 0.4596 - val_auc: 0.8572 - val_accuracy: 0.7568\n",
      "Epoch 91/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4533 - auc: 0.8667 - accuracy: 0.7838 - val_loss: 0.4592 - val_auc: 0.8570 - val_accuracy: 0.7568\n",
      "Epoch 92/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4427 - auc: 0.8662 - accuracy: 0.7994 - val_loss: 0.4589 - val_auc: 0.8570 - val_accuracy: 0.7568\n",
      "Epoch 93/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4705 - auc: 0.8500 - accuracy: 0.7640 - val_loss: 0.4584 - val_auc: 0.8577 - val_accuracy: 0.7568\n",
      "Epoch 94/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4920 - auc: 0.8139 - accuracy: 0.7597 - val_loss: 0.4580 - val_auc: 0.8578 - val_accuracy: 0.7568\n",
      "Epoch 95/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4845 - auc: 0.8313 - accuracy: 0.7378 - val_loss: 0.4577 - val_auc: 0.8582 - val_accuracy: 0.7568\n",
      "Epoch 96/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4633 - auc: 0.8527 - accuracy: 0.7863 - val_loss: 0.4573 - val_auc: 0.8580 - val_accuracy: 0.7568\n",
      "Epoch 97/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4770 - auc: 0.8504 - accuracy: 0.7770 - val_loss: 0.4569 - val_auc: 0.8577 - val_accuracy: 0.7568\n",
      "Epoch 98/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4684 - auc: 0.8456 - accuracy: 0.7700 - val_loss: 0.4565 - val_auc: 0.8576 - val_accuracy: 0.7568\n",
      "Epoch 99/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4693 - auc: 0.8412 - accuracy: 0.7732 - val_loss: 0.4561 - val_auc: 0.8581 - val_accuracy: 0.7622\n",
      "Epoch 100/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4772 - auc: 0.8307 - accuracy: 0.7613 - val_loss: 0.4558 - val_auc: 0.8583 - val_accuracy: 0.7622\n",
      "Epoch 101/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8423 - accuracy: 0.7811 - val_loss: 0.4555 - val_auc: 0.8592 - val_accuracy: 0.7622\n",
      "Epoch 102/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4526 - auc: 0.8593 - accuracy: 0.8033 - val_loss: 0.4552 - val_auc: 0.8594 - val_accuracy: 0.7622\n",
      "Epoch 103/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4628 - auc: 0.8461 - accuracy: 0.7853 - val_loss: 0.4550 - val_auc: 0.8595 - val_accuracy: 0.7622\n",
      "Epoch 104/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4622 - auc: 0.8546 - accuracy: 0.7781 - val_loss: 0.4547 - val_auc: 0.8595 - val_accuracy: 0.7622\n",
      "Epoch 105/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4622 - auc: 0.8509 - accuracy: 0.7697 - val_loss: 0.4544 - val_auc: 0.8597 - val_accuracy: 0.7622\n",
      "Epoch 106/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4398 - auc: 0.8713 - accuracy: 0.7820 - val_loss: 0.4541 - val_auc: 0.8599 - val_accuracy: 0.7676\n",
      "Epoch 107/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4515 - auc: 0.8545 - accuracy: 0.7965 - val_loss: 0.4539 - val_auc: 0.8603 - val_accuracy: 0.7676\n",
      "Epoch 108/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4933 - auc: 0.8266 - accuracy: 0.7595 - val_loss: 0.4537 - val_auc: 0.8601 - val_accuracy: 0.7676\n",
      "Epoch 109/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4688 - auc: 0.8485 - accuracy: 0.7795 - val_loss: 0.4533 - val_auc: 0.8600 - val_accuracy: 0.7730\n",
      "Epoch 110/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5076 - auc: 0.8096 - accuracy: 0.7622 - val_loss: 0.4531 - val_auc: 0.8599 - val_accuracy: 0.7730\n",
      "Epoch 111/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8413 - accuracy: 0.7835 - val_loss: 0.4529 - val_auc: 0.8595 - val_accuracy: 0.7730\n",
      "Epoch 112/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4745 - auc: 0.8337 - accuracy: 0.7454 - val_loss: 0.4527 - val_auc: 0.8595 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd7c951100>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "es_model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/ES/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "about-international",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5235 - auc: 0.8006 - accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test subset and predict.\n",
    "es_model = load_model(mc_path)\n",
    "eval = es_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-holiday",
   "metadata": {},
   "source": [
    "# 9. Learning Rate Scheduling\n",
    "En este apartado se prueba la opción de Learning Rate Scheduling. Esta se encarga de aplicarle una función al Learning Rate entre epochs, de forma tal de encontrar el mínimo de la loss de forma más rápida, y apuntando a evitar mínimos locales y, por ende, overfitting. Se sigue aplicando el concepto de **early stopping** para la AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "macro-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay, PolynomialDecay # API in https://keras.io/api/optimizers/learning_rate_schedules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "weird-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/lrs_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sticky-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "lrs_model = Sequential()\n",
    "lrs_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wound-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 164ms/step - loss: 0.5846 - auc: 0.7690 - accuracy: 0.6852 - val_loss: 0.5829 - val_auc: 0.7850 - val_accuracy: 0.6919\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5480 - auc: 0.7989 - accuracy: 0.7296 - val_loss: 0.5416 - val_auc: 0.8142 - val_accuracy: 0.7297\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4982 - auc: 0.8338 - accuracy: 0.7415 - val_loss: 0.5135 - val_auc: 0.8313 - val_accuracy: 0.7297\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5099 - auc: 0.8194 - accuracy: 0.7526 - val_loss: 0.4949 - val_auc: 0.8417 - val_accuracy: 0.7514\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5118 - auc: 0.8155 - accuracy: 0.7464 - val_loss: 0.4820 - val_auc: 0.8490 - val_accuracy: 0.7838\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4750 - auc: 0.8379 - accuracy: 0.7485 - val_loss: 0.4735 - val_auc: 0.8517 - val_accuracy: 0.7946\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4315 - auc: 0.8716 - accuracy: 0.8077 - val_loss: 0.4664 - val_auc: 0.8559 - val_accuracy: 0.7946\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4975 - auc: 0.8235 - accuracy: 0.7458 - val_loss: 0.4611 - val_auc: 0.8584 - val_accuracy: 0.8000\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4467 - auc: 0.8615 - accuracy: 0.7887 - val_loss: 0.4584 - val_auc: 0.8588 - val_accuracy: 0.7946\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4312 - auc: 0.8736 - accuracy: 0.8058 - val_loss: 0.4556 - val_auc: 0.8599 - val_accuracy: 0.7892\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4664 - auc: 0.8425 - accuracy: 0.7971 - val_loss: 0.4542 - val_auc: 0.8608 - val_accuracy: 0.7838\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4547 - auc: 0.8549 - accuracy: 0.7944 - val_loss: 0.4520 - val_auc: 0.8612 - val_accuracy: 0.7892\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4185 - auc: 0.8848 - accuracy: 0.8324 - val_loss: 0.4502 - val_auc: 0.8613 - val_accuracy: 0.7892\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4905 - auc: 0.8241 - accuracy: 0.7740 - val_loss: 0.4494 - val_auc: 0.8618 - val_accuracy: 0.7892\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4839 - auc: 0.8336 - accuracy: 0.7659 - val_loss: 0.4486 - val_auc: 0.8616 - val_accuracy: 0.7892\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4265 - auc: 0.8819 - accuracy: 0.8341 - val_loss: 0.4482 - val_auc: 0.8617 - val_accuracy: 0.7838\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4471 - auc: 0.8602 - accuracy: 0.7995 - val_loss: 0.4476 - val_auc: 0.8629 - val_accuracy: 0.7838\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8442 - accuracy: 0.7792 - val_loss: 0.4466 - val_auc: 0.8624 - val_accuracy: 0.7784\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4421 - auc: 0.8639 - accuracy: 0.8026 - val_loss: 0.4465 - val_auc: 0.8629 - val_accuracy: 0.7784\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4873 - auc: 0.8307 - accuracy: 0.7618 - val_loss: 0.4461 - val_auc: 0.8627 - val_accuracy: 0.7784\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4398 - auc: 0.8583 - accuracy: 0.8009 - val_loss: 0.4459 - val_auc: 0.8629 - val_accuracy: 0.7784\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4341 - auc: 0.8688 - accuracy: 0.8104 - val_loss: 0.4463 - val_auc: 0.8629 - val_accuracy: 0.7784\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4509 - auc: 0.8548 - accuracy: 0.7956 - val_loss: 0.4458 - val_auc: 0.8631 - val_accuracy: 0.7730\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4355 - auc: 0.8591 - accuracy: 0.8135 - val_loss: 0.4457 - val_auc: 0.8622 - val_accuracy: 0.7730\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4563 - auc: 0.8432 - accuracy: 0.7778 - val_loss: 0.4459 - val_auc: 0.8635 - val_accuracy: 0.7730\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4938 - auc: 0.8209 - accuracy: 0.7697 - val_loss: 0.4457 - val_auc: 0.8627 - val_accuracy: 0.7676\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4953 - auc: 0.8278 - accuracy: 0.7936 - val_loss: 0.4454 - val_auc: 0.8626 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd7ded3160>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define learning rate at start\n",
    "ilr = 0.1\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=100000, decay_rate=0.96, staircase=False) # Decay every (decay_steps) steps with a base of (decay_rate).\n",
    "# Compiling model\n",
    "lrs_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dominican-invention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 2ms/step - loss: 0.5273 - auc: 0.7974 - accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-sending",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Exponential Decay se lleva bien con Early Stopping?, ya que si reduzco el learning rate \"me muevo menos\", con lo cual el callback de Early Stopping cortaría prematuramente. Ahora probamos sin Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "smart-archive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 104ms/step - loss: 0.4597 - auc: 0.8499 - accuracy: 0.7902 - val_loss: 0.4475 - val_auc: 0.8622 - val_accuracy: 0.7730\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4590 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4466 - val_auc: 0.8617 - val_accuracy: 0.7730\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4593 - auc: 0.8500 - accuracy: 0.7902 - val_loss: 0.4468 - val_auc: 0.8616 - val_accuracy: 0.7730\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4088 - auc: 0.9180 - accuracy: 0.78 - 0s 3ms/step - loss: 0.4589 - auc: 0.8500 - accuracy: 0.7949 - val_loss: 0.4461 - val_auc: 0.8619 - val_accuracy: 0.7730\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4591 - auc: 0.8499 - accuracy: 0.7925 - val_loss: 0.4458 - val_auc: 0.8625 - val_accuracy: 0.7784\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4454 - val_auc: 0.8626 - val_accuracy: 0.7730\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8494 - accuracy: 0.7925 - val_loss: 0.4446 - val_auc: 0.8631 - val_accuracy: 0.7676\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8495 - accuracy: 0.7972 - val_loss: 0.4440 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - auc: 0.8495 - accuracy: 0.7949 - val_loss: 0.4439 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4580 - auc: 0.8503 - accuracy: 0.7949 - val_loss: 0.4439 - val_auc: 0.8627 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8493 - accuracy: 0.7995 - val_loss: 0.4436 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8496 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8627 - val_accuracy: 0.7676\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8496 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8627 - val_accuracy: 0.7676\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4581 - auc: 0.8497 - accuracy: 0.7949 - val_loss: 0.4440 - val_auc: 0.8637 - val_accuracy: 0.7622\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8501 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8631 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4587 - auc: 0.8494 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8643 - val_accuracy: 0.7676\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4585 - auc: 0.8495 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8497 - accuracy: 0.7902 - val_loss: 0.4428 - val_auc: 0.8649 - val_accuracy: 0.7676\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8495 - accuracy: 0.7949 - val_loss: 0.4428 - val_auc: 0.8649 - val_accuracy: 0.7622\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8495 - accuracy: 0.7925 - val_loss: 0.4433 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8493 - accuracy: 0.7972 - val_loss: 0.4429 - val_auc: 0.8641 - val_accuracy: 0.7676\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4597 - auc: 0.8480 - accuracy: 0.7902 - val_loss: 0.4430 - val_auc: 0.8632 - val_accuracy: 0.7730\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8490 - accuracy: 0.7972 - val_loss: 0.4432 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8498 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8487 - accuracy: 0.7949 - val_loss: 0.4431 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4590 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4434 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4433 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8495 - accuracy: 0.7925 - val_loss: 0.4432 - val_auc: 0.8641 - val_accuracy: 0.7676\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4584 - auc: 0.8501 - accuracy: 0.7949 - val_loss: 0.4435 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4589 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8494 - accuracy: 0.7949 - val_loss: 0.4433 - val_auc: 0.8643 - val_accuracy: 0.7676\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8498 - accuracy: 0.7972 - val_loss: 0.4430 - val_auc: 0.8648 - val_accuracy: 0.7676\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8628 - val_accuracy: 0.7784\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - auc: 0.8489 - accuracy: 0.7902 - val_loss: 0.4434 - val_auc: 0.8632 - val_accuracy: 0.7730\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8496 - accuracy: 0.7879 - val_loss: 0.4426 - val_auc: 0.8644 - val_accuracy: 0.7622\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8490 - accuracy: 0.7925 - val_loss: 0.4429 - val_auc: 0.8637 - val_accuracy: 0.7622\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8497 - accuracy: 0.7925 - val_loss: 0.4434 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4593 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8649 - val_accuracy: 0.7622\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8495 - accuracy: 0.7925 - val_loss: 0.4433 - val_auc: 0.8641 - val_accuracy: 0.7730\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8499 - accuracy: 0.7925 - val_loss: 0.4429 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8494 - accuracy: 0.7902 - val_loss: 0.4430 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4430 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4430 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8492 - accuracy: 0.7972 - val_loss: 0.4433 - val_auc: 0.8631 - val_accuracy: 0.7622\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8631 - val_accuracy: 0.7622\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4430 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8498 - accuracy: 0.7925 - val_loss: 0.4430 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8499 - accuracy: 0.7925 - val_loss: 0.4433 - val_auc: 0.8633 - val_accuracy: 0.7568\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8493 - accuracy: 0.7995 - val_loss: 0.4431 - val_auc: 0.8634 - val_accuracy: 0.7568\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8490 - accuracy: 0.7972 - val_loss: 0.4433 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8489 - accuracy: 0.7925 - val_loss: 0.4439 - val_auc: 0.8625 - val_accuracy: 0.7730\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8477 - accuracy: 0.7949 - val_loss: 0.4450 - val_auc: 0.8632 - val_accuracy: 0.7784\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8490 - accuracy: 0.7925 - val_loss: 0.4450 - val_auc: 0.8633 - val_accuracy: 0.7730\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4448 - val_auc: 0.8627 - val_accuracy: 0.7784\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4590 - auc: 0.8491 - accuracy: 0.7925 - val_loss: 0.4444 - val_auc: 0.8626 - val_accuracy: 0.7730\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8494 - accuracy: 0.7925 - val_loss: 0.4440 - val_auc: 0.8627 - val_accuracy: 0.7676\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8497 - accuracy: 0.7972 - val_loss: 0.4439 - val_auc: 0.8626 - val_accuracy: 0.7676\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8491 - accuracy: 0.7995 - val_loss: 0.4440 - val_auc: 0.8624 - val_accuracy: 0.7730\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4440 - val_auc: 0.8623 - val_accuracy: 0.7676\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4588 - auc: 0.8494 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4583 - auc: 0.8495 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8641 - val_accuracy: 0.7676\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4593 - auc: 0.8497 - accuracy: 0.7972 - val_loss: 0.4440 - val_auc: 0.8632 - val_accuracy: 0.7622\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8492 - accuracy: 0.7925 - val_loss: 0.4430 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4579 - auc: 0.8500 - accuracy: 0.7949 - val_loss: 0.4435 - val_auc: 0.8637 - val_accuracy: 0.7622\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8489 - accuracy: 0.7925 - val_loss: 0.4434 - val_auc: 0.8631 - val_accuracy: 0.7676\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8489 - accuracy: 0.7972 - val_loss: 0.4440 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8495 - accuracy: 0.7902 - val_loss: 0.4437 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8499 - accuracy: 0.7925 - val_loss: 0.4437 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4583 - auc: 0.8496 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8638 - val_accuracy: 0.7622\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8489 - accuracy: 0.7925 - val_loss: 0.4435 - val_auc: 0.8632 - val_accuracy: 0.7730\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4590 - auc: 0.8491 - accuracy: 0.7902 - val_loss: 0.4431 - val_auc: 0.8632 - val_accuracy: 0.7622\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4433 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8497 - accuracy: 0.7902 - val_loss: 0.4426 - val_auc: 0.8643 - val_accuracy: 0.7568\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4584 - auc: 0.8499 - accuracy: 0.7949 - val_loss: 0.4425 - val_auc: 0.8646 - val_accuracy: 0.7568\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8496 - accuracy: 0.7925 - val_loss: 0.4432 - val_auc: 0.8642 - val_accuracy: 0.7676\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8482 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8641 - val_accuracy: 0.7730\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8486 - accuracy: 0.7949 - val_loss: 0.4434 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4442 - val_auc: 0.8632 - val_accuracy: 0.7784\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4433 - val_auc: 0.8643 - val_accuracy: 0.7676\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8496 - accuracy: 0.7925 - val_loss: 0.4429 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8486 - accuracy: 0.7925 - val_loss: 0.4429 - val_auc: 0.8645 - val_accuracy: 0.7676\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4431 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4433 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4585 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4434 - val_auc: 0.8627 - val_accuracy: 0.7730\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4434 - val_auc: 0.8629 - val_accuracy: 0.7676\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8483 - accuracy: 0.7925 - val_loss: 0.4434 - val_auc: 0.8632 - val_accuracy: 0.7676\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8488 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8630 - val_accuracy: 0.7676\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4587 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4433 - val_auc: 0.8635 - val_accuracy: 0.7622\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.6087 - auc: 0.6901 - accuracy: 0.68 - 0s 3ms/step - loss: 0.4588 - auc: 0.8486 - accuracy: 0.7949 - val_loss: 0.4436 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8484 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8635 - val_accuracy: 0.7676\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4585 - auc: 0.8487 - accuracy: 0.7995 - val_loss: 0.4435 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8493 - accuracy: 0.7995 - val_loss: 0.4431 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8484 - accuracy: 0.7972 - val_loss: 0.4432 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8486 - accuracy: 0.7972 - val_loss: 0.4431 - val_auc: 0.8647 - val_accuracy: 0.7676\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8489 - accuracy: 0.7925 - val_loss: 0.4430 - val_auc: 0.8642 - val_accuracy: 0.7676\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8492 - accuracy: 0.7925 - val_loss: 0.4432 - val_auc: 0.8644 - val_accuracy: 0.7676\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8645 - val_accuracy: 0.7676\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4435 - val_auc: 0.8646 - val_accuracy: 0.7676\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4590 - auc: 0.8491 - accuracy: 0.7925 - val_loss: 0.4432 - val_auc: 0.8649 - val_accuracy: 0.7622\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4430 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8493 - accuracy: 0.7902 - val_loss: 0.4429 - val_auc: 0.8644 - val_accuracy: 0.7568\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8490 - accuracy: 0.7925 - val_loss: 0.4431 - val_auc: 0.8647 - val_accuracy: 0.7622\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8492 - accuracy: 0.7879 - val_loss: 0.4437 - val_auc: 0.8632 - val_accuracy: 0.7730\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8486 - accuracy: 0.7925 - val_loss: 0.4439 - val_auc: 0.8638 - val_accuracy: 0.7784\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8494 - accuracy: 0.7949 - val_loss: 0.4436 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8491 - accuracy: 0.7925 - val_loss: 0.4436 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4436 - val_auc: 0.8631 - val_accuracy: 0.7730\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8483 - accuracy: 0.7949 - val_loss: 0.4436 - val_auc: 0.8635 - val_accuracy: 0.7730\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8487 - accuracy: 0.7972 - val_loss: 0.4436 - val_auc: 0.8638 - val_accuracy: 0.7730\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8490 - accuracy: 0.7925 - val_loss: 0.4436 - val_auc: 0.8632 - val_accuracy: 0.7784\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8491 - accuracy: 0.7902 - val_loss: 0.4434 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8483 - accuracy: 0.7995 - val_loss: 0.4440 - val_auc: 0.8627 - val_accuracy: 0.7730\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8494 - accuracy: 0.7925 - val_loss: 0.4434 - val_auc: 0.8648 - val_accuracy: 0.7676\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8495 - accuracy: 0.7949 - val_loss: 0.4436 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4590 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4435 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8496 - accuracy: 0.7925 - val_loss: 0.4431 - val_auc: 0.8643 - val_accuracy: 0.7622\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8499 - accuracy: 0.7949 - val_loss: 0.4431 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8495 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8627 - val_accuracy: 0.7730\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8628 - val_accuracy: 0.7730\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4440 - val_auc: 0.8631 - val_accuracy: 0.7730\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8485 - accuracy: 0.7972 - val_loss: 0.4440 - val_auc: 0.8635 - val_accuracy: 0.7730\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4583 - auc: 0.8488 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8635 - val_accuracy: 0.7676\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4590 - auc: 0.8483 - accuracy: 0.7949 - val_loss: 0.4432 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8489 - accuracy: 0.7972 - val_loss: 0.4438 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8492 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8640 - val_accuracy: 0.7730\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4441 - val_auc: 0.8634 - val_accuracy: 0.7784\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8489 - accuracy: 0.7972 - val_loss: 0.4442 - val_auc: 0.8632 - val_accuracy: 0.7730\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8488 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8485 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8633 - val_accuracy: 0.7676\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8495 - accuracy: 0.7925 - val_loss: 0.4436 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - auc: 0.8486 - accuracy: 0.7925 - val_loss: 0.4444 - val_auc: 0.8636 - val_accuracy: 0.7784\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4443 - val_auc: 0.8634 - val_accuracy: 0.7730\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8485 - accuracy: 0.7972 - val_loss: 0.4440 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8491 - accuracy: 0.7972 - val_loss: 0.4438 - val_auc: 0.8645 - val_accuracy: 0.7676\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8491 - accuracy: 0.7972 - val_loss: 0.4439 - val_auc: 0.8645 - val_accuracy: 0.7730\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4435 - val_auc: 0.8644 - val_accuracy: 0.7676\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8651 - val_accuracy: 0.7622\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8491 - accuracy: 0.7925 - val_loss: 0.4434 - val_auc: 0.8641 - val_accuracy: 0.7568\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8497 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8632 - val_accuracy: 0.7622\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8636 - val_accuracy: 0.7622\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8497 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8634 - val_accuracy: 0.7622\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8496 - accuracy: 0.7925 - val_loss: 0.4441 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8490 - accuracy: 0.7949 - val_loss: 0.4435 - val_auc: 0.8639 - val_accuracy: 0.7676\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8487 - accuracy: 0.7949 - val_loss: 0.4430 - val_auc: 0.8644 - val_accuracy: 0.7622\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8495 - accuracy: 0.7925 - val_loss: 0.4438 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8492 - accuracy: 0.7925 - val_loss: 0.4436 - val_auc: 0.8630 - val_accuracy: 0.7676\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8488 - accuracy: 0.7995 - val_loss: 0.4437 - val_auc: 0.8631 - val_accuracy: 0.7676\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8491 - accuracy: 0.7972 - val_loss: 0.4434 - val_auc: 0.8631 - val_accuracy: 0.7622\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8494 - accuracy: 0.7949 - val_loss: 0.4431 - val_auc: 0.8641 - val_accuracy: 0.7676\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8495 - accuracy: 0.7902 - val_loss: 0.4430 - val_auc: 0.8641 - val_accuracy: 0.7676\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8486 - accuracy: 0.7995 - val_loss: 0.4429 - val_auc: 0.8642 - val_accuracy: 0.7568\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8502 - accuracy: 0.7949 - val_loss: 0.4433 - val_auc: 0.8634 - val_accuracy: 0.7676\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8491 - accuracy: 0.7925 - val_loss: 0.4436 - val_auc: 0.8631 - val_accuracy: 0.7730\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8486 - accuracy: 0.7972 - val_loss: 0.4432 - val_auc: 0.8631 - val_accuracy: 0.7730\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8489 - accuracy: 0.7949 - val_loss: 0.4429 - val_auc: 0.8644 - val_accuracy: 0.7676\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8486 - accuracy: 0.7925 - val_loss: 0.4437 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8483 - accuracy: 0.7972 - val_loss: 0.4433 - val_auc: 0.8650 - val_accuracy: 0.7622\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4581 - auc: 0.8494 - accuracy: 0.7925 - val_loss: 0.4431 - val_auc: 0.8644 - val_accuracy: 0.7676\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8494 - accuracy: 0.7949 - val_loss: 0.4429 - val_auc: 0.8644 - val_accuracy: 0.7622\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4430 - val_auc: 0.8640 - val_accuracy: 0.7622\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8496 - accuracy: 0.7925 - val_loss: 0.4428 - val_auc: 0.8643 - val_accuracy: 0.7568\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8498 - accuracy: 0.7925 - val_loss: 0.4426 - val_auc: 0.8642 - val_accuracy: 0.7622\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8485 - accuracy: 0.7925 - val_loss: 0.4433 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8488 - accuracy: 0.7949 - val_loss: 0.4435 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8490 - accuracy: 0.7925 - val_loss: 0.4436 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8500 - accuracy: 0.7949 - val_loss: 0.4433 - val_auc: 0.8645 - val_accuracy: 0.7730\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8494 - accuracy: 0.7972 - val_loss: 0.4432 - val_auc: 0.8647 - val_accuracy: 0.7730\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8494 - accuracy: 0.7925 - val_loss: 0.4429 - val_auc: 0.8640 - val_accuracy: 0.7622\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8495 - accuracy: 0.7925 - val_loss: 0.4432 - val_auc: 0.8645 - val_accuracy: 0.7568\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8490 - accuracy: 0.7949 - val_loss: 0.4440 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4591 - auc: 0.8488 - accuracy: 0.7949 - val_loss: 0.4434 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4580 - auc: 0.8496 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8647 - val_accuracy: 0.7622\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4434 - val_auc: 0.8646 - val_accuracy: 0.7622\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8491 - accuracy: 0.7925 - val_loss: 0.4438 - val_auc: 0.8651 - val_accuracy: 0.7622\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4438 - val_auc: 0.8648 - val_accuracy: 0.7676\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4434 - val_auc: 0.8648 - val_accuracy: 0.7784\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4583 - auc: 0.8494 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8644 - val_accuracy: 0.7730\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8494 - accuracy: 0.7949 - val_loss: 0.4437 - val_auc: 0.8653 - val_accuracy: 0.7730\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8496 - accuracy: 0.7972 - val_loss: 0.4437 - val_auc: 0.8647 - val_accuracy: 0.7676\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4581 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4436 - val_auc: 0.8649 - val_accuracy: 0.7730\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4584 - auc: 0.8496 - accuracy: 0.7949 - val_loss: 0.4436 - val_auc: 0.8649 - val_accuracy: 0.7730\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8492 - accuracy: 0.7925 - val_loss: 0.4440 - val_auc: 0.8651 - val_accuracy: 0.7622\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8488 - accuracy: 0.7925 - val_loss: 0.4437 - val_auc: 0.8647 - val_accuracy: 0.7676\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8492 - accuracy: 0.7925 - val_loss: 0.4441 - val_auc: 0.8642 - val_accuracy: 0.7676\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4440 - val_auc: 0.8637 - val_accuracy: 0.7676\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8494 - accuracy: 0.7925 - val_loss: 0.4445 - val_auc: 0.8640 - val_accuracy: 0.7676\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4583 - auc: 0.8491 - accuracy: 0.7925 - val_loss: 0.4447 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4447 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8490 - accuracy: 0.7949 - val_loss: 0.4441 - val_auc: 0.8632 - val_accuracy: 0.7676\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8491 - accuracy: 0.7949 - val_loss: 0.4441 - val_auc: 0.8639 - val_accuracy: 0.7730\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4589 - auc: 0.8484 - accuracy: 0.7949 - val_loss: 0.4438 - val_auc: 0.8640 - val_accuracy: 0.7676\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8493 - accuracy: 0.7925 - val_loss: 0.4444 - val_auc: 0.8642 - val_accuracy: 0.7784\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8488 - accuracy: 0.7949 - val_loss: 0.4454 - val_auc: 0.8638 - val_accuracy: 0.7730\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8493 - accuracy: 0.7949 - val_loss: 0.4449 - val_auc: 0.8643 - val_accuracy: 0.7784\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8489 - accuracy: 0.7925 - val_loss: 0.4445 - val_auc: 0.8643 - val_accuracy: 0.7784\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8492 - accuracy: 0.7949 - val_loss: 0.4446 - val_auc: 0.8638 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd7df9e820>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "flush-breast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5361 - auc: 0.7976 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-personal",
   "metadata": {},
   "source": [
    "# 10. Regularización\n",
    "La idea de la regularización es la de limitar aquellos pesos que son altos. De esta forma, se agrega una capa previa a la capa densa que contiene la capa de regularización. Se probarán dos regularizaciones distintas: L1 y L2 (donde el número significa el grado del término adicional que se suma a la función de costo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-weekend",
   "metadata": {},
   "source": [
    "# 10.1. Regularización L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "brief-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "given-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "specified-electronics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 4s 153ms/step - loss: 0.9022 - auc: 0.4035 - accuracy: 0.4308 - val_loss: 0.6704 - val_auc: 0.6258 - val_accuracy: 0.5676\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.6489 - auc: 0.6278 - accuracy: 0.6523 - val_loss: 0.5573 - val_auc: 0.7978 - val_accuracy: 0.7189\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5547 - auc: 0.8105 - accuracy: 0.7493 - val_loss: 0.5128 - val_auc: 0.8377 - val_accuracy: 0.7622\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4939 - auc: 0.8556 - accuracy: 0.7997 - val_loss: 0.4905 - val_auc: 0.8500 - val_accuracy: 0.7784\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5250 - auc: 0.8082 - accuracy: 0.7445 - val_loss: 0.4778 - val_auc: 0.8563 - val_accuracy: 0.7892\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4524 - auc: 0.8500 - accuracy: 0.8013 - val_loss: 0.4696 - val_auc: 0.8602 - val_accuracy: 0.7784\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4806 - auc: 0.8467 - accuracy: 0.7861 - val_loss: 0.4637 - val_auc: 0.8614 - val_accuracy: 0.7784\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4645 - auc: 0.8571 - accuracy: 0.7983 - val_loss: 0.4596 - val_auc: 0.8626 - val_accuracy: 0.7892\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4962 - auc: 0.8286 - accuracy: 0.7664 - val_loss: 0.4567 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8490 - accuracy: 0.7830 - val_loss: 0.4547 - val_auc: 0.8630 - val_accuracy: 0.7784\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4820 - auc: 0.8364 - accuracy: 0.7546 - val_loss: 0.4529 - val_auc: 0.8644 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4791 - auc: 0.8332 - accuracy: 0.7728 - val_loss: 0.4523 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5018 - auc: 0.8258 - accuracy: 0.7476 - val_loss: 0.4509 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4428 - auc: 0.8652 - accuracy: 0.8053 - val_loss: 0.4500 - val_auc: 0.8636 - val_accuracy: 0.7784\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4910 - auc: 0.8257 - accuracy: 0.7842 - val_loss: 0.4499 - val_auc: 0.8627 - val_accuracy: 0.7784\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4509 - auc: 0.8620 - accuracy: 0.8073 - val_loss: 0.4491 - val_auc: 0.8629 - val_accuracy: 0.7784\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4535 - auc: 0.8490 - accuracy: 0.8146 - val_loss: 0.4491 - val_auc: 0.8625 - val_accuracy: 0.7784\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4840 - auc: 0.8345 - accuracy: 0.7772 - val_loss: 0.4482 - val_auc: 0.8638 - val_accuracy: 0.7784\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4414 - auc: 0.8696 - accuracy: 0.7953 - val_loss: 0.4477 - val_auc: 0.8651 - val_accuracy: 0.7622\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8302 - accuracy: 0.7970 - val_loss: 0.4472 - val_auc: 0.8654 - val_accuracy: 0.7622\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4764 - auc: 0.8486 - accuracy: 0.7790 - val_loss: 0.4468 - val_auc: 0.8654 - val_accuracy: 0.7622\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4701 - auc: 0.8473 - accuracy: 0.7795 - val_loss: 0.4468 - val_auc: 0.8648 - val_accuracy: 0.7622\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4898 - auc: 0.8297 - accuracy: 0.7726 - val_loss: 0.4469 - val_auc: 0.8654 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4176 - auc: 0.8811 - accuracy: 0.8148 - val_loss: 0.4469 - val_auc: 0.8649 - val_accuracy: 0.7730\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8508 - accuracy: 0.7838 - val_loss: 0.4470 - val_auc: 0.8651 - val_accuracy: 0.7730\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4600 - auc: 0.8470 - accuracy: 0.7938 - val_loss: 0.4476 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4187 - auc: 0.8824 - accuracy: 0.8003 - val_loss: 0.4474 - val_auc: 0.8634 - val_accuracy: 0.7730\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4233 - auc: 0.8769 - accuracy: 0.8125 - val_loss: 0.4471 - val_auc: 0.8643 - val_accuracy: 0.7730\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4600 - auc: 0.8597 - accuracy: 0.7834 - val_loss: 0.4465 - val_auc: 0.8646 - val_accuracy: 0.7676\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4249 - auc: 0.8718 - accuracy: 0.8108 - val_loss: 0.4465 - val_auc: 0.8644 - val_accuracy: 0.7784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd7e55a070>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 Regularization\n",
    "l1_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l1(0.01)))\n",
    "# Compiling model\n",
    "l1_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "invisible-premiere",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5289 - auc: 0.8011 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "l1_model = load_model(mc_path)\n",
    "eval = l1_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-picture",
   "metadata": {},
   "source": [
    "# 10.2. Regularización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "spiritual-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "correct-nomination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 126ms/step - loss: 0.7091 - auc: 0.6454 - accuracy: 0.6047 - val_loss: 0.6136 - val_auc: 0.7347 - val_accuracy: 0.6973\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5919 - auc: 0.7642 - accuracy: 0.6823 - val_loss: 0.5454 - val_auc: 0.8031 - val_accuracy: 0.7405\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5164 - auc: 0.8222 - accuracy: 0.7394 - val_loss: 0.5110 - val_auc: 0.8312 - val_accuracy: 0.7676\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5260 - auc: 0.7997 - accuracy: 0.7235 - val_loss: 0.4918 - val_auc: 0.8431 - val_accuracy: 0.7622\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5050 - auc: 0.8319 - accuracy: 0.7586 - val_loss: 0.4800 - val_auc: 0.8468 - val_accuracy: 0.7676\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4963 - auc: 0.8386 - accuracy: 0.7682 - val_loss: 0.4720 - val_auc: 0.8517 - val_accuracy: 0.7676\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4935 - auc: 0.8265 - accuracy: 0.7691 - val_loss: 0.4658 - val_auc: 0.8547 - val_accuracy: 0.7676\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4603 - auc: 0.8506 - accuracy: 0.7806 - val_loss: 0.4616 - val_auc: 0.8566 - val_accuracy: 0.7676\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4850 - auc: 0.8365 - accuracy: 0.7736 - val_loss: 0.4588 - val_auc: 0.8560 - val_accuracy: 0.7676\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4625 - auc: 0.8485 - accuracy: 0.7921 - val_loss: 0.4567 - val_auc: 0.8576 - val_accuracy: 0.7568\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4681 - auc: 0.8575 - accuracy: 0.8006 - val_loss: 0.4546 - val_auc: 0.8590 - val_accuracy: 0.7622\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4222 - auc: 0.8749 - accuracy: 0.8235 - val_loss: 0.4536 - val_auc: 0.8588 - val_accuracy: 0.7730\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4433 - auc: 0.8534 - accuracy: 0.8039 - val_loss: 0.4523 - val_auc: 0.8592 - val_accuracy: 0.7676\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4744 - auc: 0.8474 - accuracy: 0.7631 - val_loss: 0.4511 - val_auc: 0.8602 - val_accuracy: 0.7730\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4534 - auc: 0.8653 - accuracy: 0.8008 - val_loss: 0.4502 - val_auc: 0.8607 - val_accuracy: 0.7730\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4485 - auc: 0.8545 - accuracy: 0.8030 - val_loss: 0.4495 - val_auc: 0.8609 - val_accuracy: 0.7784\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4772 - auc: 0.8335 - accuracy: 0.7736 - val_loss: 0.4485 - val_auc: 0.8610 - val_accuracy: 0.7622\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4526 - auc: 0.8517 - accuracy: 0.7978 - val_loss: 0.4476 - val_auc: 0.8617 - val_accuracy: 0.7676\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4550 - auc: 0.8519 - accuracy: 0.7869 - val_loss: 0.4472 - val_auc: 0.8627 - val_accuracy: 0.7622\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4430 - auc: 0.8774 - accuracy: 0.8166 - val_loss: 0.4466 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4342 - auc: 0.8758 - accuracy: 0.7954 - val_loss: 0.4470 - val_auc: 0.8629 - val_accuracy: 0.7622\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4858 - auc: 0.8275 - accuracy: 0.7580 - val_loss: 0.4465 - val_auc: 0.8638 - val_accuracy: 0.7676\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4233 - auc: 0.8659 - accuracy: 0.8086 - val_loss: 0.4467 - val_auc: 0.8641 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4650 - auc: 0.8466 - accuracy: 0.7801 - val_loss: 0.4466 - val_auc: 0.8644 - val_accuracy: 0.7676\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4405 - auc: 0.8629 - accuracy: 0.8039 - val_loss: 0.4469 - val_auc: 0.8636 - val_accuracy: 0.7676\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4364 - auc: 0.8625 - accuracy: 0.8043 - val_loss: 0.4468 - val_auc: 0.8631 - val_accuracy: 0.7676\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4783 - auc: 0.8441 - accuracy: 0.7854 - val_loss: 0.4473 - val_auc: 0.8630 - val_accuracy: 0.7730\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4499 - auc: 0.8650 - accuracy: 0.8114 - val_loss: 0.4469 - val_auc: 0.8636 - val_accuracy: 0.7730\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4479 - auc: 0.8643 - accuracy: 0.7949 - val_loss: 0.4467 - val_auc: 0.8626 - val_accuracy: 0.7784\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4550 - auc: 0.8532 - accuracy: 0.8000 - val_loss: 0.4464 - val_auc: 0.8637 - val_accuracy: 0.7730\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4427 - auc: 0.8528 - accuracy: 0.8111 - val_loss: 0.4465 - val_auc: 0.8631 - val_accuracy: 0.7730\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4521 - auc: 0.8583 - accuracy: 0.7997 - val_loss: 0.4458 - val_auc: 0.8629 - val_accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd0472adc0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L2 Regularization\n",
    "l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acoustic-olympus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5280 - auc: 0.7996 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "l2_model = load_model(mc_path)\n",
    "eval = l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-wright",
   "metadata": {},
   "source": [
    "# 10.3. Regularización L1+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sonic-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1+L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "respected-winner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 138ms/step - loss: 0.6400 - auc: 0.7135 - accuracy: 0.6794 - val_loss: 0.5538 - val_auc: 0.7947 - val_accuracy: 0.6865\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5358 - auc: 0.8066 - accuracy: 0.7306 - val_loss: 0.5147 - val_auc: 0.8273 - val_accuracy: 0.7081\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5397 - auc: 0.7953 - accuracy: 0.7128 - val_loss: 0.4921 - val_auc: 0.8440 - val_accuracy: 0.7514\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5149 - auc: 0.8043 - accuracy: 0.7118 - val_loss: 0.4774 - val_auc: 0.8507 - val_accuracy: 0.7784\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4562 - auc: 0.8579 - accuracy: 0.7836 - val_loss: 0.4683 - val_auc: 0.8556 - val_accuracy: 0.7676\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4690 - auc: 0.8500 - accuracy: 0.7760 - val_loss: 0.4612 - val_auc: 0.8584 - val_accuracy: 0.7676\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5069 - auc: 0.8185 - accuracy: 0.7388 - val_loss: 0.4567 - val_auc: 0.8610 - val_accuracy: 0.7676\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4482 - auc: 0.8478 - accuracy: 0.8003 - val_loss: 0.4547 - val_auc: 0.8624 - val_accuracy: 0.7622\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4540 - auc: 0.8449 - accuracy: 0.7780 - val_loss: 0.4522 - val_auc: 0.8633 - val_accuracy: 0.7622\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5118 - auc: 0.8115 - accuracy: 0.7459 - val_loss: 0.4502 - val_auc: 0.8639 - val_accuracy: 0.7622\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4427 - auc: 0.8815 - accuracy: 0.7886 - val_loss: 0.4488 - val_auc: 0.8641 - val_accuracy: 0.7568\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4771 - auc: 0.8437 - accuracy: 0.7816 - val_loss: 0.4482 - val_auc: 0.8635 - val_accuracy: 0.7568\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4557 - auc: 0.8469 - accuracy: 0.7853 - val_loss: 0.4478 - val_auc: 0.8644 - val_accuracy: 0.7622\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4447 - auc: 0.8476 - accuracy: 0.7991 - val_loss: 0.4466 - val_auc: 0.8649 - val_accuracy: 0.7622\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4766 - auc: 0.8381 - accuracy: 0.7727 - val_loss: 0.4465 - val_auc: 0.8651 - val_accuracy: 0.7514\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4812 - auc: 0.8390 - accuracy: 0.7711 - val_loss: 0.4465 - val_auc: 0.8639 - val_accuracy: 0.7568\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4705 - auc: 0.8421 - accuracy: 0.7760 - val_loss: 0.4463 - val_auc: 0.8641 - val_accuracy: 0.7622\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8413 - accuracy: 0.7770 - val_loss: 0.4462 - val_auc: 0.8633 - val_accuracy: 0.7568\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4903 - auc: 0.8259 - accuracy: 0.7712 - val_loss: 0.4457 - val_auc: 0.8640 - val_accuracy: 0.7568\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4433 - auc: 0.8676 - accuracy: 0.8248 - val_loss: 0.4446 - val_auc: 0.8649 - val_accuracy: 0.7622\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4841 - auc: 0.8306 - accuracy: 0.7796 - val_loss: 0.4445 - val_auc: 0.8651 - val_accuracy: 0.7622\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4315 - auc: 0.8652 - accuracy: 0.7921 - val_loss: 0.4444 - val_auc: 0.8654 - val_accuracy: 0.7676\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4610 - auc: 0.8395 - accuracy: 0.7862 - val_loss: 0.4444 - val_auc: 0.8654 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4536 - auc: 0.8542 - accuracy: 0.8002 - val_loss: 0.4442 - val_auc: 0.8657 - val_accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cd06cb6850>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 and L2 Regularization\n",
    "l1l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l1l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "signal-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5276 - auc: 0.8001 - accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "l1l2_model = load_model(mc_path)\n",
    "eval = l1l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-reception",
   "metadata": {},
   "source": [
    "En este caso, se nota una leve mejora en la métrica empleando regularización L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-talent",
   "metadata": {},
   "source": [
    "# 11. Dropout\n",
    "Se emplea una capa extra de dropout para minimizar el overfitting. Este regularizador funciona ignorando a neuronas de forma aleatoria. Se realiza dropout **solo en la etapa de entrenamiento**. **En teoría, no se lleva muy bien con la normalización por capas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "documented-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fallen-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/dropout_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "royal-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4434 - auc: 0.8635 - accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44342607259750366, 0.8634992837905884, 0.7675675749778748]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model\n",
    "do_model = Sequential()\n",
    "# Adding dropout layer to network\n",
    "do_model.add(Dropout(0))\n",
    "# Adding Dense layer\n",
    "do_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "do_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "do_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "do_model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bridal-choice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4421 - auc: 0.8661 - accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44205453991889954, 0.866138756275177, 0.7621621489524841]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-understanding",
   "metadata": {},
   "source": [
    "# 12. Feature Engineering. Features Polinomiales\n",
    "El objertivo de esta sección es el de agregar variables de entrada al modelo, que surgen de combinar las variables originales. El grado del polinomio determina la cantidad de nuevas variables que se suman al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cardiovascular-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/poly2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "forbidden-namibia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 1/14 [=>............................] - ETA: 11s - loss: 0.7464 - auc: 0.6977 - accuracy: 0.5625WARNING:tensorflow:Trace already enabled\n",
      "14/14 [==============================] - 3s 129ms/step - loss: 0.7534 - auc: 0.6584 - accuracy: 0.6136 - val_loss: 0.5381 - val_auc: 0.7939 - val_accuracy: 0.7189\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5527 - auc: 0.7751 - accuracy: 0.6966 - val_loss: 0.5026 - val_auc: 0.8433 - val_accuracy: 0.7892\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5019 - auc: 0.8157 - accuracy: 0.7494 - val_loss: 0.4729 - val_auc: 0.8592 - val_accuracy: 0.8054\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4834 - auc: 0.8317 - accuracy: 0.7647 - val_loss: 0.4712 - val_auc: 0.8647 - val_accuracy: 0.7784\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8502 - accuracy: 0.7770 - val_loss: 0.4641 - val_auc: 0.8651 - val_accuracy: 0.8108\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4625 - auc: 0.8535 - accuracy: 0.7833 - val_loss: 0.4642 - val_auc: 0.8600 - val_accuracy: 0.7892\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4449 - auc: 0.8558 - accuracy: 0.7710 - val_loss: 0.4516 - val_auc: 0.8668 - val_accuracy: 0.7838\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4023 - auc: 0.8834 - accuracy: 0.7985 - val_loss: 0.4663 - val_auc: 0.8680 - val_accuracy: 0.7784\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4083 - auc: 0.8861 - accuracy: 0.8021 - val_loss: 0.4598 - val_auc: 0.8692 - val_accuracy: 0.7838\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4367 - auc: 0.8675 - accuracy: 0.7863 - val_loss: 0.4660 - val_auc: 0.8649 - val_accuracy: 0.7730\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4219 - auc: 0.8757 - accuracy: 0.8111 - val_loss: 0.4572 - val_auc: 0.8616 - val_accuracy: 0.7838\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4057 - auc: 0.8944 - accuracy: 0.7924 - val_loss: 0.4572 - val_auc: 0.8625 - val_accuracy: 0.7622\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4086 - auc: 0.8855 - accuracy: 0.7922 - val_loss: 0.4768 - val_auc: 0.8594 - val_accuracy: 0.7730\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4269 - auc: 0.8703 - accuracy: 0.7875 - val_loss: 0.4657 - val_auc: 0.8575 - val_accuracy: 0.7784\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.3903 - auc: 0.8892 - accuracy: 0.8189 - val_loss: 0.4604 - val_auc: 0.8615 - val_accuracy: 0.7730\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3664 - auc: 0.9126 - accuracy: 0.8238 - val_loss: 0.4637 - val_auc: 0.8595 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3777 - auc: 0.9043 - accuracy: 0.8183 - val_loss: 0.4660 - val_auc: 0.8577 - val_accuracy: 0.7676\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3915 - auc: 0.8941 - accuracy: 0.8100 - val_loss: 0.4672 - val_auc: 0.8567 - val_accuracy: 0.7568\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4172 - auc: 0.8727 - accuracy: 0.8058 - val_loss: 0.4671 - val_auc: 0.8562 - val_accuracy: 0.7514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce22f3e760>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating polynomial features\n",
    "poly2 = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly2.fit(x_train)\n",
    "# Creating model\n",
    "p2_model  = Sequential()\n",
    "p2_model.add(Dense(1, input_shape=(poly2.n_output_features_,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "p2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Fit model\n",
    "p2_model.fit(poly2.transform(x_train), y_train, validation_data=(poly2.transform(x_valid), y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "after-beatles",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6481 - auc: 0.7419 - accuracy: 0.7078\n"
     ]
    }
   ],
   "source": [
    "p2_model = load_model(mc_path)\n",
    "eval = p2_model.evaluate(x=poly2.transform(x_test), y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-blood",
   "metadata": {},
   "source": [
    "A continuación se observa la progresión de la métrica en **train** y **valid** en función del grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "complimentary-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-presence",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: Realizar la normalización de los datos **después** de aplicar el feature polinomial, sino se rompe todo :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "tested-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polynomial degrees to train and compute metrics\n",
    "poly_degrees = np.arange(1, 11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "underlying-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LRS callback\n",
    "# Define learning rate at start\n",
    "ilr = 0.1 # ilr=0.5, ds = 100000, dr=0.8, stc=False\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=1000, decay_rate=0.8, staircase=True) # Decay every (decay_steps) steps with a base of (decay_rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "linear-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model Checkpoint callback.\n",
    "mc_path = 'model_checkpoints/get_best_poly_deg_checkpoint.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-radius",
   "metadata": {},
   "source": [
    "En este punto cabe aclarar que se probó el parámetro *interaction_only* del preprocesador de polinomios y se llegó a la conclusión de que el desempeño mejora con este valor en *True*. Esto es así dado que, al activarlo, se logra un número mucho menor de variables en cada orden. Esto contribuye ampliamente a **reducir el overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "developing-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Polynomial order = 1 ---\n",
      "Input count = 8\n",
      "AUC for TRAIN subset is 0.8498\n",
      "AUC for VALID subset is 0.8656\n",
      "--- Polynomial order = 2 ---\n",
      "Input count = 36\n",
      "AUC for TRAIN subset is 0.8817\n",
      "AUC for VALID subset is 0.8570\n",
      "--- Polynomial order = 3 ---\n",
      "Input count = 92\n",
      "AUC for TRAIN subset is 0.8841\n",
      "AUC for VALID subset is 0.8480\n",
      "--- Polynomial order = 4 ---\n",
      "Input count = 162\n",
      "AUC for TRAIN subset is 0.9420\n",
      "AUC for VALID subset is 0.8382\n",
      "--- Polynomial order = 5 ---\n",
      "Input count = 218\n",
      "AUC for TRAIN subset is 0.8933\n",
      "AUC for VALID subset is 0.8497\n",
      "--- Polynomial order = 6 ---\n",
      "Input count = 246\n",
      "AUC for TRAIN subset is 0.8769\n",
      "AUC for VALID subset is 0.8546\n",
      "--- Polynomial order = 7 ---\n",
      "Input count = 254\n",
      "AUC for TRAIN subset is 0.9340\n",
      "AUC for VALID subset is 0.8411\n",
      "--- Polynomial order = 8 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.8746\n",
      "AUC for VALID subset is 0.8627\n",
      "--- Polynomial order = 9 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9372\n",
      "AUC for VALID subset is 0.8426\n",
      "--- Polynomial order = 10 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.8700\n",
      "AUC for VALID subset is 0.8579\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=True)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    \n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "academic-damages",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
