{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "boxed-sperm",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 2 - Ejercicio 1 - Regresión Logística\n",
    "# Notebook #2: Implementación de una Regresión Lineal\n",
    "En esta notebook se busca implementar una regresión logística para poder estimar la condición de diabético de un paciente, perteneciente al Pima Indians Dataset analizado en la notebook anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-quest",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "* Chequear correcto reemplazo de NaN por mean.\n",
    "* Meter el z-score en scripts comunes a ambos ejercicios. Chequear StandardScaler **correctamente inicializado**. **¿Errores de discretización?**\n",
    "    * ¿Dónde meto el área bajo la curva ROC y el F2? -> Respondido por Luqui y Karina.\n",
    "* Añadir **tensorboard** para log entre epochs. Migrar **TODOS LOS GRÁFICOS** a TensorBoard.\n",
    "    * Agregar evolución de f2-score sobre train en selección del umbral.\n",
    "* Graficar **learning rate**.\n",
    "* Sacar los evaluate con **test**, para evitar malas interpretaciones.\n",
    "* PRIMERA PRUEBA DE POLY (2) ESTÁ MAL! **Falta normalizar despues del poly**\n",
    "* Informar métricas secundarias\n",
    "* ¿Kernel/Activity regulariizer? -> **kernel regularizer** afecta a los pesos, **activity regularizer** a las salidas.\n",
    "\n",
    "# ¿Qué cosas puedo variar?\n",
    "* Función de activación:\n",
    "    * Sigmoid\n",
    "    * RELU\n",
    "    * ELU\n",
    "    * tanh\n",
    "    * Leaky RELU\n",
    "    \n",
    "* Optimizador:\n",
    "    * SGD\n",
    "    * Adam\n",
    "    \n",
    "* Early Stopping: Para el entrenamiento cuando la **loss** deja de mejorar. Se pasa a través de un **callback**. (https://keras.io/api/callbacks/early_stopping/) (https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "* Kernel Initializer: Esto es, como son los pesos y bias iniciales. (https://keras.io/api/layers/initializers/)\n",
    "* Model Checkpoint: Guarda un checkpoint del modelo. Puede configurarse para elegir el mejor. Se pasa por **callback**. (https://keras.io/api/callbacks/model_checkpoint/)\n",
    "* Scheduling Learning Rate: Se hace variar el **learning rate** con una función. Es un **callback**. (https://keras.io/api/callbacks/learning_rate_scheduler/)\n",
    "* Reg. dropout: Para evitar overfitting, la capa de dropout \"borra\" una entrada de forma aleatoria y escala el resto. Es una **capa**. (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "* Regularización L1 y L2: Limita el espacio de soluciones agregando un término a la **función de costo**. (https://keras.io/api/layers/regularizers/)\n",
    "* Data Augmentation\n",
    "* Batch Normalization: Normaliza las entradas (media=0, dev=1). Es una **capa**. (https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-fitness",
   "metadata": {},
   "source": [
    "# Dudas\n",
    "* Al generar la métrica F2, ¿me devuelve por batch o por epoch? -> Esto finalmente se explica más adelante.\n",
    "* Al evaluar el predict en threshold selection ¿batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-oxygen",
   "metadata": {},
   "source": [
    "# ¿Cuáles son los requerimientos para el **clasificador**?\n",
    "* Métrica principal: **Área bajo la curva ROC**\n",
    "* Buscar el **umbral de decisión** para maximizar el **f2 score** \n",
    "* Informar métricas secundarias:\n",
    "    * Especificidad - Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "    * Sensibilidad\n",
    "    * Valor predictivo positivo\n",
    "    * Valor predictivo negativo\n",
    "    \n",
    "* **Pregunta adicional**:\n",
    "Dada la situación en la cual cambia la prevalencia de la enfermedad en la población a ser del 20%. Se desea reutilizar el modelo sin volver a entrenar, ¿Cómo lo harían? ¿Qué métricas se mantienen igual y cuáles cambiarian?. **¿clases desbalanceadas -> class weight?**. Las f-score son buenas para casos no balanceados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-duncan",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "educational-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "external-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "valued-table",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read database from .csv\n",
    "df = pd.read_csv('../../databases/diabetes.csv', delimiter=',')\n",
    "\n",
    "# Show first rows of data\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-picking",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-williams",
   "metadata": {},
   "source": [
    "## 2.1 Filtrado de valores inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "literary-corrections",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>733.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.153420</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>12.382158</td>\n",
       "      <td>10.476982</td>\n",
       "      <td>118.775855</td>\n",
       "      <td>6.924988</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  763.000000     733.000000     541.000000  394.000000   \n",
       "mean      3.845052  121.686763      72.405184      29.153420  155.548223   \n",
       "std       3.369578   30.535641      12.382158      10.476982  118.775855   \n",
       "min       0.000000   44.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   76.250000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  125.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  190.000000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  757.000000                768.000000  768.000000  768.000000  \n",
       "mean    32.457464                  0.471876   33.240885    0.348958  \n",
       "std      6.924988                  0.331329   11.760232    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.500000                  0.243750   24.000000    0.000000  \n",
       "50%     32.300000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering Glucose values\n",
    "df['Glucose'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Blood Pressure values\n",
    "df['BloodPressure'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Skin Thickness values\n",
    "df['SkinThickness'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Insulin values\n",
    "df['Insulin'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Body Mass Index values\n",
    "df['BMI'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-louisville",
   "metadata": {},
   "source": [
    "## 2.2 Remoción de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intensive-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import remove_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hindu-wireless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>764.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>759.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.786649</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.115438</td>\n",
       "      <td>28.903346</td>\n",
       "      <td>132.610811</td>\n",
       "      <td>32.204005</td>\n",
       "      <td>0.429832</td>\n",
       "      <td>32.805007</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.278714</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>11.239072</td>\n",
       "      <td>9.865480</td>\n",
       "      <td>74.285393</td>\n",
       "      <td>6.491385</td>\n",
       "      <td>0.249684</td>\n",
       "      <td>11.113182</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   764.000000  763.000000     719.000000     538.000000  370.000000   \n",
       "mean      3.786649  121.686763      72.115438      28.903346  132.610811   \n",
       "std       3.278714   30.535641      11.239072       9.865480   74.285393   \n",
       "min       0.000000   44.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   75.000000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  120.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  177.500000   \n",
       "max      13.000000  199.000000     104.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  749.000000                739.000000  759.000000  768.000000  \n",
       "mean    32.204005                  0.429832   32.805007    0.348958  \n",
       "std      6.491385                  0.249684   11.113182    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.400000                  0.238000   24.000000    0.000000  \n",
       "50%     32.000000                  0.356000   29.000000    0.000000  \n",
       "75%     36.500000                  0.587000   40.000000    1.000000  \n",
       "max     50.000000                  1.191000   66.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "y_labels = ['Outcome']\n",
    "\n",
    "for column in x_labels:\n",
    "    remove_outliers(df, column)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-elevation",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "featured-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "explicit-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "photographic-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output variables for the model\n",
    "df_x = df[x_labels]\n",
    "df_y = df[y_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "micro-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Split the train_valid sub-dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=23, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "average-excuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>427.000000</td>\n",
       "      <td>426.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>286.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>414.000000</td>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.859485</td>\n",
       "      <td>120.514085</td>\n",
       "      <td>72.045113</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000962</td>\n",
       "      <td>0.430853</td>\n",
       "      <td>32.868235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.285896</td>\n",
       "      <td>29.742282</td>\n",
       "      <td>11.556850</td>\n",
       "      <td>9.853631</td>\n",
       "      <td>70.564358</td>\n",
       "      <td>6.568853</td>\n",
       "      <td>0.255626</td>\n",
       "      <td>11.111848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.343000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>36.100000</td>\n",
       "      <td>0.603250</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.182000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   427.000000  426.000000     399.000000     286.000000  203.000000   \n",
       "mean      3.859485  120.514085      72.045113      28.811189  132.458128   \n",
       "std       3.285896   29.742282      11.556850       9.853631   70.564358   \n",
       "min       0.000000   56.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000  100.000000      64.000000      22.000000   76.000000   \n",
       "50%       3.000000  115.000000      72.000000      29.000000  122.000000   \n",
       "75%       6.000000  138.000000      80.000000      35.750000  179.000000   \n",
       "max      13.000000  198.000000     102.000000      52.000000  335.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  416.000000                414.000000  425.000000  \n",
       "mean    32.000962                  0.430853   32.868235  \n",
       "std      6.568853                  0.255626   11.111848  \n",
       "min     18.200000                  0.085000   21.000000  \n",
       "25%     27.100000                  0.238000   24.000000  \n",
       "50%     31.600000                  0.343000   29.000000  \n",
       "75%     36.100000                  0.603250   40.000000  \n",
       "max     50.000000                  1.182000   66.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set before NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-thousand",
   "metadata": {},
   "source": [
    "# 4. Reemplazo de valores inválidos\n",
    "Como se destacó en el análisis estadístico de datos, el dataset suministrado posee varios valores faltantes en algunos individuos. Se asume que en la etapa de producción el modelo contará con todas las variables correctamente informadas, no admitiendo el faltante de alguna de ellas. Luego, se decide reemplazar aquellos valores inválidos en **train**, **valid** y **test** por la correspondiente media en el dataset de train. En este caso, se considera a la media como un estimador correcto para la ocasión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "saving-stupid",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of training\n",
    "train_means = x_train.mean().to_numpy()\n",
    "\n",
    "# Replacing nan values of the train dataset with training mean values\n",
    "for index, column in enumerate(x_train.columns):\n",
    "    x_train.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "\n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_test.columns):\n",
    "    x_test.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "    \n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_valid.columns):\n",
    "    x_valid.loc[:,column].replace(np.nan, train_means[index], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prompt-portugal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.859485</td>\n",
       "      <td>120.514085</td>\n",
       "      <td>72.045113</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000962</td>\n",
       "      <td>0.430853</td>\n",
       "      <td>32.868235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.278209</td>\n",
       "      <td>29.637862</td>\n",
       "      <td>11.144462</td>\n",
       "      <td>8.040755</td>\n",
       "      <td>48.477386</td>\n",
       "      <td>6.468323</td>\n",
       "      <td>0.251107</td>\n",
       "      <td>11.059801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>72.045113</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.182000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   429.000000  429.000000     429.000000     429.000000  429.000000   \n",
       "mean      3.859485  120.514085      72.045113      28.811189  132.458128   \n",
       "std       3.278209   29.637862      11.144462       8.040755   48.477386   \n",
       "min       0.000000   56.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000  100.000000      64.000000      26.000000  126.000000   \n",
       "50%       3.000000  116.000000      72.045113      28.811189  132.458128   \n",
       "75%       6.000000  138.000000      80.000000      32.000000  132.458128   \n",
       "max      13.000000  198.000000     102.000000      52.000000  335.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  429.000000                429.000000  429.000000  \n",
       "mean    32.000962                  0.430853   32.868235  \n",
       "std      6.468323                  0.251107   11.059801  \n",
       "min     18.200000                  0.085000   21.000000  \n",
       "25%     27.300000                  0.240000   24.000000  \n",
       "50%     32.000000                  0.351000   29.000000  \n",
       "75%     36.000000                  0.591000   40.000000  \n",
       "max     50.000000                  1.182000   66.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set after NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "informed-palmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.776859</td>\n",
       "      <td>124.081233</td>\n",
       "      <td>72.510059</td>\n",
       "      <td>29.026687</td>\n",
       "      <td>132.833231</td>\n",
       "      <td>32.662178</td>\n",
       "      <td>0.425718</td>\n",
       "      <td>32.824890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.233510</td>\n",
       "      <td>30.668200</td>\n",
       "      <td>10.629606</td>\n",
       "      <td>8.261217</td>\n",
       "      <td>52.817100</td>\n",
       "      <td>6.221732</td>\n",
       "      <td>0.236337</td>\n",
       "      <td>10.749211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>28.800000</td>\n",
       "      <td>0.236000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.400000</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>36.800000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   185.000000  185.000000     185.000000     185.000000  185.000000   \n",
       "mean      3.776859  124.081233      72.510059      29.026687  132.833231   \n",
       "std       3.233510   30.668200      10.629606       8.261217   52.817100   \n",
       "min       0.000000   44.000000      50.000000      11.000000   15.000000   \n",
       "25%       1.000000  101.000000      65.000000      24.000000  115.000000   \n",
       "50%       3.000000  120.000000      72.000000      28.811189  132.458128   \n",
       "75%       6.000000  145.000000      78.000000      33.000000  132.458128   \n",
       "max      13.000000  199.000000     104.000000      54.000000  330.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  185.000000                185.000000  185.000000  \n",
       "mean    32.662178                  0.425718   32.824890  \n",
       "std      6.221732                  0.236337   10.749211  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     28.800000                  0.236000   24.000000  \n",
       "50%     32.400000                  0.389000   30.000000  \n",
       "75%     36.800000                  0.549000   39.000000  \n",
       "max     49.600000                  1.189000   63.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set after NaN replacement\n",
    "x_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elder-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.597403</td>\n",
       "      <td>122.038961</td>\n",
       "      <td>71.814911</td>\n",
       "      <td>28.874262</td>\n",
       "      <td>132.374352</td>\n",
       "      <td>32.194175</td>\n",
       "      <td>0.432124</td>\n",
       "      <td>32.608678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.304818</td>\n",
       "      <td>32.320876</td>\n",
       "      <td>10.448675</td>\n",
       "      <td>8.867564</td>\n",
       "      <td>58.136767</td>\n",
       "      <td>6.484634</td>\n",
       "      <td>0.238998</td>\n",
       "      <td>11.431653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>26.925000</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.811189</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>32.000962</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>142.750000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>132.458128</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>40.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>49.300000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   154.000000  154.000000     154.000000     154.000000  154.000000   \n",
       "mean      3.597403  122.038961      71.814911      28.874262  132.374352   \n",
       "std       3.304818   32.320876      10.448675       8.867564   58.136767   \n",
       "min       0.000000   61.000000      44.000000       7.000000   23.000000   \n",
       "25%       1.000000   95.250000      64.000000      23.250000  108.250000   \n",
       "50%       3.000000  117.000000      72.000000      28.811189  132.458128   \n",
       "75%       5.750000  142.750000      79.500000      33.000000  132.458128   \n",
       "max      13.000000  197.000000      94.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  154.000000                154.000000  154.000000  \n",
       "mean    32.194175                  0.432124   32.608678  \n",
       "std      6.484634                  0.238998   11.431653  \n",
       "min     18.400000                  0.078000   21.000000  \n",
       "25%     26.925000                  0.254000   24.000000  \n",
       "50%     32.000962                  0.376500   28.000000  \n",
       "75%     36.625000                  0.567000   40.750000  \n",
       "max     49.300000                  1.191000   66.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set after NaN replacement\n",
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-advertising",
   "metadata": {},
   "source": [
    "# 5. Normalización de datos de entrada. Z Score. \n",
    "Dado que todas las variables en juego son numéricas, se puede aplicar z-score a todo el dataset. Esta operación se hace con el objetivo de poder obtener mayor información de los pesos calculados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "virtual-blowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! Backup unnormalized subsets for further utilization\n",
    "x_train_un = x_train\n",
    "x_valid_un = x_valid\n",
    "x_test_un = x_test\n",
    "\n",
    "# Apply z-score to all sub-datasets\n",
    "scalable_variables = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])\n",
    "    x_valid.loc[:, scalable_variables] = scaler.transform(x_valid.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "traditional-hands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.832142e-17</td>\n",
       "      <td>-7.349728e-17</td>\n",
       "      <td>2.204918e-16</td>\n",
       "      <td>5.465713e-16</td>\n",
       "      <td>-1.656277e-17</td>\n",
       "      <td>-3.519588e-17</td>\n",
       "      <td>-9.937661e-17</td>\n",
       "      <td>7.867315e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.178689e+00</td>\n",
       "      <td>-2.179287e+00</td>\n",
       "      <td>-2.878786e+00</td>\n",
       "      <td>-2.715747e+00</td>\n",
       "      <td>-2.446428e+00</td>\n",
       "      <td>-2.136114e+00</td>\n",
       "      <td>-1.378921e+00</td>\n",
       "      <td>-1.074349e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.732887e-01</td>\n",
       "      <td>-6.929662e-01</td>\n",
       "      <td>-7.227362e-01</td>\n",
       "      <td>-3.500257e-01</td>\n",
       "      <td>-1.333749e-01</td>\n",
       "      <td>-7.276152e-01</td>\n",
       "      <td>-7.609333e-01</td>\n",
       "      <td>-8.027802e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.624873e-01</td>\n",
       "      <td>-1.524859e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.423542e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.488270e-04</td>\n",
       "      <td>-3.183741e-01</td>\n",
       "      <td>-3.501647e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.537149e-01</td>\n",
       "      <td>5.906746e-01</td>\n",
       "      <td>7.146307e-01</td>\n",
       "      <td>3.970441e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.189715e-01</td>\n",
       "      <td>6.385106e-01</td>\n",
       "      <td>6.455895e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.791520e+00</td>\n",
       "      <td>2.617476e+00</td>\n",
       "      <td>2.691010e+00</td>\n",
       "      <td>2.887277e+00</td>\n",
       "      <td>4.182947e+00</td>\n",
       "      <td>2.785893e+00</td>\n",
       "      <td>2.994839e+00</td>\n",
       "      <td>2.999190e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pregnancies       Glucose  BloodPressure  SkinThickness       Insulin  \\\n",
       "count  4.290000e+02  4.290000e+02   4.290000e+02   4.290000e+02  4.290000e+02   \n",
       "mean   6.832142e-17 -7.349728e-17   2.204918e-16   5.465713e-16 -1.656277e-17   \n",
       "std    1.001168e+00  1.001168e+00   1.001168e+00   1.001168e+00  1.001168e+00   \n",
       "min   -1.178689e+00 -2.179287e+00  -2.878786e+00  -2.715747e+00 -2.446428e+00   \n",
       "25%   -8.732887e-01 -6.929662e-01  -7.227362e-01  -3.500257e-01 -1.333749e-01   \n",
       "50%   -2.624873e-01 -1.524859e-01   0.000000e+00   4.423542e-16  0.000000e+00   \n",
       "75%    6.537149e-01  5.906746e-01   7.146307e-01   3.970441e-01  0.000000e+00   \n",
       "max    2.791520e+00  2.617476e+00   2.691010e+00   2.887277e+00  4.182947e+00   \n",
       "\n",
       "                BMI  DiabetesPedigreeFunction           Age  \n",
       "count  4.290000e+02              4.290000e+02  4.290000e+02  \n",
       "mean  -3.519588e-17             -9.937661e-17  7.867315e-17  \n",
       "std    1.001168e+00              1.001168e+00  1.001168e+00  \n",
       "min   -2.136114e+00             -1.378921e+00 -1.074349e+00  \n",
       "25%   -7.276152e-01             -7.609333e-01 -8.027802e-01  \n",
       "50%   -1.488270e-04             -3.183741e-01 -3.501647e-01  \n",
       "75%    6.189715e-01              6.385106e-01  6.455895e-01  \n",
       "max    2.785893e+00              2.994839e+00  2.999190e+00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-distribution",
   "metadata": {},
   "source": [
    "# 6. Regresión Logística - Test #1\n",
    "Primera prueba de regresión logística. Se usa SGD y AUC como métrica principal. Se emplea la Binary Cross-Entropy como loss subrogada, dado que **la AUC no es diferenciable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "numerical-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TensorBoard for learning logging\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hairy-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "corrected-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import SensitivityAtSpecificity\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "brazilian-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/logistic_regression_first_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "virgin-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Get model brief\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "revised-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics definition\n",
    "metrics = ['AUC', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "motivated-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 12s 118ms/step - loss: 0.6307 - auc: 0.7346 - accuracy: 0.6796 - val_loss: 0.6738 - val_auc: 0.7110 - val_accuracy: 0.6486\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6529 - auc: 0.7107 - accuracy: 0.6771 - val_loss: 0.6610 - val_auc: 0.7200 - val_accuracy: 0.6432\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6388 - auc: 0.7266 - accuracy: 0.6646 - val_loss: 0.6490 - val_auc: 0.7296 - val_accuracy: 0.6541\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.6517 - auc: 0.7108 - accuracy: 0.6463 - val_loss: 0.6380 - val_auc: 0.7379 - val_accuracy: 0.6486\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6401 - auc: 0.7212 - accuracy: 0.6659 - val_loss: 0.6279 - val_auc: 0.7450 - val_accuracy: 0.6649\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6440 - auc: 0.7091 - accuracy: 0.6236 - val_loss: 0.6182 - val_auc: 0.7514 - val_accuracy: 0.6703\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6371 - auc: 0.7130 - accuracy: 0.6452 - val_loss: 0.6090 - val_auc: 0.7563 - val_accuracy: 0.6703\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6182 - auc: 0.7152 - accuracy: 0.6672 - val_loss: 0.6005 - val_auc: 0.7606 - val_accuracy: 0.6811\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5794 - auc: 0.7672 - accuracy: 0.6932 - val_loss: 0.5928 - val_auc: 0.7660 - val_accuracy: 0.6757\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5965 - auc: 0.7429 - accuracy: 0.6789 - val_loss: 0.5855 - val_auc: 0.7729 - val_accuracy: 0.6811\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5959 - auc: 0.7550 - accuracy: 0.6575 - val_loss: 0.5785 - val_auc: 0.7802 - val_accuracy: 0.6757\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6278 - auc: 0.7099 - accuracy: 0.6470 - val_loss: 0.5721 - val_auc: 0.7846 - val_accuracy: 0.6919\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6032 - auc: 0.7386 - accuracy: 0.6556 - val_loss: 0.5657 - val_auc: 0.7904 - val_accuracy: 0.6973\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6051 - auc: 0.7306 - accuracy: 0.6641 - val_loss: 0.5601 - val_auc: 0.7944 - val_accuracy: 0.7027\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5932 - auc: 0.7399 - accuracy: 0.6742 - val_loss: 0.5546 - val_auc: 0.7972 - val_accuracy: 0.7027\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5423 - auc: 0.7906 - accuracy: 0.7167 - val_loss: 0.5497 - val_auc: 0.8010 - val_accuracy: 0.7027\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5440 - auc: 0.7971 - accuracy: 0.7333 - val_loss: 0.5447 - val_auc: 0.8040 - val_accuracy: 0.7135\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5755 - auc: 0.7492 - accuracy: 0.6889 - val_loss: 0.5402 - val_auc: 0.8075 - val_accuracy: 0.7135\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5249 - auc: 0.8075 - accuracy: 0.7387 - val_loss: 0.5361 - val_auc: 0.8103 - val_accuracy: 0.7135\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5134 - auc: 0.8199 - accuracy: 0.7543 - val_loss: 0.5321 - val_auc: 0.8121 - val_accuracy: 0.7135\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5329 - auc: 0.7994 - accuracy: 0.7332 - val_loss: 0.5284 - val_auc: 0.8147 - val_accuracy: 0.7243\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5596 - auc: 0.7758 - accuracy: 0.7148 - val_loss: 0.5249 - val_auc: 0.8167 - val_accuracy: 0.7243\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5208 - auc: 0.8145 - accuracy: 0.7483 - val_loss: 0.5215 - val_auc: 0.8196 - val_accuracy: 0.7243\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4957 - auc: 0.8206 - accuracy: 0.7652 - val_loss: 0.5182 - val_auc: 0.8215 - val_accuracy: 0.7297\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5601 - auc: 0.7700 - accuracy: 0.7063 - val_loss: 0.5153 - val_auc: 0.8239 - val_accuracy: 0.7351\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5722 - auc: 0.7621 - accuracy: 0.7270 - val_loss: 0.5124 - val_auc: 0.8249 - val_accuracy: 0.7351\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5108 - auc: 0.8209 - accuracy: 0.7611 - val_loss: 0.5096 - val_auc: 0.8271 - val_accuracy: 0.7351\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4799 - auc: 0.8405 - accuracy: 0.7810 - val_loss: 0.5071 - val_auc: 0.8291 - val_accuracy: 0.7351\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4973 - auc: 0.8224 - accuracy: 0.7388 - val_loss: 0.5045 - val_auc: 0.8308 - val_accuracy: 0.7405\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5003 - auc: 0.8243 - accuracy: 0.7597 - val_loss: 0.5021 - val_auc: 0.8320 - val_accuracy: 0.7459\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4913 - auc: 0.8274 - accuracy: 0.7647 - val_loss: 0.4999 - val_auc: 0.8334 - val_accuracy: 0.7514\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5112 - auc: 0.7989 - accuracy: 0.7366 - val_loss: 0.4976 - val_auc: 0.8343 - val_accuracy: 0.7568\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4943 - auc: 0.8215 - accuracy: 0.7679 - val_loss: 0.4956 - val_auc: 0.8352 - val_accuracy: 0.7622\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5306 - auc: 0.7951 - accuracy: 0.7431 - val_loss: 0.4937 - val_auc: 0.8370 - val_accuracy: 0.7622\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4866 - auc: 0.8336 - accuracy: 0.7700 - val_loss: 0.4919 - val_auc: 0.8375 - val_accuracy: 0.7676\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5305 - auc: 0.7947 - accuracy: 0.7398 - val_loss: 0.4902 - val_auc: 0.8393 - val_accuracy: 0.7730\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4898 - auc: 0.8219 - accuracy: 0.7636 - val_loss: 0.4886 - val_auc: 0.8406 - val_accuracy: 0.7784\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4794 - auc: 0.8451 - accuracy: 0.7806 - val_loss: 0.4870 - val_auc: 0.8419 - val_accuracy: 0.7784\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5039 - auc: 0.8130 - accuracy: 0.7560 - val_loss: 0.4856 - val_auc: 0.8414 - val_accuracy: 0.7892\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4934 - auc: 0.8289 - accuracy: 0.7542 - val_loss: 0.4840 - val_auc: 0.8428 - val_accuracy: 0.7892\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5402 - auc: 0.7690 - accuracy: 0.7420 - val_loss: 0.4827 - val_auc: 0.8440 - val_accuracy: 0.7892\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5033 - auc: 0.8177 - accuracy: 0.7620 - val_loss: 0.4813 - val_auc: 0.8448 - val_accuracy: 0.7838\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5031 - auc: 0.8115 - accuracy: 0.7474 - val_loss: 0.4800 - val_auc: 0.8460 - val_accuracy: 0.7838\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5140 - auc: 0.7997 - accuracy: 0.7462 - val_loss: 0.4788 - val_auc: 0.8467 - val_accuracy: 0.7838\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5149 - auc: 0.8015 - accuracy: 0.7680 - val_loss: 0.4777 - val_auc: 0.8475 - val_accuracy: 0.7892\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4864 - auc: 0.8214 - accuracy: 0.7742 - val_loss: 0.4766 - val_auc: 0.8481 - val_accuracy: 0.7892\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8106 - accuracy: 0.7887 - val_loss: 0.4757 - val_auc: 0.8492 - val_accuracy: 0.7946\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4787 - auc: 0.8381 - accuracy: 0.7877 - val_loss: 0.4747 - val_auc: 0.8502 - val_accuracy: 0.7946\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8304 - accuracy: 0.7708 - val_loss: 0.4738 - val_auc: 0.8509 - val_accuracy: 0.7946\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4753 - auc: 0.8343 - accuracy: 0.7671 - val_loss: 0.4729 - val_auc: 0.8516 - val_accuracy: 0.7892\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4925 - auc: 0.8273 - accuracy: 0.7690 - val_loss: 0.4719 - val_auc: 0.8521 - val_accuracy: 0.7892\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4674 - auc: 0.8431 - accuracy: 0.7888 - val_loss: 0.4711 - val_auc: 0.8522 - val_accuracy: 0.7946\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4587 - auc: 0.8559 - accuracy: 0.8009 - val_loss: 0.4703 - val_auc: 0.8535 - val_accuracy: 0.7946\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4682 - auc: 0.8477 - accuracy: 0.7797 - val_loss: 0.4695 - val_auc: 0.8537 - val_accuracy: 0.7946\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4601 - auc: 0.8461 - accuracy: 0.7844 - val_loss: 0.4687 - val_auc: 0.8537 - val_accuracy: 0.8000\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8313 - accuracy: 0.7764 - val_loss: 0.4681 - val_auc: 0.8540 - val_accuracy: 0.7946\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4670 - auc: 0.8462 - accuracy: 0.7667 - val_loss: 0.4674 - val_auc: 0.8547 - val_accuracy: 0.8000\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4952 - auc: 0.8142 - accuracy: 0.7751 - val_loss: 0.4667 - val_auc: 0.8548 - val_accuracy: 0.7946\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4968 - auc: 0.8157 - accuracy: 0.7659 - val_loss: 0.4660 - val_auc: 0.8548 - val_accuracy: 0.8000\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5095 - auc: 0.8134 - accuracy: 0.7597 - val_loss: 0.4654 - val_auc: 0.8548 - val_accuracy: 0.8000\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4819 - auc: 0.8282 - accuracy: 0.7753 - val_loss: 0.4649 - val_auc: 0.8550 - val_accuracy: 0.7946\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4769 - auc: 0.8382 - accuracy: 0.7885 - val_loss: 0.4643 - val_auc: 0.8552 - val_accuracy: 0.7946\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4565 - auc: 0.8477 - accuracy: 0.8004 - val_loss: 0.4638 - val_auc: 0.8556 - val_accuracy: 0.7946\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8377 - accuracy: 0.7847 - val_loss: 0.4632 - val_auc: 0.8558 - val_accuracy: 0.7946\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4779 - auc: 0.8351 - accuracy: 0.7771 - val_loss: 0.4628 - val_auc: 0.8558 - val_accuracy: 0.7946\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4812 - auc: 0.8302 - accuracy: 0.7759 - val_loss: 0.4623 - val_auc: 0.8556 - val_accuracy: 0.7946\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4921 - auc: 0.8252 - accuracy: 0.7667 - val_loss: 0.4618 - val_auc: 0.8569 - val_accuracy: 0.7946\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4855 - auc: 0.8309 - accuracy: 0.7862 - val_loss: 0.4613 - val_auc: 0.8570 - val_accuracy: 0.7892\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4682 - auc: 0.8396 - accuracy: 0.7916 - val_loss: 0.4608 - val_auc: 0.8575 - val_accuracy: 0.7892\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4675 - auc: 0.8381 - accuracy: 0.7867 - val_loss: 0.4604 - val_auc: 0.8577 - val_accuracy: 0.7892\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4648 - auc: 0.8466 - accuracy: 0.7772 - val_loss: 0.4600 - val_auc: 0.8578 - val_accuracy: 0.7892\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4488 - auc: 0.8534 - accuracy: 0.7790 - val_loss: 0.4596 - val_auc: 0.8573 - val_accuracy: 0.7892\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4875 - auc: 0.8187 - accuracy: 0.7515 - val_loss: 0.4593 - val_auc: 0.8579 - val_accuracy: 0.7946\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4830 - auc: 0.8358 - accuracy: 0.7858 - val_loss: 0.4589 - val_auc: 0.8582 - val_accuracy: 0.7946\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4627 - auc: 0.8512 - accuracy: 0.7876 - val_loss: 0.4586 - val_auc: 0.8581 - val_accuracy: 0.7946\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4736 - auc: 0.8419 - accuracy: 0.7650 - val_loss: 0.4582 - val_auc: 0.8581 - val_accuracy: 0.7946\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4740 - auc: 0.8383 - accuracy: 0.7787 - val_loss: 0.4579 - val_auc: 0.8585 - val_accuracy: 0.7946\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4519 - auc: 0.8454 - accuracy: 0.7924 - val_loss: 0.4576 - val_auc: 0.8586 - val_accuracy: 0.7838\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4907 - auc: 0.8221 - accuracy: 0.7693 - val_loss: 0.4573 - val_auc: 0.8588 - val_accuracy: 0.7838\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4814 - auc: 0.8420 - accuracy: 0.7664 - val_loss: 0.4571 - val_auc: 0.8590 - val_accuracy: 0.7838\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4846 - auc: 0.8317 - accuracy: 0.7658 - val_loss: 0.4568 - val_auc: 0.8588 - val_accuracy: 0.7838\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4886 - auc: 0.8254 - accuracy: 0.7729 - val_loss: 0.4566 - val_auc: 0.8590 - val_accuracy: 0.7838\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4758 - auc: 0.8367 - accuracy: 0.7615 - val_loss: 0.4564 - val_auc: 0.8596 - val_accuracy: 0.7838\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4810 - auc: 0.8355 - accuracy: 0.7721 - val_loss: 0.4561 - val_auc: 0.8597 - val_accuracy: 0.7838\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4797 - auc: 0.8255 - accuracy: 0.7928 - val_loss: 0.4559 - val_auc: 0.8600 - val_accuracy: 0.7838\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4562 - auc: 0.8494 - accuracy: 0.7974 - val_loss: 0.4556 - val_auc: 0.8607 - val_accuracy: 0.7838\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4923 - auc: 0.8303 - accuracy: 0.7605 - val_loss: 0.4554 - val_auc: 0.8609 - val_accuracy: 0.7838\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4979 - auc: 0.8221 - accuracy: 0.7656 - val_loss: 0.4552 - val_auc: 0.8604 - val_accuracy: 0.7838\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4786 - auc: 0.8425 - accuracy: 0.7727 - val_loss: 0.4549 - val_auc: 0.8606 - val_accuracy: 0.7838\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4481 - auc: 0.8550 - accuracy: 0.7908 - val_loss: 0.4546 - val_auc: 0.8607 - val_accuracy: 0.7838\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4944 - auc: 0.8288 - accuracy: 0.7665 - val_loss: 0.4544 - val_auc: 0.8608 - val_accuracy: 0.7838\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4938 - auc: 0.8287 - accuracy: 0.7642 - val_loss: 0.4542 - val_auc: 0.8607 - val_accuracy: 0.7838\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4767 - auc: 0.8376 - accuracy: 0.7783 - val_loss: 0.4540 - val_auc: 0.8614 - val_accuracy: 0.7838\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4652 - auc: 0.8397 - accuracy: 0.7743 - val_loss: 0.4538 - val_auc: 0.8617 - val_accuracy: 0.7838\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8557 - accuracy: 0.7725 - val_loss: 0.4536 - val_auc: 0.8618 - val_accuracy: 0.7838\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4539 - auc: 0.8590 - accuracy: 0.7921 - val_loss: 0.4534 - val_auc: 0.8618 - val_accuracy: 0.7838\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4363 - auc: 0.8621 - accuracy: 0.8001 - val_loss: 0.4533 - val_auc: 0.8616 - val_accuracy: 0.7838\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4211 - auc: 0.8753 - accuracy: 0.8081 - val_loss: 0.4532 - val_auc: 0.8614 - val_accuracy: 0.7838\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4756 - auc: 0.8297 - accuracy: 0.7563 - val_loss: 0.4530 - val_auc: 0.8616 - val_accuracy: 0.7838\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4837 - auc: 0.8257 - accuracy: 0.7720 - val_loss: 0.4528 - val_auc: 0.8615 - val_accuracy: 0.7838\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4847 - auc: 0.8207 - accuracy: 0.7574 - val_loss: 0.4527 - val_auc: 0.8619 - val_accuracy: 0.7838\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4344 - auc: 0.8669 - accuracy: 0.7929 - val_loss: 0.4525 - val_auc: 0.8617 - val_accuracy: 0.7838\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4741 - auc: 0.8224 - accuracy: 0.7670 - val_loss: 0.4525 - val_auc: 0.8617 - val_accuracy: 0.7838\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4816 - auc: 0.8287 - accuracy: 0.7518 - val_loss: 0.4523 - val_auc: 0.8624 - val_accuracy: 0.7838\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4893 - auc: 0.8314 - accuracy: 0.7539 - val_loss: 0.4521 - val_auc: 0.8624 - val_accuracy: 0.7892\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4672 - auc: 0.8385 - accuracy: 0.7724 - val_loss: 0.4519 - val_auc: 0.8624 - val_accuracy: 0.7892\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4508 - auc: 0.8608 - accuracy: 0.7780 - val_loss: 0.4518 - val_auc: 0.8622 - val_accuracy: 0.7892\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4777 - auc: 0.8207 - accuracy: 0.7505 - val_loss: 0.4517 - val_auc: 0.8625 - val_accuracy: 0.7838\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4532 - auc: 0.8614 - accuracy: 0.7849 - val_loss: 0.4516 - val_auc: 0.8626 - val_accuracy: 0.7838\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4455 - auc: 0.8607 - accuracy: 0.8008 - val_loss: 0.4514 - val_auc: 0.8629 - val_accuracy: 0.7838\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4406 - auc: 0.8649 - accuracy: 0.7867 - val_loss: 0.4514 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4556 - auc: 0.8478 - accuracy: 0.7909 - val_loss: 0.4512 - val_auc: 0.8631 - val_accuracy: 0.7838\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4821 - auc: 0.8317 - accuracy: 0.7588 - val_loss: 0.4510 - val_auc: 0.8631 - val_accuracy: 0.7838\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4524 - auc: 0.8589 - accuracy: 0.7963 - val_loss: 0.4509 - val_auc: 0.8633 - val_accuracy: 0.7838\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4878 - auc: 0.8265 - accuracy: 0.7692 - val_loss: 0.4508 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4685 - auc: 0.8367 - accuracy: 0.7596 - val_loss: 0.4506 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4676 - auc: 0.8337 - accuracy: 0.7774 - val_loss: 0.4505 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4698 - auc: 0.8318 - accuracy: 0.7720 - val_loss: 0.4504 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4842 - auc: 0.8302 - accuracy: 0.7619 - val_loss: 0.4502 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4684 - auc: 0.8366 - accuracy: 0.7753 - val_loss: 0.4501 - val_auc: 0.8631 - val_accuracy: 0.7838\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4548 - auc: 0.8522 - accuracy: 0.7873 - val_loss: 0.4500 - val_auc: 0.8626 - val_accuracy: 0.7784\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5065 - auc: 0.8088 - accuracy: 0.7563 - val_loss: 0.4499 - val_auc: 0.8628 - val_accuracy: 0.7784\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4462 - auc: 0.8579 - accuracy: 0.7794 - val_loss: 0.4498 - val_auc: 0.8628 - val_accuracy: 0.7784\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4878 - auc: 0.8259 - accuracy: 0.7799 - val_loss: 0.4497 - val_auc: 0.8630 - val_accuracy: 0.7784\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4854 - auc: 0.8310 - accuracy: 0.7453 - val_loss: 0.4497 - val_auc: 0.8627 - val_accuracy: 0.7784\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5066 - auc: 0.8124 - accuracy: 0.7565 - val_loss: 0.4496 - val_auc: 0.8628 - val_accuracy: 0.7784\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4661 - auc: 0.8460 - accuracy: 0.7738 - val_loss: 0.4495 - val_auc: 0.8627 - val_accuracy: 0.7784\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4675 - auc: 0.8426 - accuracy: 0.7774 - val_loss: 0.4494 - val_auc: 0.8627 - val_accuracy: 0.7784\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4758 - auc: 0.8421 - accuracy: 0.7733 - val_loss: 0.4493 - val_auc: 0.8631 - val_accuracy: 0.7784\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4895 - auc: 0.8247 - accuracy: 0.7557 - val_loss: 0.4493 - val_auc: 0.8629 - val_accuracy: 0.7784\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4625 - auc: 0.8420 - accuracy: 0.7728 - val_loss: 0.4492 - val_auc: 0.8628 - val_accuracy: 0.7784\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4888 - auc: 0.8280 - accuracy: 0.7838 - val_loss: 0.4491 - val_auc: 0.8629 - val_accuracy: 0.7784\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4371 - auc: 0.8480 - accuracy: 0.8113 - val_loss: 0.4491 - val_auc: 0.8627 - val_accuracy: 0.7784\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4737 - auc: 0.8300 - accuracy: 0.7732 - val_loss: 0.4490 - val_auc: 0.8626 - val_accuracy: 0.7784\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4385 - auc: 0.8697 - accuracy: 0.7822 - val_loss: 0.4489 - val_auc: 0.8629 - val_accuracy: 0.7784\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4754 - auc: 0.8373 - accuracy: 0.7795 - val_loss: 0.4488 - val_auc: 0.8631 - val_accuracy: 0.7784\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4749 - auc: 0.8261 - accuracy: 0.7613 - val_loss: 0.4487 - val_auc: 0.8632 - val_accuracy: 0.7784\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4629 - auc: 0.8267 - accuracy: 0.7951 - val_loss: 0.4486 - val_auc: 0.8632 - val_accuracy: 0.7784\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4673 - auc: 0.8440 - accuracy: 0.7779 - val_loss: 0.4486 - val_auc: 0.8632 - val_accuracy: 0.7784\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4514 - auc: 0.8579 - accuracy: 0.7877 - val_loss: 0.4485 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4875 - auc: 0.8253 - accuracy: 0.7753 - val_loss: 0.4485 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8524 - accuracy: 0.7804 - val_loss: 0.4484 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4906 - auc: 0.8268 - accuracy: 0.7666 - val_loss: 0.4484 - val_auc: 0.8633 - val_accuracy: 0.7838\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4442 - auc: 0.8611 - accuracy: 0.8132 - val_loss: 0.4484 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4674 - auc: 0.8441 - accuracy: 0.7862 - val_loss: 0.4484 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4723 - auc: 0.8416 - accuracy: 0.7713 - val_loss: 0.4484 - val_auc: 0.8633 - val_accuracy: 0.7838\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4959 - auc: 0.8278 - accuracy: 0.7587 - val_loss: 0.4483 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4992 - auc: 0.8253 - accuracy: 0.7577 - val_loss: 0.4482 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4183 - auc: 0.8689 - accuracy: 0.8251 - val_loss: 0.4482 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4673 - auc: 0.8432 - accuracy: 0.7766 - val_loss: 0.4483 - val_auc: 0.8633 - val_accuracy: 0.7838\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4761 - auc: 0.8441 - accuracy: 0.7705 - val_loss: 0.4483 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4532 - auc: 0.8462 - accuracy: 0.7772 - val_loss: 0.4482 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4626 - auc: 0.8247 - accuracy: 0.7653 - val_loss: 0.4481 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4548 - auc: 0.8590 - accuracy: 0.7972 - val_loss: 0.4480 - val_auc: 0.8635 - val_accuracy: 0.7838\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4833 - auc: 0.8257 - accuracy: 0.7611 - val_loss: 0.4480 - val_auc: 0.8633 - val_accuracy: 0.7838\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4646 - auc: 0.8433 - accuracy: 0.7866 - val_loss: 0.4479 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4881 - auc: 0.8290 - accuracy: 0.7543 - val_loss: 0.4478 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4912 - auc: 0.8210 - accuracy: 0.7655 - val_loss: 0.4478 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4756 - auc: 0.8295 - accuracy: 0.7651 - val_loss: 0.4477 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4643 - auc: 0.8598 - accuracy: 0.7716 - val_loss: 0.4477 - val_auc: 0.8638 - val_accuracy: 0.7838\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4638 - auc: 0.8452 - accuracy: 0.7830 - val_loss: 0.4476 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4805 - auc: 0.8297 - accuracy: 0.7738 - val_loss: 0.4476 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4878 - auc: 0.8251 - accuracy: 0.7528 - val_loss: 0.4475 - val_auc: 0.8633 - val_accuracy: 0.7838\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4528 - auc: 0.8589 - accuracy: 0.7858 - val_loss: 0.4475 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4630 - auc: 0.8400 - accuracy: 0.7798 - val_loss: 0.4474 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4577 - auc: 0.8589 - accuracy: 0.7879 - val_loss: 0.4473 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4596 - auc: 0.8570 - accuracy: 0.7816 - val_loss: 0.4473 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4570 - auc: 0.8502 - accuracy: 0.7985 - val_loss: 0.4473 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4398 - auc: 0.8632 - accuracy: 0.7987 - val_loss: 0.4473 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8341 - accuracy: 0.7766 - val_loss: 0.4473 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4562 - auc: 0.8409 - accuracy: 0.7853 - val_loss: 0.4472 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8261 - accuracy: 0.7688 - val_loss: 0.4472 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4725 - auc: 0.8468 - accuracy: 0.7735 - val_loss: 0.4471 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4800 - auc: 0.8364 - accuracy: 0.7682 - val_loss: 0.4471 - val_auc: 0.8634 - val_accuracy: 0.7838\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4852 - auc: 0.8293 - accuracy: 0.7595 - val_loss: 0.4471 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4670 - auc: 0.8459 - accuracy: 0.7722 - val_loss: 0.4471 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4483 - auc: 0.8547 - accuracy: 0.7834 - val_loss: 0.4470 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4521 - auc: 0.8604 - accuracy: 0.7795 - val_loss: 0.4469 - val_auc: 0.8636 - val_accuracy: 0.7838\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4775 - auc: 0.8345 - accuracy: 0.7760 - val_loss: 0.4468 - val_auc: 0.8638 - val_accuracy: 0.7838\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4459 - auc: 0.8645 - accuracy: 0.7836 - val_loss: 0.4468 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4791 - auc: 0.8372 - accuracy: 0.7696 - val_loss: 0.4468 - val_auc: 0.8638 - val_accuracy: 0.7838\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5002 - auc: 0.8102 - accuracy: 0.7636 - val_loss: 0.4467 - val_auc: 0.8637 - val_accuracy: 0.7838\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8522 - accuracy: 0.7814 - val_loss: 0.4467 - val_auc: 0.8640 - val_accuracy: 0.7838\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4804 - auc: 0.8333 - accuracy: 0.7843 - val_loss: 0.4467 - val_auc: 0.8638 - val_accuracy: 0.7838\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5071 - auc: 0.8119 - accuracy: 0.7527 - val_loss: 0.4466 - val_auc: 0.8640 - val_accuracy: 0.7838\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4697 - auc: 0.8403 - accuracy: 0.7696 - val_loss: 0.4467 - val_auc: 0.8640 - val_accuracy: 0.7838\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5061 - auc: 0.8103 - accuracy: 0.7475 - val_loss: 0.4467 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4919 - auc: 0.8326 - accuracy: 0.7573 - val_loss: 0.4467 - val_auc: 0.8644 - val_accuracy: 0.7838\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4510 - auc: 0.8625 - accuracy: 0.8016 - val_loss: 0.4467 - val_auc: 0.8641 - val_accuracy: 0.7838\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4298 - auc: 0.8653 - accuracy: 0.8022 - val_loss: 0.4467 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4857 - auc: 0.8081 - accuracy: 0.7757 - val_loss: 0.4466 - val_auc: 0.8642 - val_accuracy: 0.7838\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4687 - auc: 0.8379 - accuracy: 0.7806 - val_loss: 0.4466 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4853 - auc: 0.8259 - accuracy: 0.7484 - val_loss: 0.4465 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4812 - auc: 0.8357 - accuracy: 0.7625 - val_loss: 0.4465 - val_auc: 0.8639 - val_accuracy: 0.7838\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4633 - auc: 0.8543 - accuracy: 0.7851 - val_loss: 0.4465 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8384 - accuracy: 0.7700 - val_loss: 0.4465 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4802 - auc: 0.8291 - accuracy: 0.7845 - val_loss: 0.4464 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4684 - auc: 0.8398 - accuracy: 0.7919 - val_loss: 0.4464 - val_auc: 0.8650 - val_accuracy: 0.7838\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4363 - auc: 0.8651 - accuracy: 0.8133 - val_loss: 0.4464 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4916 - auc: 0.8157 - accuracy: 0.7647 - val_loss: 0.4465 - val_auc: 0.8648 - val_accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219c72fe7c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=1, callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "animated-factor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14064), started 0:58:04 ago. (Use '!kill 14064' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6d8c582459ffd285\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6d8c582459ffd285\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard launch\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "amber-football",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5086 - auc: 0.7983 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "model = load_model(mc_path)\n",
    "eval = model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-carnival",
   "metadata": {},
   "source": [
    "# 7. Elección del umbral usando f2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-configuration",
   "metadata": {},
   "source": [
    "A la prueba anterior, se suma la selección del umbral (o **threshiold**) con el cual el clasificador discrimina entre clases. El mejor umbral de clasificación se calcula para todos los modelos, después del correspondiente entrenamiento. Para esta elección se elije el mejor valor del f2-score sobre el subset de **valid**. También se muestra la evolución de esta métrica respecto al umbral en el subset de **train**. En teoría, este umbral **no modifica la mérica principal del modelo, que es el área bajo la curva ROC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "equipped-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def round_threshold(vector, threshold=0.5):\n",
    "    rounded_vector = []\n",
    "    for element in vector:\n",
    "        if element >= threshold:\n",
    "            rounded_vector.append(1)\n",
    "        else:\n",
    "            rounded_vector.append(0)\n",
    "            \n",
    "    return np.array(rounded_vector)\n",
    "        \n",
    "def f2_threshold_selection(y_probs_valid, y_true_valid, y_probs_train, y_true_train, steps=100, plot=True):\n",
    "    # Thresholds and f2-score vectors\n",
    "    thresholds = np.linspace(0, 1, steps)\n",
    "    f2_score_valid = []\n",
    "    f2_score_train = []\n",
    "    \n",
    "    for thld in thresholds:\n",
    "        # Generate predictions with current threshold\n",
    "        y_pred_valid = round_threshold(vector=y_probs_valid, threshold=thld)\n",
    "        y_pred_train = round_threshold(vector=y_probs_train, threshold=thld)\n",
    "        # Compute f2 score for that threshold and append\n",
    "        score_valid = fbeta_score(y_true=y_true_valid, y_pred=y_pred_valid, beta=2)\n",
    "        score_train = fbeta_score(y_true=y_true_train, y_pred=y_pred_train, beta=2)\n",
    "        f2_score_valid.append(score_valid)\n",
    "        f2_score_train.append(score_train)\n",
    "    \n",
    "    idx = np.argmax(f2_score_valid)\n",
    "    if plot == True:\n",
    "        plt.plot(thresholds, f2_score_valid, label='valid')\n",
    "        plt.plot(thresholds, f2_score_train, label='train')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('F2 score')\n",
    "        plt.axvline(thresholds[idx], color='black', linestyle='--')\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "        plt.grid(b=True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return thresholds, f2_score_valid, idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "north-program",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.27272727272727276"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get binary-class probability from model\n",
    "y_probs_valid = model.predict(x_valid)\n",
    "y_probs_train = model.predict(x_train)\n",
    "thresholds, f2_score, idx = f2_threshold_selection(y_probs_valid, y_valid, y_probs_train, y_train, steps=100)\n",
    "thresholds[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-broadcasting",
   "metadata": {},
   "source": [
    "# 8. Early Stopping\n",
    "Habiendo concluido el test #1, se cree necesario agregar un callback de early stopping al modelo. Este callback deberá detener el proceso de aprendizaje en el momento en el que la **métrica principal** del modelo **deje de aumentar**. Posteriormente, se recupera el modelo con mejor performance en cuanto a esta métrica (AUC). Cabe aclarar que esta técnica es especialmente útil cuando la métrica principal no es diferenciable, y por ende se debe emplear una **loss subrogada** (en este caso, la binary cross entropy). De esta forma, el número de epochs que recorra el proceso de entrenamiento se verá limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "afraid-cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Early Stopping callback from keras.\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "moderate-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Early Stopping callback\n",
    "es_callback = EarlyStopping(monitor='val_auc', mode='max', min_delta=0.001, patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stretch-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Checkpoint callback\n",
    "mc_path = 'model_checkpoints/early_stopping_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "statistical-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "es_model = Sequential()\n",
    "es_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "proved-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "14/14 [==============================] - 3s 166ms/step - loss: 1.0044 - auc: 0.2718 - accuracy: 0.3230 - val_loss: 1.0214 - val_auc: 0.2673 - val_accuracy: 0.3676\n",
      "Epoch 2/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.9317 - auc: 0.3183 - accuracy: 0.3563 - val_loss: 0.9779 - val_auc: 0.2886 - val_accuracy: 0.3838\n",
      "Epoch 3/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.9261 - auc: 0.3438 - accuracy: 0.4109 - val_loss: 0.9391 - val_auc: 0.3077 - val_accuracy: 0.4108\n",
      "Epoch 4/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8948 - auc: 0.3260 - accuracy: 0.4179 - val_loss: 0.9043 - val_auc: 0.3329 - val_accuracy: 0.4054\n",
      "Epoch 5/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8354 - auc: 0.4063 - accuracy: 0.4643 - val_loss: 0.8722 - val_auc: 0.3575 - val_accuracy: 0.4378\n",
      "Epoch 6/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8164 - auc: 0.4270 - accuracy: 0.4970 - val_loss: 0.8437 - val_auc: 0.3835 - val_accuracy: 0.4703\n",
      "Epoch 7/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8037 - auc: 0.4488 - accuracy: 0.5216 - val_loss: 0.8183 - val_auc: 0.4086 - val_accuracy: 0.4865\n",
      "Epoch 8/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7831 - auc: 0.4783 - accuracy: 0.5346 - val_loss: 0.7944 - val_auc: 0.4370 - val_accuracy: 0.4811\n",
      "Epoch 9/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.8064 - auc: 0.4516 - accuracy: 0.5213 - val_loss: 0.7736 - val_auc: 0.4649 - val_accuracy: 0.4973\n",
      "Epoch 10/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7367 - auc: 0.5153 - accuracy: 0.5585 - val_loss: 0.7547 - val_auc: 0.4935 - val_accuracy: 0.5135\n",
      "Epoch 11/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7206 - auc: 0.5513 - accuracy: 0.5908 - val_loss: 0.7372 - val_auc: 0.5190 - val_accuracy: 0.5405\n",
      "Epoch 12/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7149 - auc: 0.5691 - accuracy: 0.5676 - val_loss: 0.7211 - val_auc: 0.5433 - val_accuracy: 0.5676\n",
      "Epoch 13/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7115 - auc: 0.5506 - accuracy: 0.5822 - val_loss: 0.7058 - val_auc: 0.5688 - val_accuracy: 0.5676\n",
      "Epoch 14/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6887 - auc: 0.5912 - accuracy: 0.6025 - val_loss: 0.6917 - val_auc: 0.5904 - val_accuracy: 0.5838\n",
      "Epoch 15/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6656 - auc: 0.6226 - accuracy: 0.6382 - val_loss: 0.6790 - val_auc: 0.6116 - val_accuracy: 0.6108\n",
      "Epoch 16/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.6351 - auc: 0.6651 - accuracy: 0.6767 - val_loss: 0.6672 - val_auc: 0.6304 - val_accuracy: 0.6162\n",
      "Epoch 17/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6259 - auc: 0.6892 - accuracy: 0.6742 - val_loss: 0.6564 - val_auc: 0.6476 - val_accuracy: 0.6324\n",
      "Epoch 18/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6422 - auc: 0.6696 - accuracy: 0.6509 - val_loss: 0.6467 - val_auc: 0.6618 - val_accuracy: 0.6486\n",
      "Epoch 19/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6542 - auc: 0.6408 - accuracy: 0.6353 - val_loss: 0.6371 - val_auc: 0.6748 - val_accuracy: 0.6486\n",
      "Epoch 20/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6586 - auc: 0.6550 - accuracy: 0.6488 - val_loss: 0.6284 - val_auc: 0.6900 - val_accuracy: 0.6486\n",
      "Epoch 21/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6110 - auc: 0.7050 - accuracy: 0.6578 - val_loss: 0.6198 - val_auc: 0.7026 - val_accuracy: 0.6378\n",
      "Epoch 22/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6264 - auc: 0.6899 - accuracy: 0.6564 - val_loss: 0.6122 - val_auc: 0.7122 - val_accuracy: 0.6486\n",
      "Epoch 23/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6184 - auc: 0.6975 - accuracy: 0.6517 - val_loss: 0.6049 - val_auc: 0.7210 - val_accuracy: 0.6595\n",
      "Epoch 24/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6264 - auc: 0.6909 - accuracy: 0.6378 - val_loss: 0.5981 - val_auc: 0.7288 - val_accuracy: 0.6649\n",
      "Epoch 25/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5750 - auc: 0.7592 - accuracy: 0.7003 - val_loss: 0.5916 - val_auc: 0.7391 - val_accuracy: 0.6811\n",
      "Epoch 26/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5764 - auc: 0.7535 - accuracy: 0.6862 - val_loss: 0.5857 - val_auc: 0.7461 - val_accuracy: 0.6973\n",
      "Epoch 27/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5807 - auc: 0.7575 - accuracy: 0.7036 - val_loss: 0.5802 - val_auc: 0.7521 - val_accuracy: 0.6973\n",
      "Epoch 28/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5797 - auc: 0.7527 - accuracy: 0.6966 - val_loss: 0.5748 - val_auc: 0.7580 - val_accuracy: 0.7027\n",
      "Epoch 29/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5828 - auc: 0.7472 - accuracy: 0.6721 - val_loss: 0.5700 - val_auc: 0.7621 - val_accuracy: 0.7189\n",
      "Epoch 30/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5854 - auc: 0.7322 - accuracy: 0.6748 - val_loss: 0.5653 - val_auc: 0.7660 - val_accuracy: 0.7297\n",
      "Epoch 31/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5749 - auc: 0.7603 - accuracy: 0.6888 - val_loss: 0.5609 - val_auc: 0.7721 - val_accuracy: 0.7189\n",
      "Epoch 32/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5920 - auc: 0.7550 - accuracy: 0.6714 - val_loss: 0.5565 - val_auc: 0.7761 - val_accuracy: 0.7243\n",
      "Epoch 33/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5716 - auc: 0.7504 - accuracy: 0.6848 - val_loss: 0.5526 - val_auc: 0.7800 - val_accuracy: 0.7297\n",
      "Epoch 34/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5414 - auc: 0.7959 - accuracy: 0.7128 - val_loss: 0.5485 - val_auc: 0.7863 - val_accuracy: 0.7351\n",
      "Epoch 35/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5644 - auc: 0.7608 - accuracy: 0.6914 - val_loss: 0.5446 - val_auc: 0.7914 - val_accuracy: 0.7243\n",
      "Epoch 36/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5772 - auc: 0.7641 - accuracy: 0.7041 - val_loss: 0.5412 - val_auc: 0.7942 - val_accuracy: 0.7243\n",
      "Epoch 37/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5396 - auc: 0.7893 - accuracy: 0.7165 - val_loss: 0.5378 - val_auc: 0.7966 - val_accuracy: 0.7243\n",
      "Epoch 38/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5196 - auc: 0.8188 - accuracy: 0.7269 - val_loss: 0.5346 - val_auc: 0.8000 - val_accuracy: 0.7189\n",
      "Epoch 39/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5657 - auc: 0.7707 - accuracy: 0.7083 - val_loss: 0.5315 - val_auc: 0.8032 - val_accuracy: 0.7189\n",
      "Epoch 40/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5333 - auc: 0.7977 - accuracy: 0.7256 - val_loss: 0.5285 - val_auc: 0.8052 - val_accuracy: 0.7189\n",
      "Epoch 41/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5614 - auc: 0.7747 - accuracy: 0.6955 - val_loss: 0.5257 - val_auc: 0.8079 - val_accuracy: 0.7189\n",
      "Epoch 42/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5425 - auc: 0.7724 - accuracy: 0.7200 - val_loss: 0.5229 - val_auc: 0.8100 - val_accuracy: 0.7351\n",
      "Epoch 43/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5567 - auc: 0.7834 - accuracy: 0.7102 - val_loss: 0.5204 - val_auc: 0.8127 - val_accuracy: 0.7405\n",
      "Epoch 44/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5473 - auc: 0.7911 - accuracy: 0.7239 - val_loss: 0.5180 - val_auc: 0.8147 - val_accuracy: 0.7459\n",
      "Epoch 45/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5301 - auc: 0.7915 - accuracy: 0.7352 - val_loss: 0.5157 - val_auc: 0.8161 - val_accuracy: 0.7459\n",
      "Epoch 46/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5457 - auc: 0.7978 - accuracy: 0.7373 - val_loss: 0.5133 - val_auc: 0.8185 - val_accuracy: 0.7514\n",
      "Epoch 47/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5297 - auc: 0.7976 - accuracy: 0.7269 - val_loss: 0.5112 - val_auc: 0.8204 - val_accuracy: 0.7514\n",
      "Epoch 48/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5545 - auc: 0.7775 - accuracy: 0.7123 - val_loss: 0.5090 - val_auc: 0.8211 - val_accuracy: 0.7514\n",
      "Epoch 49/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5298 - auc: 0.8073 - accuracy: 0.7384 - val_loss: 0.5071 - val_auc: 0.8233 - val_accuracy: 0.7568\n",
      "Epoch 50/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5244 - auc: 0.8023 - accuracy: 0.7454 - val_loss: 0.5049 - val_auc: 0.8265 - val_accuracy: 0.7568\n",
      "Epoch 51/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5129 - auc: 0.8133 - accuracy: 0.7462 - val_loss: 0.5030 - val_auc: 0.8293 - val_accuracy: 0.7622\n",
      "Epoch 52/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5062 - auc: 0.8276 - accuracy: 0.7605 - val_loss: 0.5013 - val_auc: 0.8314 - val_accuracy: 0.7622\n",
      "Epoch 53/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5435 - auc: 0.8039 - accuracy: 0.7323 - val_loss: 0.4994 - val_auc: 0.8317 - val_accuracy: 0.7568\n",
      "Epoch 54/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5253 - auc: 0.8060 - accuracy: 0.7413 - val_loss: 0.4978 - val_auc: 0.8336 - val_accuracy: 0.7568\n",
      "Epoch 55/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5164 - auc: 0.8064 - accuracy: 0.7609 - val_loss: 0.4961 - val_auc: 0.8336 - val_accuracy: 0.7568\n",
      "Epoch 56/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5319 - auc: 0.7935 - accuracy: 0.7409 - val_loss: 0.4944 - val_auc: 0.8353 - val_accuracy: 0.7622\n",
      "Epoch 57/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4895 - auc: 0.8352 - accuracy: 0.7702 - val_loss: 0.4930 - val_auc: 0.8371 - val_accuracy: 0.7622\n",
      "Epoch 58/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5085 - auc: 0.8105 - accuracy: 0.7348 - val_loss: 0.4915 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 59/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5138 - auc: 0.8129 - accuracy: 0.7374 - val_loss: 0.4902 - val_auc: 0.8390 - val_accuracy: 0.7730\n",
      "Epoch 60/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5441 - auc: 0.7663 - accuracy: 0.7157 - val_loss: 0.4889 - val_auc: 0.8393 - val_accuracy: 0.7784\n",
      "Epoch 61/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4849 - auc: 0.8325 - accuracy: 0.7708 - val_loss: 0.4875 - val_auc: 0.8403 - val_accuracy: 0.7784\n",
      "Epoch 62/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4884 - auc: 0.8383 - accuracy: 0.7658 - val_loss: 0.4863 - val_auc: 0.8408 - val_accuracy: 0.7784\n",
      "Epoch 63/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4923 - auc: 0.8465 - accuracy: 0.7733 - val_loss: 0.4851 - val_auc: 0.8411 - val_accuracy: 0.7784\n",
      "Epoch 64/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5352 - auc: 0.8099 - accuracy: 0.7320 - val_loss: 0.4839 - val_auc: 0.8424 - val_accuracy: 0.7784\n",
      "Epoch 65/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5120 - auc: 0.8019 - accuracy: 0.7510 - val_loss: 0.4828 - val_auc: 0.8422 - val_accuracy: 0.7784\n",
      "Epoch 66/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4566 - auc: 0.8634 - accuracy: 0.7876 - val_loss: 0.4817 - val_auc: 0.8428 - val_accuracy: 0.7784\n",
      "Epoch 67/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4948 - auc: 0.8319 - accuracy: 0.7565 - val_loss: 0.4807 - val_auc: 0.8438 - val_accuracy: 0.7892\n",
      "Epoch 68/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4915 - auc: 0.8387 - accuracy: 0.7526 - val_loss: 0.4797 - val_auc: 0.8440 - val_accuracy: 0.7892\n",
      "Epoch 69/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4860 - auc: 0.8331 - accuracy: 0.7732 - val_loss: 0.4787 - val_auc: 0.8444 - val_accuracy: 0.7892\n",
      "Epoch 70/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4975 - auc: 0.8226 - accuracy: 0.7564 - val_loss: 0.4778 - val_auc: 0.8451 - val_accuracy: 0.7892\n",
      "Epoch 71/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8243 - accuracy: 0.7637 - val_loss: 0.4768 - val_auc: 0.8451 - val_accuracy: 0.7892\n",
      "Epoch 72/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4964 - auc: 0.8290 - accuracy: 0.7520 - val_loss: 0.4759 - val_auc: 0.8449 - val_accuracy: 0.7892\n",
      "Epoch 73/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4917 - auc: 0.8216 - accuracy: 0.7586 - val_loss: 0.4750 - val_auc: 0.8457 - val_accuracy: 0.7892\n",
      "Epoch 74/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4743 - auc: 0.8461 - accuracy: 0.7749 - val_loss: 0.4741 - val_auc: 0.8460 - val_accuracy: 0.7892\n",
      "Epoch 75/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5001 - auc: 0.8281 - accuracy: 0.7580 - val_loss: 0.4733 - val_auc: 0.8466 - val_accuracy: 0.7946\n",
      "Epoch 76/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4976 - auc: 0.8258 - accuracy: 0.7489 - val_loss: 0.4725 - val_auc: 0.8473 - val_accuracy: 0.7946\n",
      "Epoch 77/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4768 - auc: 0.8380 - accuracy: 0.7646 - val_loss: 0.4718 - val_auc: 0.8484 - val_accuracy: 0.7946\n",
      "Epoch 78/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4783 - auc: 0.8423 - accuracy: 0.7698 - val_loss: 0.4712 - val_auc: 0.8482 - val_accuracy: 0.7946\n",
      "Epoch 79/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4999 - auc: 0.8284 - accuracy: 0.7516 - val_loss: 0.4704 - val_auc: 0.8491 - val_accuracy: 0.7946\n",
      "Epoch 80/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5328 - auc: 0.8016 - accuracy: 0.7264 - val_loss: 0.4696 - val_auc: 0.8480 - val_accuracy: 0.7892\n",
      "Epoch 81/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4814 - auc: 0.8425 - accuracy: 0.7755 - val_loss: 0.4690 - val_auc: 0.8490 - val_accuracy: 0.7892\n",
      "Epoch 82/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4691 - auc: 0.8530 - accuracy: 0.7758 - val_loss: 0.4683 - val_auc: 0.8497 - val_accuracy: 0.7892\n",
      "Epoch 83/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4689 - auc: 0.8282 - accuracy: 0.7662 - val_loss: 0.4677 - val_auc: 0.8503 - val_accuracy: 0.7946\n",
      "Epoch 84/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4794 - auc: 0.8463 - accuracy: 0.7915 - val_loss: 0.4670 - val_auc: 0.8499 - val_accuracy: 0.7946\n",
      "Epoch 85/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4947 - auc: 0.8294 - accuracy: 0.7518 - val_loss: 0.4664 - val_auc: 0.8507 - val_accuracy: 0.7946\n",
      "Epoch 86/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4873 - auc: 0.8232 - accuracy: 0.7685 - val_loss: 0.4657 - val_auc: 0.8512 - val_accuracy: 0.7946\n",
      "Epoch 87/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4908 - auc: 0.8312 - accuracy: 0.7673 - val_loss: 0.4652 - val_auc: 0.8513 - val_accuracy: 0.7946\n",
      "Epoch 88/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4695 - auc: 0.8423 - accuracy: 0.7791 - val_loss: 0.4646 - val_auc: 0.8521 - val_accuracy: 0.7946\n",
      "Epoch 89/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8633 - accuracy: 0.7879 - val_loss: 0.4641 - val_auc: 0.8527 - val_accuracy: 0.7946\n",
      "Epoch 90/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4521 - auc: 0.8664 - accuracy: 0.7936 - val_loss: 0.4636 - val_auc: 0.8528 - val_accuracy: 0.7946\n",
      "Epoch 91/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4819 - auc: 0.8337 - accuracy: 0.7613 - val_loss: 0.4630 - val_auc: 0.8533 - val_accuracy: 0.7946\n",
      "Epoch 92/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4894 - auc: 0.8236 - accuracy: 0.7629 - val_loss: 0.4626 - val_auc: 0.8535 - val_accuracy: 0.7946\n",
      "Epoch 93/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4928 - auc: 0.8275 - accuracy: 0.7456 - val_loss: 0.4621 - val_auc: 0.8534 - val_accuracy: 0.7946\n",
      "Epoch 94/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4729 - auc: 0.8448 - accuracy: 0.7686 - val_loss: 0.4617 - val_auc: 0.8544 - val_accuracy: 0.7946\n",
      "Epoch 95/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4850 - auc: 0.8338 - accuracy: 0.7554 - val_loss: 0.4612 - val_auc: 0.8551 - val_accuracy: 0.7946\n",
      "Epoch 96/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5030 - auc: 0.8226 - accuracy: 0.7448 - val_loss: 0.4607 - val_auc: 0.8548 - val_accuracy: 0.7946\n",
      "Epoch 97/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5140 - auc: 0.8069 - accuracy: 0.7370 - val_loss: 0.4603 - val_auc: 0.8548 - val_accuracy: 0.7946\n",
      "Epoch 98/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4837 - auc: 0.8308 - accuracy: 0.7657 - val_loss: 0.4599 - val_auc: 0.8553 - val_accuracy: 0.7946\n",
      "Epoch 99/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4917 - auc: 0.8145 - accuracy: 0.7427 - val_loss: 0.4595 - val_auc: 0.8560 - val_accuracy: 0.8000\n",
      "Epoch 100/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4844 - auc: 0.8225 - accuracy: 0.7644 - val_loss: 0.4591 - val_auc: 0.8558 - val_accuracy: 0.8000\n",
      "Epoch 101/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4950 - auc: 0.8361 - accuracy: 0.7625 - val_loss: 0.4588 - val_auc: 0.8564 - val_accuracy: 0.8000\n",
      "Epoch 102/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4826 - auc: 0.8400 - accuracy: 0.7482 - val_loss: 0.4584 - val_auc: 0.8566 - val_accuracy: 0.8000\n",
      "Epoch 103/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5100 - auc: 0.8296 - accuracy: 0.7439 - val_loss: 0.4580 - val_auc: 0.8569 - val_accuracy: 0.8000\n",
      "Epoch 104/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4667 - auc: 0.8391 - accuracy: 0.7754 - val_loss: 0.4577 - val_auc: 0.8565 - val_accuracy: 0.8000\n",
      "Epoch 105/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4762 - auc: 0.8435 - accuracy: 0.7559 - val_loss: 0.4575 - val_auc: 0.8570 - val_accuracy: 0.8000\n",
      "Epoch 106/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8532 - accuracy: 0.7820 - val_loss: 0.4572 - val_auc: 0.8580 - val_accuracy: 0.8000\n",
      "Epoch 107/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4735 - auc: 0.8427 - accuracy: 0.7683 - val_loss: 0.4568 - val_auc: 0.8586 - val_accuracy: 0.8000\n",
      "Epoch 108/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4781 - auc: 0.8379 - accuracy: 0.7820 - val_loss: 0.4566 - val_auc: 0.8587 - val_accuracy: 0.8000\n",
      "Epoch 109/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4681 - auc: 0.8481 - accuracy: 0.7681 - val_loss: 0.4563 - val_auc: 0.8588 - val_accuracy: 0.8000\n",
      "Epoch 110/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8528 - accuracy: 0.7835 - val_loss: 0.4560 - val_auc: 0.8593 - val_accuracy: 0.8000\n",
      "Epoch 111/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4761 - auc: 0.8406 - accuracy: 0.7699 - val_loss: 0.4558 - val_auc: 0.8596 - val_accuracy: 0.8000\n",
      "Epoch 112/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5043 - auc: 0.8254 - accuracy: 0.7627 - val_loss: 0.4555 - val_auc: 0.8600 - val_accuracy: 0.8000\n",
      "Epoch 113/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4742 - auc: 0.8399 - accuracy: 0.7582 - val_loss: 0.4554 - val_auc: 0.8602 - val_accuracy: 0.8000\n",
      "Epoch 114/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4929 - auc: 0.8076 - accuracy: 0.7536 - val_loss: 0.4552 - val_auc: 0.8603 - val_accuracy: 0.8000\n",
      "Epoch 115/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4724 - auc: 0.8317 - accuracy: 0.7620 - val_loss: 0.4550 - val_auc: 0.8609 - val_accuracy: 0.8000\n",
      "Epoch 116/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5018 - auc: 0.8098 - accuracy: 0.7447 - val_loss: 0.4547 - val_auc: 0.8609 - val_accuracy: 0.8000\n",
      "Epoch 117/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5033 - auc: 0.8102 - accuracy: 0.7311 - val_loss: 0.4546 - val_auc: 0.8610 - val_accuracy: 0.8000\n",
      "Epoch 118/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4631 - auc: 0.8423 - accuracy: 0.7534 - val_loss: 0.4544 - val_auc: 0.8612 - val_accuracy: 0.8000\n",
      "Epoch 119/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4637 - auc: 0.8450 - accuracy: 0.7906 - val_loss: 0.4542 - val_auc: 0.8610 - val_accuracy: 0.8000\n",
      "Epoch 120/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4453 - auc: 0.8623 - accuracy: 0.7883 - val_loss: 0.4540 - val_auc: 0.8610 - val_accuracy: 0.8000\n",
      "Epoch 121/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4899 - auc: 0.8238 - accuracy: 0.7493 - val_loss: 0.4539 - val_auc: 0.8605 - val_accuracy: 0.8000\n",
      "Epoch 122/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4832 - auc: 0.8402 - accuracy: 0.7736 - val_loss: 0.4536 - val_auc: 0.8610 - val_accuracy: 0.8000\n",
      "Epoch 123/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4723 - auc: 0.8445 - accuracy: 0.7766 - val_loss: 0.4534 - val_auc: 0.8612 - val_accuracy: 0.8000\n",
      "Epoch 124/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4650 - auc: 0.8526 - accuracy: 0.7930 - val_loss: 0.4533 - val_auc: 0.8610 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219d11a3340>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "es_model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/ES/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "female-accreditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5065 - auc: 0.7978 - accuracy: 0.7338\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test subset and predict.\n",
    "es_model = load_model(mc_path)\n",
    "eval = es_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-andrews",
   "metadata": {},
   "source": [
    "# 9. Learning Rate Scheduling\n",
    "En este apartado se prueba la opción de Learning Rate Scheduling. Esta se encarga de aplicarle una función al Learning Rate entre epochs, de forma tal de encontrar el mínimo de la loss de forma más rápida, y apuntando a evitar mínimos locales y, por ende, overfitting. Se sigue aplicando el concepto de **early stopping** para la AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "motivated-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay, PolynomialDecay # API in https://keras.io/api/optimizers/learning_rate_schedules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dietary-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/lrs_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "deadly-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new model\n",
    "lrs_model = Sequential()\n",
    "lrs_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "tutorial-johnston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 159ms/step - loss: 0.9938 - auc: 0.2552 - accuracy: 0.3290 - val_loss: 0.7441 - val_auc: 0.4607 - val_accuracy: 0.5514\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6979 - auc: 0.5124 - accuracy: 0.5593 - val_loss: 0.6027 - val_auc: 0.7179 - val_accuracy: 0.6757\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5710 - auc: 0.7635 - accuracy: 0.7173 - val_loss: 0.5454 - val_auc: 0.7941 - val_accuracy: 0.7189\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5551 - auc: 0.7868 - accuracy: 0.6922 - val_loss: 0.5126 - val_auc: 0.8244 - val_accuracy: 0.7351\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5182 - auc: 0.7988 - accuracy: 0.7493 - val_loss: 0.4918 - val_auc: 0.8408 - val_accuracy: 0.7622\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4924 - auc: 0.8371 - accuracy: 0.7641 - val_loss: 0.4780 - val_auc: 0.8522 - val_accuracy: 0.7622\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4815 - auc: 0.8278 - accuracy: 0.7775 - val_loss: 0.4690 - val_auc: 0.8570 - val_accuracy: 0.7676\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8513 - accuracy: 0.7825 - val_loss: 0.4622 - val_auc: 0.8606 - val_accuracy: 0.7730\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4926 - auc: 0.8192 - accuracy: 0.7496 - val_loss: 0.4578 - val_auc: 0.8622 - val_accuracy: 0.7784\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4628 - auc: 0.8481 - accuracy: 0.7923 - val_loss: 0.4550 - val_auc: 0.8646 - val_accuracy: 0.7892\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4666 - auc: 0.8378 - accuracy: 0.7762 - val_loss: 0.4526 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4755 - auc: 0.8339 - accuracy: 0.7664 - val_loss: 0.4510 - val_auc: 0.8651 - val_accuracy: 0.7946\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4690 - auc: 0.8395 - accuracy: 0.7609 - val_loss: 0.4500 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8494 - accuracy: 0.7652 - val_loss: 0.4492 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4765 - auc: 0.8401 - accuracy: 0.7692 - val_loss: 0.4481 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4863 - auc: 0.8386 - accuracy: 0.7719 - val_loss: 0.4475 - val_auc: 0.8663 - val_accuracy: 0.8000\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4568 - auc: 0.8497 - accuracy: 0.7573 - val_loss: 0.4480 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5024 - auc: 0.8139 - accuracy: 0.7378 - val_loss: 0.4471 - val_auc: 0.8666 - val_accuracy: 0.7892\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4512 - auc: 0.8570 - accuracy: 0.7781 - val_loss: 0.4466 - val_auc: 0.8671 - val_accuracy: 0.7892\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4782 - auc: 0.8377 - accuracy: 0.7862 - val_loss: 0.4463 - val_auc: 0.8668 - val_accuracy: 0.7946\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4769 - auc: 0.8304 - accuracy: 0.7600 - val_loss: 0.4457 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4915 - auc: 0.8377 - accuracy: 0.7445 - val_loss: 0.4459 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4837 - auc: 0.8360 - accuracy: 0.7598 - val_loss: 0.4459 - val_auc: 0.8663 - val_accuracy: 0.7892\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4573 - auc: 0.8296 - accuracy: 0.7916 - val_loss: 0.4459 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4972 - auc: 0.8109 - accuracy: 0.7345 - val_loss: 0.4456 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4841 - auc: 0.8263 - accuracy: 0.7805 - val_loss: 0.4459 - val_auc: 0.8660 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219d2855ca0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define learning rate at start\n",
    "ilr = 0.1\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=100000, decay_rate=0.96, staircase=False) # Decay every (decay_steps) steps with a base of (decay_rate).\n",
    "# Compiling model\n",
    "lrs_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "connected-violence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5024 - auc: 0.8021 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-southeast",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Exponential Decay se lleva bien con Early Stopping?, ya que si reduzco el learning rate \"me muevo menos\", con lo cual el callback de Early Stopping cortaría prematuramente. Ahora probamos sin Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "growing-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 93ms/step - loss: 0.4734 - auc: 0.8390 - accuracy: 0.7669 - val_loss: 0.4479 - val_auc: 0.8664 - val_accuracy: 0.7892\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4726 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4471 - val_auc: 0.8666 - val_accuracy: 0.7892\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8387 - accuracy: 0.7692 - val_loss: 0.4466 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4724 - auc: 0.8389 - accuracy: 0.7692 - val_loss: 0.4471 - val_auc: 0.8671 - val_accuracy: 0.7892\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4725 - auc: 0.8387 - accuracy: 0.7739 - val_loss: 0.4465 - val_auc: 0.8667 - val_accuracy: 0.7892\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8396 - accuracy: 0.7739 - val_loss: 0.4467 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8395 - accuracy: 0.7739 - val_loss: 0.4465 - val_auc: 0.8662 - val_accuracy: 0.7946\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4461 - val_auc: 0.8660 - val_accuracy: 0.7838\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8399 - accuracy: 0.7762 - val_loss: 0.4466 - val_auc: 0.8658 - val_accuracy: 0.7946\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8400 - accuracy: 0.7692 - val_loss: 0.4462 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4718 - auc: 0.8388 - accuracy: 0.7762 - val_loss: 0.4458 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4710 - auc: 0.8394 - accuracy: 0.7786 - val_loss: 0.4460 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8397 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4460 - val_auc: 0.8655 - val_accuracy: 0.7838\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4459 - val_auc: 0.8658 - val_accuracy: 0.7838\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8396 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8384 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8396 - accuracy: 0.7716 - val_loss: 0.4457 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7739 - val_loss: 0.4459 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8389 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8391 - accuracy: 0.7692 - val_loss: 0.4459 - val_auc: 0.8650 - val_accuracy: 0.7838\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8394 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8388 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8644 - val_accuracy: 0.7892\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8389 - accuracy: 0.7716 - val_loss: 0.4459 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8393 - accuracy: 0.7786 - val_loss: 0.4460 - val_auc: 0.8649 - val_accuracy: 0.7784\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4461 - val_auc: 0.8644 - val_accuracy: 0.7838\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8392 - accuracy: 0.7692 - val_loss: 0.4458 - val_auc: 0.8643 - val_accuracy: 0.7838\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4458 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8391 - accuracy: 0.7786 - val_loss: 0.4460 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8399 - accuracy: 0.7809 - val_loss: 0.4461 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8390 - accuracy: 0.7739 - val_loss: 0.4466 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4473 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4470 - val_auc: 0.8656 - val_accuracy: 0.7946\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4467 - val_auc: 0.8650 - val_accuracy: 0.7946\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4465 - val_auc: 0.8653 - val_accuracy: 0.7946\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8402 - accuracy: 0.7762 - val_loss: 0.4465 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7809 - val_loss: 0.4462 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8387 - accuracy: 0.7716 - val_loss: 0.4458 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8382 - accuracy: 0.7692 - val_loss: 0.4454 - val_auc: 0.8641 - val_accuracy: 0.7892\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7762 - val_loss: 0.4453 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8390 - accuracy: 0.7739 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8388 - accuracy: 0.7739 - val_loss: 0.4463 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8396 - accuracy: 0.7786 - val_loss: 0.4462 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4462 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4466 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4463 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8388 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8384 - accuracy: 0.7739 - val_loss: 0.4463 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4463 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4463 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4461 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4460 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8390 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4457 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8396 - accuracy: 0.7762 - val_loss: 0.4461 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8394 - accuracy: 0.7716 - val_loss: 0.4458 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4458 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4716 - auc: 0.8387 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8645 - val_accuracy: 0.7838\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8652 - val_accuracy: 0.7838\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4714 - auc: 0.8391 - accuracy: 0.7786 - val_loss: 0.4459 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8398 - accuracy: 0.7739 - val_loss: 0.4460 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8389 - accuracy: 0.7692 - val_loss: 0.4458 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8395 - accuracy: 0.7762 - val_loss: 0.4454 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8395 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8654 - val_accuracy: 0.7838\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4449 - val_auc: 0.8655 - val_accuracy: 0.7838\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4450 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8395 - accuracy: 0.7739 - val_loss: 0.4451 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8396 - accuracy: 0.7739 - val_loss: 0.4453 - val_auc: 0.8650 - val_accuracy: 0.7784\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8398 - accuracy: 0.7739 - val_loss: 0.4454 - val_auc: 0.8650 - val_accuracy: 0.7784\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8400 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4459 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8392 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8395 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8385 - accuracy: 0.7762 - val_loss: 0.4453 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8393 - accuracy: 0.7786 - val_loss: 0.4454 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4707 - auc: 0.8396 - accuracy: 0.7762 - val_loss: 0.4451 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4455 - val_auc: 0.8646 - val_accuracy: 0.7892\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8393 - accuracy: 0.7786 - val_loss: 0.4457 - val_auc: 0.8645 - val_accuracy: 0.7892\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7739 - val_loss: 0.4455 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8397 - accuracy: 0.7762 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4457 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7786 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7946\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4715 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4458 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8395 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8645 - val_accuracy: 0.7946\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4465 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8384 - accuracy: 0.7669 - val_loss: 0.4466 - val_auc: 0.8659 - val_accuracy: 0.7838\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8395 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8389 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8647 - val_accuracy: 0.7892\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8652 - val_accuracy: 0.7838\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8393 - accuracy: 0.7716 - val_loss: 0.4463 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4459 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4459 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7786 - val_loss: 0.4459 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7809 - val_loss: 0.4457 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8388 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4467 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8385 - accuracy: 0.7716 - val_loss: 0.4473 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8379 - accuracy: 0.7762 - val_loss: 0.4467 - val_auc: 0.8655 - val_accuracy: 0.7838\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8385 - accuracy: 0.7762 - val_loss: 0.4463 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8380 - accuracy: 0.7786 - val_loss: 0.4466 - val_auc: 0.8655 - val_accuracy: 0.7838\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8388 - accuracy: 0.7762 - val_loss: 0.4471 - val_auc: 0.8662 - val_accuracy: 0.7838\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8385 - accuracy: 0.7762 - val_loss: 0.4473 - val_auc: 0.8660 - val_accuracy: 0.7838\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8385 - accuracy: 0.7762 - val_loss: 0.4476 - val_auc: 0.8651 - val_accuracy: 0.7946\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4725 - auc: 0.8389 - accuracy: 0.7809 - val_loss: 0.4469 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4470 - val_auc: 0.8659 - val_accuracy: 0.7892\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8387 - accuracy: 0.7716 - val_loss: 0.4466 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4471 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8384 - accuracy: 0.7786 - val_loss: 0.4464 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8382 - accuracy: 0.7716 - val_loss: 0.4462 - val_auc: 0.8661 - val_accuracy: 0.7892\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8395 - accuracy: 0.7762 - val_loss: 0.4464 - val_auc: 0.8654 - val_accuracy: 0.7838\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4466 - val_auc: 0.8662 - val_accuracy: 0.7838\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8388 - accuracy: 0.7786 - val_loss: 0.4468 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8653 - val_accuracy: 0.7838\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4464 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8390 - accuracy: 0.7739 - val_loss: 0.4463 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8388 - accuracy: 0.7716 - val_loss: 0.4461 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4465 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8389 - accuracy: 0.7739 - val_loss: 0.4462 - val_auc: 0.8655 - val_accuracy: 0.7892\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8385 - accuracy: 0.7739 - val_loss: 0.4458 - val_auc: 0.8653 - val_accuracy: 0.7946\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8398 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8395 - accuracy: 0.7809 - val_loss: 0.4458 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8395 - accuracy: 0.7809 - val_loss: 0.4463 - val_auc: 0.8665 - val_accuracy: 0.7892\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4461 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8389 - accuracy: 0.7716 - val_loss: 0.4457 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4454 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8390 - accuracy: 0.7739 - val_loss: 0.4454 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8389 - accuracy: 0.7762 - val_loss: 0.4451 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4452 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4450 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4449 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8401 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8399 - accuracy: 0.7716 - val_loss: 0.4452 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8397 - accuracy: 0.7762 - val_loss: 0.4450 - val_auc: 0.8650 - val_accuracy: 0.7892\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8395 - accuracy: 0.7762 - val_loss: 0.4449 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8398 - accuracy: 0.7786 - val_loss: 0.4450 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4724 - auc: 0.8385 - accuracy: 0.7716 - val_loss: 0.4451 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4454 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4452 - val_auc: 0.8659 - val_accuracy: 0.7946\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8396 - accuracy: 0.7786 - val_loss: 0.4457 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8399 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8387 - accuracy: 0.7809 - val_loss: 0.4458 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8391 - accuracy: 0.7669 - val_loss: 0.4454 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8400 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8389 - accuracy: 0.7716 - val_loss: 0.4464 - val_auc: 0.8649 - val_accuracy: 0.7838\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8393 - accuracy: 0.7786 - val_loss: 0.4465 - val_auc: 0.8651 - val_accuracy: 0.7892\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4465 - val_auc: 0.8652 - val_accuracy: 0.7838\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8389 - accuracy: 0.7692 - val_loss: 0.4463 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8389 - accuracy: 0.7739 - val_loss: 0.4464 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8390 - accuracy: 0.7739 - val_loss: 0.4465 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8396 - accuracy: 0.7739 - val_loss: 0.4464 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8392 - accuracy: 0.7739 - val_loss: 0.4469 - val_auc: 0.8649 - val_accuracy: 0.8000\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4460 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8393 - accuracy: 0.7739 - val_loss: 0.4460 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4458 - val_auc: 0.8643 - val_accuracy: 0.7892\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8393 - accuracy: 0.7786 - val_loss: 0.4460 - val_auc: 0.8643 - val_accuracy: 0.7946\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8396 - accuracy: 0.7786 - val_loss: 0.4460 - val_auc: 0.8647 - val_accuracy: 0.7946\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4457 - val_auc: 0.8651 - val_accuracy: 0.7946\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4456 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8391 - accuracy: 0.7762 - val_loss: 0.4461 - val_auc: 0.8654 - val_accuracy: 0.7946\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8392 - accuracy: 0.7692 - val_loss: 0.4467 - val_auc: 0.8647 - val_accuracy: 0.7946\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8391 - accuracy: 0.7692 - val_loss: 0.4482 - val_auc: 0.8641 - val_accuracy: 0.7946\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8390 - accuracy: 0.7692 - val_loss: 0.4476 - val_auc: 0.8648 - val_accuracy: 0.8000\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4718 - auc: 0.8386 - accuracy: 0.7692 - val_loss: 0.4475 - val_auc: 0.8646 - val_accuracy: 0.7946\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4709 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4467 - val_auc: 0.8650 - val_accuracy: 0.7892\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8397 - accuracy: 0.7762 - val_loss: 0.4464 - val_auc: 0.8643 - val_accuracy: 0.7946\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8385 - accuracy: 0.7786 - val_loss: 0.4462 - val_auc: 0.8646 - val_accuracy: 0.7892\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4460 - val_auc: 0.8646 - val_accuracy: 0.7838\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8392 - accuracy: 0.7786 - val_loss: 0.4456 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8394 - accuracy: 0.7762 - val_loss: 0.4452 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8397 - accuracy: 0.7786 - val_loss: 0.4451 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8399 - accuracy: 0.7739 - val_loss: 0.4456 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8395 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8648 - val_accuracy: 0.7838\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4719 - auc: 0.8390 - accuracy: 0.7692 - val_loss: 0.4454 - val_auc: 0.8656 - val_accuracy: 0.7838\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8389 - accuracy: 0.7786 - val_loss: 0.4456 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4715 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4455 - val_auc: 0.8658 - val_accuracy: 0.7838\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8386 - accuracy: 0.7692 - val_loss: 0.4457 - val_auc: 0.8659 - val_accuracy: 0.7838\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8397 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8662 - val_accuracy: 0.7784\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4717 - auc: 0.8397 - accuracy: 0.7739 - val_loss: 0.4454 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4708 - auc: 0.8398 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8384 - accuracy: 0.7716 - val_loss: 0.4450 - val_auc: 0.8666 - val_accuracy: 0.7892\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8390 - accuracy: 0.7716 - val_loss: 0.4453 - val_auc: 0.8654 - val_accuracy: 0.7838\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8403 - accuracy: 0.7716 - val_loss: 0.4453 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8395 - accuracy: 0.7739 - val_loss: 0.4455 - val_auc: 0.8656 - val_accuracy: 0.7892\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8395 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8391 - accuracy: 0.7692 - val_loss: 0.4455 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8401 - accuracy: 0.7716 - val_loss: 0.4454 - val_auc: 0.8657 - val_accuracy: 0.7892\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8399 - accuracy: 0.7692 - val_loss: 0.4457 - val_auc: 0.8652 - val_accuracy: 0.7892\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8395 - accuracy: 0.7716 - val_loss: 0.4456 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4710 - auc: 0.8391 - accuracy: 0.7716 - val_loss: 0.4453 - val_auc: 0.8654 - val_accuracy: 0.7892\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8396 - accuracy: 0.7786 - val_loss: 0.4451 - val_auc: 0.8658 - val_accuracy: 0.7892\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8394 - accuracy: 0.7786 - val_loss: 0.4454 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8393 - accuracy: 0.7762 - val_loss: 0.4454 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8392 - accuracy: 0.7762 - val_loss: 0.4455 - val_auc: 0.8654 - val_accuracy: 0.7946\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8394 - accuracy: 0.7739 - val_loss: 0.4452 - val_auc: 0.8660 - val_accuracy: 0.7892\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4712 - auc: 0.8399 - accuracy: 0.7762 - val_loss: 0.4452 - val_auc: 0.8654 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219d333b070>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "brazilian-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5112 - auc: 0.7995 - accuracy: 0.7338\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-society",
   "metadata": {},
   "source": [
    "# 10. Regularización\n",
    "La idea de la regularización es la de limitar aquellos pesos que son altos. De esta forma, se agrega una capa previa a la capa densa que contiene la capa de regularización. Se probarán dos regularizaciones distintas: L1 y L2 (donde el número significa el grado del término adicional que se suma a la función de costo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-chase",
   "metadata": {},
   "source": [
    "# 10.1. Regularización L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "impossible-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "partial-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "completed-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 4s 173ms/step - loss: 0.7716 - auc: 0.6757 - accuracy: 0.6365 - val_loss: 0.6382 - val_auc: 0.7541 - val_accuracy: 0.6919\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6387 - auc: 0.7381 - accuracy: 0.7076 - val_loss: 0.5665 - val_auc: 0.8029 - val_accuracy: 0.7568\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5820 - auc: 0.7744 - accuracy: 0.7162 - val_loss: 0.5237 - val_auc: 0.8263 - val_accuracy: 0.7784\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5302 - auc: 0.8178 - accuracy: 0.7549 - val_loss: 0.4979 - val_auc: 0.8396 - val_accuracy: 0.7892\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4993 - auc: 0.8339 - accuracy: 0.7588 - val_loss: 0.4813 - val_auc: 0.8482 - val_accuracy: 0.7892\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4859 - auc: 0.8383 - accuracy: 0.7602 - val_loss: 0.4702 - val_auc: 0.8556 - val_accuracy: 0.7730\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4875 - auc: 0.8383 - accuracy: 0.7649 - val_loss: 0.4635 - val_auc: 0.8592 - val_accuracy: 0.7730\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4974 - auc: 0.8087 - accuracy: 0.7421 - val_loss: 0.4591 - val_auc: 0.8614 - val_accuracy: 0.7730\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4901 - auc: 0.8233 - accuracy: 0.7542 - val_loss: 0.4564 - val_auc: 0.8640 - val_accuracy: 0.7784\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4750 - auc: 0.8406 - accuracy: 0.7744 - val_loss: 0.4543 - val_auc: 0.8651 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4550 - auc: 0.8581 - accuracy: 0.7702 - val_loss: 0.4531 - val_auc: 0.8644 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5187 - auc: 0.8019 - accuracy: 0.7314 - val_loss: 0.4515 - val_auc: 0.8646 - val_accuracy: 0.7730\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4946 - auc: 0.8155 - accuracy: 0.7580 - val_loss: 0.4507 - val_auc: 0.8653 - val_accuracy: 0.7784\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4728 - auc: 0.8355 - accuracy: 0.7610 - val_loss: 0.4504 - val_auc: 0.8653 - val_accuracy: 0.7784\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4841 - auc: 0.8159 - accuracy: 0.7681 - val_loss: 0.4503 - val_auc: 0.8651 - val_accuracy: 0.7838\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5104 - auc: 0.8153 - accuracy: 0.7559 - val_loss: 0.4503 - val_auc: 0.8654 - val_accuracy: 0.7784\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4897 - auc: 0.8332 - accuracy: 0.7585 - val_loss: 0.4497 - val_auc: 0.8658 - val_accuracy: 0.7784\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4667 - auc: 0.8441 - accuracy: 0.7718 - val_loss: 0.4494 - val_auc: 0.8658 - val_accuracy: 0.7730\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5000 - auc: 0.8179 - accuracy: 0.7433 - val_loss: 0.4489 - val_auc: 0.8660 - val_accuracy: 0.7730\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4990 - auc: 0.8080 - accuracy: 0.7497 - val_loss: 0.4491 - val_auc: 0.8665 - val_accuracy: 0.7730\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5035 - auc: 0.8230 - accuracy: 0.7571 - val_loss: 0.4492 - val_auc: 0.8661 - val_accuracy: 0.7730\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4799 - auc: 0.8513 - accuracy: 0.7743 - val_loss: 0.4490 - val_auc: 0.8661 - val_accuracy: 0.7784\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4355 - auc: 0.8564 - accuracy: 0.8162 - val_loss: 0.4499 - val_auc: 0.8656 - val_accuracy: 0.7784\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4503 - auc: 0.8559 - accuracy: 0.7852 - val_loss: 0.4499 - val_auc: 0.8661 - val_accuracy: 0.7784\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4426 - auc: 0.8621 - accuracy: 0.8004 - val_loss: 0.4495 - val_auc: 0.8665 - val_accuracy: 0.7838\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4850 - auc: 0.8412 - accuracy: 0.7650 - val_loss: 0.4490 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4911 - auc: 0.8276 - accuracy: 0.7597 - val_loss: 0.4492 - val_auc: 0.8665 - val_accuracy: 0.7892\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4360 - auc: 0.8633 - accuracy: 0.7912 - val_loss: 0.4493 - val_auc: 0.8661 - val_accuracy: 0.7838\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4541 - auc: 0.8512 - accuracy: 0.7752 - val_loss: 0.4498 - val_auc: 0.8644 - val_accuracy: 0.7892\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4720 - auc: 0.8497 - accuracy: 0.7753 - val_loss: 0.4496 - val_auc: 0.8655 - val_accuracy: 0.7838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219d2ee6df0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 Regularization\n",
    "l1_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l1(0.01)))\n",
    "# Compiling model\n",
    "l1_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "split-spice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5090 - auc: 0.8005 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "l1_model = load_model(mc_path)\n",
    "eval = l1_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-shopping",
   "metadata": {},
   "source": [
    "# 10.2. Regularización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "gothic-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "posted-exclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 147ms/step - loss: 0.7111 - auc: 0.7003 - accuracy: 0.6530 - val_loss: 0.6216 - val_auc: 0.7423 - val_accuracy: 0.6703\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6130 - auc: 0.7401 - accuracy: 0.7008 - val_loss: 0.5660 - val_auc: 0.7803 - val_accuracy: 0.7027\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5836 - auc: 0.7531 - accuracy: 0.7002 - val_loss: 0.5295 - val_auc: 0.8067 - val_accuracy: 0.7351\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5459 - auc: 0.7912 - accuracy: 0.7328 - val_loss: 0.5070 - val_auc: 0.8255 - val_accuracy: 0.7351\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5489 - auc: 0.7747 - accuracy: 0.7203 - val_loss: 0.4909 - val_auc: 0.8359 - val_accuracy: 0.7568\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5115 - auc: 0.8153 - accuracy: 0.7604 - val_loss: 0.4797 - val_auc: 0.8461 - val_accuracy: 0.7784\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4879 - auc: 0.8374 - accuracy: 0.7796 - val_loss: 0.4719 - val_auc: 0.8498 - val_accuracy: 0.7946\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5096 - auc: 0.8131 - accuracy: 0.7240 - val_loss: 0.4658 - val_auc: 0.8558 - val_accuracy: 0.7946\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4873 - auc: 0.8205 - accuracy: 0.7475 - val_loss: 0.4626 - val_auc: 0.8572 - val_accuracy: 0.8000\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4678 - auc: 0.8363 - accuracy: 0.7885 - val_loss: 0.4592 - val_auc: 0.8594 - val_accuracy: 0.8000\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4654 - auc: 0.8375 - accuracy: 0.7797 - val_loss: 0.4577 - val_auc: 0.8614 - val_accuracy: 0.8000\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4914 - auc: 0.8206 - accuracy: 0.7574 - val_loss: 0.4556 - val_auc: 0.8621 - val_accuracy: 0.8000\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4427 - auc: 0.8582 - accuracy: 0.7890 - val_loss: 0.4542 - val_auc: 0.8629 - val_accuracy: 0.8000\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4691 - auc: 0.8433 - accuracy: 0.7793 - val_loss: 0.4527 - val_auc: 0.8643 - val_accuracy: 0.8108\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4954 - auc: 0.8377 - accuracy: 0.7705 - val_loss: 0.4516 - val_auc: 0.8643 - val_accuracy: 0.8108\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4666 - auc: 0.8392 - accuracy: 0.7879 - val_loss: 0.4510 - val_auc: 0.8648 - val_accuracy: 0.8108\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4933 - auc: 0.8377 - accuracy: 0.7614 - val_loss: 0.4502 - val_auc: 0.8653 - val_accuracy: 0.8108\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4851 - auc: 0.8198 - accuracy: 0.7600 - val_loss: 0.4498 - val_auc: 0.8639 - val_accuracy: 0.8054\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4772 - auc: 0.8295 - accuracy: 0.7703 - val_loss: 0.4501 - val_auc: 0.8641 - val_accuracy: 0.7946\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4326 - auc: 0.8732 - accuracy: 0.7940 - val_loss: 0.4498 - val_auc: 0.8658 - val_accuracy: 0.8000\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4415 - auc: 0.8605 - accuracy: 0.7914 - val_loss: 0.4498 - val_auc: 0.8648 - val_accuracy: 0.8000\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4751 - auc: 0.8384 - accuracy: 0.7802 - val_loss: 0.4496 - val_auc: 0.8656 - val_accuracy: 0.8054\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4823 - auc: 0.8346 - accuracy: 0.7684 - val_loss: 0.4498 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4713 - auc: 0.8472 - accuracy: 0.7767 - val_loss: 0.4495 - val_auc: 0.8649 - val_accuracy: 0.8054\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4702 - auc: 0.8556 - accuracy: 0.7807 - val_loss: 0.4491 - val_auc: 0.8653 - val_accuracy: 0.7946\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4417 - auc: 0.8624 - accuracy: 0.7932 - val_loss: 0.4492 - val_auc: 0.8648 - val_accuracy: 0.7946\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4863 - auc: 0.8249 - accuracy: 0.7568 - val_loss: 0.4491 - val_auc: 0.8649 - val_accuracy: 0.7946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219d365a130>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L2 Regularization\n",
    "l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "broadband-liabilities",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5085 - auc: 0.8010 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "l2_model = load_model(mc_path)\n",
    "eval = l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-soundtrack",
   "metadata": {},
   "source": [
    "# 10.3. Regularización L1+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "brilliant-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1+L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "naval-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 130ms/step - loss: 0.7193 - auc: 0.5915 - accuracy: 0.5560 - val_loss: 0.6379 - val_auc: 0.6818 - val_accuracy: 0.5892\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5859 - auc: 0.7314 - accuracy: 0.6786 - val_loss: 0.5574 - val_auc: 0.7898 - val_accuracy: 0.7081\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5103 - auc: 0.8162 - accuracy: 0.7377 - val_loss: 0.5181 - val_auc: 0.8259 - val_accuracy: 0.7351\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5313 - auc: 0.8028 - accuracy: 0.7140 - val_loss: 0.4973 - val_auc: 0.8398 - val_accuracy: 0.7514\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5306 - auc: 0.8055 - accuracy: 0.7297 - val_loss: 0.4829 - val_auc: 0.8472 - val_accuracy: 0.7622\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5176 - auc: 0.8056 - accuracy: 0.7342 - val_loss: 0.4735 - val_auc: 0.8515 - val_accuracy: 0.7730\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4923 - auc: 0.8389 - accuracy: 0.7677 - val_loss: 0.4678 - val_auc: 0.8553 - val_accuracy: 0.7730\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4932 - auc: 0.8159 - accuracy: 0.7677 - val_loss: 0.4631 - val_auc: 0.8580 - val_accuracy: 0.7730\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5008 - auc: 0.8194 - accuracy: 0.7486 - val_loss: 0.4599 - val_auc: 0.8584 - val_accuracy: 0.7730\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4905 - auc: 0.8182 - accuracy: 0.7527 - val_loss: 0.4571 - val_auc: 0.8595 - val_accuracy: 0.7730\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8347 - accuracy: 0.7834 - val_loss: 0.4557 - val_auc: 0.8598 - val_accuracy: 0.7784\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4755 - auc: 0.8330 - accuracy: 0.7738 - val_loss: 0.4545 - val_auc: 0.8603 - val_accuracy: 0.7838\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4804 - auc: 0.8345 - accuracy: 0.7575 - val_loss: 0.4529 - val_auc: 0.8614 - val_accuracy: 0.7838\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5202 - auc: 0.8015 - accuracy: 0.7367 - val_loss: 0.4522 - val_auc: 0.8618 - val_accuracy: 0.7784\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4531 - auc: 0.8508 - accuracy: 0.7684 - val_loss: 0.4512 - val_auc: 0.8623 - val_accuracy: 0.7784\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4937 - auc: 0.8319 - accuracy: 0.7792 - val_loss: 0.4505 - val_auc: 0.8632 - val_accuracy: 0.7838\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4836 - auc: 0.8323 - accuracy: 0.7546 - val_loss: 0.4496 - val_auc: 0.8631 - val_accuracy: 0.7892\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4879 - auc: 0.8311 - accuracy: 0.7677 - val_loss: 0.4495 - val_auc: 0.8644 - val_accuracy: 0.7892\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4592 - auc: 0.8423 - accuracy: 0.7830 - val_loss: 0.4489 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4841 - auc: 0.8423 - accuracy: 0.7794 - val_loss: 0.4492 - val_auc: 0.8653 - val_accuracy: 0.7892\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4547 - auc: 0.8559 - accuracy: 0.7905 - val_loss: 0.4488 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4956 - auc: 0.8067 - accuracy: 0.7603 - val_loss: 0.4490 - val_auc: 0.8642 - val_accuracy: 0.7892\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4523 - auc: 0.8543 - accuracy: 0.7882 - val_loss: 0.4490 - val_auc: 0.8637 - val_accuracy: 0.7892\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4294 - auc: 0.8746 - accuracy: 0.8212 - val_loss: 0.4485 - val_auc: 0.8644 - val_accuracy: 0.7892\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8355 - accuracy: 0.7761 - val_loss: 0.4487 - val_auc: 0.8646 - val_accuracy: 0.7892\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4586 - auc: 0.8472 - accuracy: 0.7864 - val_loss: 0.4487 - val_auc: 0.8648 - val_accuracy: 0.7892\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8247 - accuracy: 0.7791 - val_loss: 0.4489 - val_auc: 0.8649 - val_accuracy: 0.7892\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4363 - auc: 0.8635 - accuracy: 0.8122 - val_loss: 0.4486 - val_auc: 0.8651 - val_accuracy: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219d51ab0a0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new model for L1 and L2 Regularization\n",
    "l1l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l1l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l1l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l1l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "filled-profit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5119 - auc: 0.7965 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "l1l2_model = load_model(mc_path)\n",
    "eval = l1l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-rabbit",
   "metadata": {},
   "source": [
    "En este caso, se nota una leve mejora en la métrica empleando regularización L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-share",
   "metadata": {},
   "source": [
    "# 11. Dropout\n",
    "Se emplea una capa extra de dropout para minimizar el overfitting. Este regularizador funciona ignorando a neuronas de forma aleatoria. Se realiza dropout **solo en la etapa de entrenamiento**. **En teoría, no se lleva muy bien con la normalización por capas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "conscious-singapore",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "saved-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/dropout_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "parallel-repeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4466 - auc: 0.8665 - accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4466334879398346, 0.8664530515670776, 0.800000011920929]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating model\n",
    "do_model = Sequential()\n",
    "# Adding dropout layer to network\n",
    "do_model.add(Dropout(0))\n",
    "# Adding Dense layer\n",
    "do_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "do_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "do_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "do_model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bound-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4490 - auc: 0.8670 - accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4489608705043793, 0.8669557571411133, 0.800000011920929]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-ethiopia",
   "metadata": {},
   "source": [
    "# 12. Feature Engineering. Features Polinomiales\n",
    "El objertivo de esta sección es el de agregar variables de entrada al modelo, que surgen de combinar las variables originales. El grado del polinomio determina la cantidad de nuevas variables que se suman al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "amended-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/poly2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "thick-treaty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 98ms/step - loss: 0.8208 - auc: 0.5607 - accuracy: 0.5843 - val_loss: 0.6535 - val_auc: 0.7002 - val_accuracy: 0.6919\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.6351 - auc: 0.7013 - accuracy: 0.6699 - val_loss: 0.5852 - val_auc: 0.7638 - val_accuracy: 0.7081\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4877 - auc: 0.8184 - accuracy: 0.7614 - val_loss: 0.5553 - val_auc: 0.8024 - val_accuracy: 0.7243\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4693 - auc: 0.8518 - accuracy: 0.7690 - val_loss: 0.5297 - val_auc: 0.8119 - val_accuracy: 0.7351\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4372 - auc: 0.8715 - accuracy: 0.7917 - val_loss: 0.5192 - val_auc: 0.8196 - val_accuracy: 0.7351\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4463 - auc: 0.8605 - accuracy: 0.7705 - val_loss: 0.5115 - val_auc: 0.8225 - val_accuracy: 0.7459\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4264 - auc: 0.8747 - accuracy: 0.8090 - val_loss: 0.5077 - val_auc: 0.8272 - val_accuracy: 0.7622\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4504 - auc: 0.8587 - accuracy: 0.7855 - val_loss: 0.5053 - val_auc: 0.8298 - val_accuracy: 0.7730\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4111 - auc: 0.8849 - accuracy: 0.7924 - val_loss: 0.4995 - val_auc: 0.8338 - val_accuracy: 0.7838\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4135 - auc: 0.8833 - accuracy: 0.7907 - val_loss: 0.5045 - val_auc: 0.8382 - val_accuracy: 0.7568\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4171 - auc: 0.8801 - accuracy: 0.7919 - val_loss: 0.4985 - val_auc: 0.8350 - val_accuracy: 0.7892\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4160 - auc: 0.8788 - accuracy: 0.7821 - val_loss: 0.4977 - val_auc: 0.8403 - val_accuracy: 0.7838\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3971 - auc: 0.8940 - accuracy: 0.8033 - val_loss: 0.4977 - val_auc: 0.8357 - val_accuracy: 0.7838\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4308 - auc: 0.8678 - accuracy: 0.7889 - val_loss: 0.5035 - val_auc: 0.8322 - val_accuracy: 0.7676\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4246 - auc: 0.8758 - accuracy: 0.7919 - val_loss: 0.4991 - val_auc: 0.8366 - val_accuracy: 0.7730\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4209 - auc: 0.8698 - accuracy: 0.7726 - val_loss: 0.5010 - val_auc: 0.8379 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4076 - auc: 0.8799 - accuracy: 0.7780 - val_loss: 0.5031 - val_auc: 0.8385 - val_accuracy: 0.7514\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4013 - auc: 0.8880 - accuracy: 0.7911 - val_loss: 0.5126 - val_auc: 0.8263 - val_accuracy: 0.7676\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3679 - auc: 0.9104 - accuracy: 0.7894 - val_loss: 0.4996 - val_auc: 0.8419 - val_accuracy: 0.7622\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4392 - auc: 0.8561 - accuracy: 0.7686 - val_loss: 0.4994 - val_auc: 0.8382 - val_accuracy: 0.7514\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3983 - auc: 0.8884 - accuracy: 0.8128 - val_loss: 0.5031 - val_auc: 0.8330 - val_accuracy: 0.7514\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3860 - auc: 0.8975 - accuracy: 0.8128 - val_loss: 0.5011 - val_auc: 0.8389 - val_accuracy: 0.7568\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3958 - auc: 0.8851 - accuracy: 0.7854 - val_loss: 0.5062 - val_auc: 0.8313 - val_accuracy: 0.7622\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3978 - auc: 0.8876 - accuracy: 0.7933 - val_loss: 0.5128 - val_auc: 0.8257 - val_accuracy: 0.7676\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4148 - auc: 0.8850 - accuracy: 0.7860 - val_loss: 0.5022 - val_auc: 0.8363 - val_accuracy: 0.7568\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4057 - auc: 0.8806 - accuracy: 0.7890 - val_loss: 0.5075 - val_auc: 0.8329 - val_accuracy: 0.7622\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4126 - auc: 0.8742 - accuracy: 0.7927 - val_loss: 0.5073 - val_auc: 0.8351 - val_accuracy: 0.7514\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3958 - auc: 0.8903 - accuracy: 0.7862 - val_loss: 0.5131 - val_auc: 0.8305 - val_accuracy: 0.7622\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.3763 - auc: 0.8961 - accuracy: 0.8337 - val_loss: 0.5095 - val_auc: 0.8383 - val_accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x219dca4ad60>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating polynomial features\n",
    "poly2 = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly2.fit(x_train)\n",
    "# Creating model\n",
    "p2_model  = Sequential()\n",
    "p2_model.add(Dense(1, input_shape=(poly2.n_output_features_,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "# Compiling model\n",
    "p2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Fit model\n",
    "p2_model.fit(poly2.transform(x_train), y_train, validation_data=(poly2.transform(x_valid), y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "consistent-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5927 - auc: 0.7797 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "p2_model = load_model(mc_path)\n",
    "eval = p2_model.evaluate(x=poly2.transform(x_test), y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-router",
   "metadata": {},
   "source": [
    "A continuación se observa la progresión de la métrica en **train** y **valid** en función del grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "saved-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-shore",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: Realizar la normalización de los datos **después** de aplicar el feature polinomial, sino se rompe todo :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "stupid-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define polynomial degrees to train and compute metrics\n",
    "poly_degrees = np.arange(1, 11, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "infectious-hunger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LRS callback\n",
    "# Define learning rate at start\n",
    "ilr = 0.1 # ilr=0.5, ds = 100000, dr=0.8, stc=False\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=1000, decay_rate=0.8, staircase=True) # Decay every (decay_steps) steps with a base of (decay_rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dying-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model Checkpoint callback.\n",
    "mc_path = 'model_checkpoints/get_best_poly_deg_checkpoint.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-princeton",
   "metadata": {},
   "source": [
    "En este punto cabe aclarar que se probó el parámetro *interaction_only* del preprocesador de polinomios y se llegó a la conclusión de que el desempeño mejora con este valor en *True*. Esto es así dado que, al activarlo, se logra un número mucho menor de variables en cada orden. Esto contribuye ampliamente a **reducir el overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "juvenile-census",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Polynomial order = 1 ---\n",
      "Input count = 8\n",
      "AUC for TRAIN subset is 0.8400\n",
      "AUC for VALID subset is 0.8666\n",
      "--- Polynomial order = 2 ---\n",
      "Input count = 36\n",
      "AUC for TRAIN subset is 0.8718\n",
      "AUC for VALID subset is 0.8462\n",
      "--- Polynomial order = 3 ---\n",
      "Input count = 92\n",
      "AUC for TRAIN subset is 0.9036\n",
      "AUC for VALID subset is 0.8404\n",
      "--- Polynomial order = 4 ---\n",
      "Input count = 162\n",
      "AUC for TRAIN subset is 0.9371\n",
      "AUC for VALID subset is 0.8007\n",
      "--- Polynomial order = 5 ---\n",
      "Input count = 218\n",
      "AUC for TRAIN subset is 0.9360\n",
      "AUC for VALID subset is 0.7993\n",
      "--- Polynomial order = 6 ---\n",
      "Input count = 246\n",
      "AUC for TRAIN subset is 0.8382\n",
      "AUC for VALID subset is 0.7924\n",
      "--- Polynomial order = 7 ---\n",
      "Input count = 254\n",
      "AUC for TRAIN subset is 0.8776\n",
      "AUC for VALID subset is 0.7878\n",
      "--- Polynomial order = 8 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9386\n",
      "AUC for VALID subset is 0.7853\n",
      "--- Polynomial order = 9 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9193\n",
      "AUC for VALID subset is 0.7919\n",
      "--- Polynomial order = 10 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.8545\n",
      "AUC for VALID subset is 0.8125\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=True)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    \n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(1e-4)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "convenient-position",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "auburn-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KFold from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_crossval(model, x, y, metrics, n_splits=2, shuffle=False, random_state=None, epochs=1, verbose='auto', callbacks=None, batch_size=32):\n",
    "    x = x.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    # Initialize kfold splitter\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    fold_num = 1\n",
    "    \n",
    "    # Create arrays for metrics\n",
    "    auc_valid = []\n",
    "    auc_train = []\n",
    "    \n",
    "    for train_index, valid_index in kf.split(x):\n",
    "        print(f'----- Fold N = {fold_num} -----')\n",
    "        # Get train and valid splits\n",
    "        x_train, y_train = x[train_index], y[train_index]\n",
    "        x_valid, y_valid = x[valid_index], y[valid_index]\n",
    "        \n",
    "        # Fit model\n",
    "        history = model.fit(x=x_train, \n",
    "                  y=y_train, \n",
    "                  validation_data=(x_valid, y_valid), \n",
    "                  shuffle=shuffle, \n",
    "                  epochs=epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  verbose=verbose, \n",
    "                  callbacks=callbacks\n",
    "                 )\n",
    "        \n",
    "        # Get metrics\n",
    "        eval_train = model.evaluate(x=x_train, y=y_train, return_dict=True, verbose=0)\n",
    "        eval_valid = model.evaluate(x=x_valid, y=y_valid, return_dict=True, verbose=0)\n",
    "        \n",
    "        tscore = eval_train['auc']\n",
    "        vscore = eval_valid['auc']\n",
    "        \n",
    "        tsize = x_train.shape[0]\n",
    "        vsize = x_valid.shape[0]\n",
    "        \n",
    "        # Append metrics\n",
    "        auc_train.append(tscore)\n",
    "        auc_valid.append(vscore)\n",
    "        \n",
    "        print(f'Size for TRAIN is {tsize}')\n",
    "        print(f'Size for VALID is {vsize}')\n",
    "        \n",
    "        print(f'AUC for TRAIN is {tscore:.4f}')\n",
    "        print(f'AUC for VALID is {vscore:.4f}')\n",
    "        \n",
    "        # Increase fold number\n",
    "        fold_num = fold_num + 1\n",
    "        \n",
    "    return auc_train, auc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "alpine-forwarding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Fold N = 1 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8464\n",
      "AUC for VALID is 0.7690\n",
      "----- Fold N = 2 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8225\n",
      "AUC for VALID is 0.8807\n",
      "----- Fold N = 3 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8303\n",
      "AUC for VALID is 0.8545\n",
      "----- Fold N = 4 -----\n",
      "Size for TRAIN is 343\n",
      "Size for VALID is 86\n",
      "AUC for TRAIN is 0.8261\n",
      "AUC for VALID is 0.8794\n",
      "----- Fold N = 5 -----\n",
      "Size for TRAIN is 344\n",
      "Size for VALID is 85\n",
      "AUC for TRAIN is 0.8471\n",
      "AUC for VALID is 0.7915\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "auct, aucv = kfold_crossval(model=model, x=x_train, y=y_train, metrics=metrics,\n",
    "                            n_splits=5, shuffle=False, random_state=None, epochs=1000, verbose=0, \n",
    "                            callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "#es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "facial-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average AUC for TRAIN is 0.8345\n",
      "Average AUC for VALID is 0.8350\n"
     ]
    }
   ],
   "source": [
    "aucv_mean = np.mean(aucv)\n",
    "auct_mean = np.mean(auct)\n",
    "print(f'Average AUC for TRAIN is {auct_mean:.4f}')\n",
    "print(f'Average AUC for VALID is {aucv_mean:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
