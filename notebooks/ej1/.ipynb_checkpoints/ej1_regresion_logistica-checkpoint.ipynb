{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "foster-gamma",
   "metadata": {},
   "source": [
    "# Redes Neuronales - Trabajo Práctico N° 2 - Ejercicio 1 - Regresión Logística\n",
    "# Notebook #2: Implementación de una Regresión Lineal\n",
    "En esta notebook se busca implementar una regresión logística para poder estimar la condición de diabético de un paciente, perteneciente al Pima Indians Dataset analizado en la notebook anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-provincial",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "* Chequear correcto reemplazo de NaN por mean.\n",
    "* Meter el z-score en scripts comunes a ambos ejercicios. Chequear StandardScaler **correctamente inicializado**. **¿Errores de discretización?**\n",
    "    * ¿Dónde meto el área bajo la curva ROC y el F2? -> Respondido por Luqui y Karina.\n",
    "* Añadir **tensorboard** para log entre epochs. Migrar **TODOS LOS GRÁFICOS** a TensorBoard.\n",
    "    * Agregar evolución de f2-score sobre train en selección del umbral.\n",
    "* Graficar **learning rate**.\n",
    "* Sacar los evaluate con **test**, para evitar malas interpretaciones.\n",
    "* PRIMERA PRUEBA DE POLY (2) ESTÁ MAL! **Falta normalizar despues del poly**\n",
    "* Informar métricas secundarias\n",
    "* ¿Kernel/Activity regulariizer? -> **kernel regularizer** afecta a los pesos, **activity regularizer** a las salidas.\n",
    "* Chequear **Dropout**. Creo que no tendría sentido graficarlo en una sola capa.\n",
    "\n",
    "# ¿Qué cosas puedo variar?\n",
    "* Función de activación:\n",
    "    * Sigmoid\n",
    "    * RELU\n",
    "    * ELU\n",
    "    * tanh\n",
    "    * Leaky RELU\n",
    "    \n",
    "* Optimizador:\n",
    "    * SGD\n",
    "    * Adam\n",
    "    \n",
    "* Early Stopping: Para el entrenamiento cuando la **loss** deja de mejorar. Se pasa a través de un **callback**. (https://keras.io/api/callbacks/early_stopping/) (https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/)\n",
    "* Kernel Initializer: Esto es, como son los pesos y bias iniciales. (https://keras.io/api/layers/initializers/)\n",
    "* Model Checkpoint: Guarda un checkpoint del modelo. Puede configurarse para elegir el mejor. Se pasa por **callback**. (https://keras.io/api/callbacks/model_checkpoint/)\n",
    "* Scheduling Learning Rate: Se hace variar el **learning rate** con una función. Es un **callback**. (https://keras.io/api/callbacks/learning_rate_scheduler/)\n",
    "* Reg. dropout: Para evitar overfitting, la capa de dropout \"borra\" una entrada de forma aleatoria y escala el resto. Es una **capa**. (https://keras.io/api/layers/regularization_layers/dropout/)\n",
    "* Regularización L1 y L2: Limita el espacio de soluciones agregando un término a la **función de costo**. (https://keras.io/api/layers/regularizers/)\n",
    "* Data Augmentation\n",
    "* Batch Normalization: Normaliza las entradas (media=0, dev=1). Es una **capa**. (https://keras.io/api/layers/normalization_layers/batch_normalization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-account",
   "metadata": {},
   "source": [
    "# Dudas\n",
    "* Al generar la métrica F2, ¿me devuelve por batch o por epoch? -> Esto finalmente se explica más adelante.\n",
    "* Al evaluar el predict en threshold selection ¿batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-sending",
   "metadata": {},
   "source": [
    "# ¿Cuáles son los requerimientos para el **clasificador**?\n",
    "* Métrica principal: **Área bajo la curva ROC**\n",
    "* Buscar el **umbral de decisión** para maximizar el **f2 score** \n",
    "* Informar métricas secundarias:\n",
    "    * Especificidad - Specificity (True Negative rate) measures the proportion of negatives that are correctly identified (i.e. the proportion of those who do not have the condition (unaffected) who are correctly identified as not having the condition).\n",
    "    * Sensibilidad\n",
    "    * Valor predictivo positivo\n",
    "    * Valor predictivo negativo\n",
    "    \n",
    "* **Pregunta adicional**:\n",
    "Dada la situación en la cual cambia la prevalencia de la enfermedad en la población a ser del 20%. Se desea reutilizar el modelo sin volver a entrenar, ¿Cómo lo harían? ¿Qué métricas se mantienen igual y cuáles cambiarian?. **¿clases desbalanceadas -> class weight?**. Las f-score son buenas para casos no balanceados!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-hospital",
   "metadata": {},
   "source": [
    "# 1. Cargando base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "diagnostic-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polyphonic-durham",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defensive-tours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read database from .csv\n",
    "df = pd.read_csv('../../databases/diabetes.csv', delimiter=',')\n",
    "\n",
    "# Show first rows of data\n",
    "df.head()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-substitute",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-brief",
   "metadata": {},
   "source": [
    "## 2.1 Filtrado de valores inválidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "centered-class",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>733.000000</td>\n",
       "      <td>541.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.405184</td>\n",
       "      <td>29.153420</td>\n",
       "      <td>155.548223</td>\n",
       "      <td>32.457464</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>12.382158</td>\n",
       "      <td>10.476982</td>\n",
       "      <td>118.775855</td>\n",
       "      <td>6.924988</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>76.250000</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>32.300000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  763.000000     733.000000     541.000000  394.000000   \n",
       "mean      3.845052  121.686763      72.405184      29.153420  155.548223   \n",
       "std       3.369578   30.535641      12.382158      10.476982  118.775855   \n",
       "min       0.000000   44.000000      24.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   76.250000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  125.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  190.000000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  757.000000                768.000000  768.000000  768.000000  \n",
       "mean    32.457464                  0.471876   33.240885    0.348958  \n",
       "std      6.924988                  0.331329   11.760232    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.500000                  0.243750   24.000000    0.000000  \n",
       "50%     32.300000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering Glucose values\n",
    "df['Glucose'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Blood Pressure values\n",
    "df['BloodPressure'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Skin Thickness values\n",
    "df['SkinThickness'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Insulin values\n",
    "df['Insulin'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Filtering Body Mass Index values\n",
    "df['BMI'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-recorder",
   "metadata": {},
   "source": [
    "## 2.2 Remoción de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bound-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from src.helper import remove_outliers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complex-cleaning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>764.000000</td>\n",
       "      <td>763.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>538.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>749.000000</td>\n",
       "      <td>739.000000</td>\n",
       "      <td>759.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.786649</td>\n",
       "      <td>121.686763</td>\n",
       "      <td>72.115438</td>\n",
       "      <td>28.903346</td>\n",
       "      <td>132.610811</td>\n",
       "      <td>32.204005</td>\n",
       "      <td>0.429832</td>\n",
       "      <td>32.805007</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.278714</td>\n",
       "      <td>30.535641</td>\n",
       "      <td>11.239072</td>\n",
       "      <td>9.865480</td>\n",
       "      <td>74.285393</td>\n",
       "      <td>6.491385</td>\n",
       "      <td>0.249684</td>\n",
       "      <td>11.113182</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>27.400000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>177.500000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.587000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   764.000000  763.000000     719.000000     538.000000  370.000000   \n",
       "mean      3.786649  121.686763      72.115438      28.903346  132.610811   \n",
       "std       3.278714   30.535641      11.239072       9.865480   74.285393   \n",
       "min       0.000000   44.000000      40.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      64.000000      22.000000   75.000000   \n",
       "50%       3.000000  117.000000      72.000000      29.000000  120.000000   \n",
       "75%       6.000000  141.000000      80.000000      36.000000  177.500000   \n",
       "max      13.000000  199.000000     104.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  749.000000                739.000000  759.000000  768.000000  \n",
       "mean    32.204005                  0.429832   32.805007    0.348958  \n",
       "std      6.491385                  0.249684   11.113182    0.476951  \n",
       "min     18.200000                  0.078000   21.000000    0.000000  \n",
       "25%     27.400000                  0.238000   24.000000    0.000000  \n",
       "50%     32.000000                  0.356000   29.000000    0.000000  \n",
       "75%     36.500000                  0.587000   40.000000    1.000000  \n",
       "max     50.000000                  1.191000   66.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_labels = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "y_labels = ['Outcome']\n",
    "\n",
    "for column in x_labels:\n",
    "    remove_outliers(df, column)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-gateway",
   "metadata": {},
   "source": [
    "# 3. Separación del conjunto de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "racial-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecological-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedicated-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output variables for the model\n",
    "df_x = df[x_labels]\n",
    "df_y = df[y_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ignored-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train_valid and test\n",
    "x_train_valid, x_test, y_train_valid, y_test = model_selection.train_test_split(df_x, df_y, test_size=0.2, random_state=15, shuffle=True)\n",
    "\n",
    "# Split the train_valid sub-dataset into train and valid\n",
    "x_train, x_valid, y_train, y_valid = model_selection.train_test_split(x_train_valid, y_train_valid, test_size=0.3, random_state=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sitting-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>425.000000</td>\n",
       "      <td>425.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>291.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>417.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>425.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.896471</td>\n",
       "      <td>122.360000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.430713</td>\n",
       "      <td>32.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.269876</td>\n",
       "      <td>30.066982</td>\n",
       "      <td>10.805353</td>\n",
       "      <td>9.793471</td>\n",
       "      <td>70.854164</td>\n",
       "      <td>6.341281</td>\n",
       "      <td>0.252835</td>\n",
       "      <td>10.681080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>27.700000</td>\n",
       "      <td>0.235500</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   425.000000  425.000000     403.000000     291.000000  205.000000   \n",
       "mean      3.896471  122.360000      71.665012      28.862543  134.414634   \n",
       "std       3.269876   30.066982      10.805353       9.793471   70.854164   \n",
       "min       0.000000   44.000000      40.000000       8.000000   18.000000   \n",
       "25%       1.000000  100.000000      64.000000      21.500000   76.000000   \n",
       "50%       3.000000  118.000000      72.000000      28.000000  125.000000   \n",
       "75%       6.000000  142.000000      78.000000      36.000000  180.000000   \n",
       "max      13.000000  199.000000     104.000000      52.000000  328.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  417.000000                411.000000  425.000000  \n",
       "mean    32.166667                  0.430713   32.494118  \n",
       "std      6.341281                  0.252835   10.681080  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     27.700000                  0.235500   24.000000  \n",
       "50%     31.600000                  0.355000   29.000000  \n",
       "75%     36.300000                  0.600500   39.000000  \n",
       "max     49.600000                  1.189000   66.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set before NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-disorder",
   "metadata": {},
   "source": [
    "# 4. Reemplazo de valores inválidos\n",
    "Como se destacó en el análisis estadístico de datos, el dataset suministrado posee varios valores faltantes en algunos individuos. Se asume que en la etapa de producción el modelo contará con todas las variables correctamente informadas, no admitiendo el faltante de alguna de ellas. Luego, se decide reemplazar aquellos valores inválidos en **train**, **valid** y **test** por la correspondiente media en el dataset de train. En este caso, se considera a la media como un estimador correcto para la ocasión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "duplicate-adventure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\series.py:4509: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n"
     ]
    }
   ],
   "source": [
    "# Compute the mean of training\n",
    "train_means = x_train.mean().to_numpy()\n",
    "\n",
    "# Replacing nan values of the train dataset with training mean values\n",
    "for index, column in enumerate(x_train.columns):\n",
    "    x_train.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "\n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_test.columns):\n",
    "    x_test.loc[:,column].replace(np.nan, train_means[index], inplace=True)\n",
    "    \n",
    "# Replacing nan values of the test dataset with training mean values\n",
    "for index, column in enumerate(x_valid.columns):\n",
    "    x_valid.loc[:,column].replace(np.nan, train_means[index], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "original-impression",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.896471</td>\n",
       "      <td>122.360000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.430713</td>\n",
       "      <td>32.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.254560</td>\n",
       "      <td>29.926152</td>\n",
       "      <td>10.472012</td>\n",
       "      <td>8.061461</td>\n",
       "      <td>48.916861</td>\n",
       "      <td>6.251752</td>\n",
       "      <td>0.247461</td>\n",
       "      <td>10.631051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.200000</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>27.800000</td>\n",
       "      <td>0.238000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.591000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>328.000000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>1.189000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   429.000000  429.000000     429.000000     429.000000  429.000000   \n",
       "mean      3.896471  122.360000      71.665012      28.862543  134.414634   \n",
       "std       3.254560   29.926152      10.472012       8.061461   48.916861   \n",
       "min       0.000000   44.000000      40.000000       8.000000   18.000000   \n",
       "25%       1.000000  100.000000      64.000000      25.000000  129.000000   \n",
       "50%       3.000000  119.000000      71.665012      28.862543  134.414634   \n",
       "75%       6.000000  141.000000      78.000000      32.000000  134.414634   \n",
       "max      13.000000  199.000000     104.000000      52.000000  328.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  429.000000                429.000000  429.000000  \n",
       "mean    32.166667                  0.430713   32.494118  \n",
       "std      6.251752                  0.247461   10.631051  \n",
       "min     18.200000                  0.084000   21.000000  \n",
       "25%     27.800000                  0.238000   24.000000  \n",
       "50%     32.000000                  0.371000   29.000000  \n",
       "75%     36.000000                  0.591000   39.000000  \n",
       "max     49.600000                  1.189000   66.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train set after NaN replacement\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "welcome-governor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.691892</td>\n",
       "      <td>119.850595</td>\n",
       "      <td>73.313406</td>\n",
       "      <td>28.960620</td>\n",
       "      <td>131.659328</td>\n",
       "      <td>32.292252</td>\n",
       "      <td>0.426025</td>\n",
       "      <td>33.678283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.284878</td>\n",
       "      <td>30.084793</td>\n",
       "      <td>12.045479</td>\n",
       "      <td>8.215413</td>\n",
       "      <td>51.838933</td>\n",
       "      <td>6.738731</td>\n",
       "      <td>0.245125</td>\n",
       "      <td>11.670442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>26.600000</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.542000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.159000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   185.000000  185.000000     185.000000     185.000000  185.000000   \n",
       "mean      3.691892  119.850595      73.313406      28.960620  131.659328   \n",
       "std       3.284878   30.084793      12.045479       8.215413   51.838933   \n",
       "min       0.000000   57.000000      44.000000       7.000000   14.000000   \n",
       "25%       1.000000   99.000000      65.000000      26.000000  116.000000   \n",
       "50%       3.000000  114.000000      72.000000      28.862543  134.414634   \n",
       "75%       6.000000  136.000000      82.000000      32.000000  134.414634   \n",
       "max      13.000000  198.000000     100.000000      54.000000  335.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  185.000000                185.000000  185.000000  \n",
       "mean    32.292252                  0.426025   33.678283  \n",
       "std      6.738731                  0.245125   11.670442  \n",
       "min     19.600000                  0.096000   21.000000  \n",
       "25%     26.600000                  0.241000   24.000000  \n",
       "50%     32.500000                  0.365000   30.000000  \n",
       "75%     36.500000                  0.542000   42.000000  \n",
       "max     50.000000                  1.159000   66.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation set after NaN replacement\n",
    "x_valid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "metallic-following",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.597403</td>\n",
       "      <td>122.038961</td>\n",
       "      <td>71.787761</td>\n",
       "      <td>28.887267</td>\n",
       "      <td>133.390719</td>\n",
       "      <td>32.197403</td>\n",
       "      <td>0.432119</td>\n",
       "      <td>32.603820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.304818</td>\n",
       "      <td>32.320876</td>\n",
       "      <td>10.448535</td>\n",
       "      <td>8.867500</td>\n",
       "      <td>58.146512</td>\n",
       "      <td>6.484578</td>\n",
       "      <td>0.238998</td>\n",
       "      <td>11.431621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>95.250000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>23.250000</td>\n",
       "      <td>108.250000</td>\n",
       "      <td>26.925000</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>71.665012</td>\n",
       "      <td>28.862543</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>32.166667</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.750000</td>\n",
       "      <td>142.750000</td>\n",
       "      <td>79.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>134.414634</td>\n",
       "      <td>36.625000</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>40.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>49.300000</td>\n",
       "      <td>1.191000</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   154.000000  154.000000     154.000000     154.000000  154.000000   \n",
       "mean      3.597403  122.038961      71.787761      28.887267  133.390719   \n",
       "std       3.304818   32.320876      10.448535       8.867500   58.146512   \n",
       "min       0.000000   61.000000      44.000000       7.000000   23.000000   \n",
       "25%       1.000000   95.250000      64.000000      23.250000  108.250000   \n",
       "50%       3.000000  117.000000      71.665012      28.862543  134.414634   \n",
       "75%       5.750000  142.750000      79.500000      33.000000  134.414634   \n",
       "max      13.000000  197.000000      94.000000      56.000000  360.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age  \n",
       "count  154.000000                154.000000  154.000000  \n",
       "mean    32.197403                  0.432119   32.603820  \n",
       "std      6.484578                  0.238998   11.431621  \n",
       "min     18.400000                  0.078000   21.000000  \n",
       "25%     26.925000                  0.254000   24.000000  \n",
       "50%     32.166667                  0.376500   28.000000  \n",
       "75%     36.625000                  0.567000   40.750000  \n",
       "max     49.300000                  1.191000   66.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test set after NaN replacement\n",
    "x_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-masters",
   "metadata": {},
   "source": [
    "# 5. Normalización de datos de entrada. Z Score. \n",
    "Dado que todas las variables en juego son numéricas, se puede aplicar z-score a todo el dataset. Esta operación se hace con el objetivo de poder obtener mayor información de los pesos calculados por el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "foreign-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "c:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT! Backup unnormalized subsets for further utilization\n",
    "x_train_un = x_train\n",
    "x_valid_un = x_valid\n",
    "x_test_un = x_test\n",
    "\n",
    "# Apply z-score to all sub-datasets\n",
    "scalable_variables = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction','Age']\n",
    "\n",
    "if scalable_variables:\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train.loc[:, scalable_variables])\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train.loc[:, scalable_variables] = scaler.transform(x_train.loc[:, scalable_variables])\n",
    "    x_test.loc[:, scalable_variables] = scaler.transform(x_test.loc[:, scalable_variables])\n",
    "    x_valid.loc[:, scalable_variables] = scaler.transform(x_valid.loc[:, scalable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "golden-honolulu",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "      <td>4.290000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.140692e-17</td>\n",
       "      <td>2.484415e-17</td>\n",
       "      <td>-3.498885e-16</td>\n",
       "      <td>3.685216e-16</td>\n",
       "      <td>-5.010237e-16</td>\n",
       "      <td>3.457478e-16</td>\n",
       "      <td>1.242208e-16</td>\n",
       "      <td>1.780498e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "      <td>1.001168e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.198632e+00</td>\n",
       "      <td>-2.621503e+00</td>\n",
       "      <td>-3.027306e+00</td>\n",
       "      <td>-2.590957e+00</td>\n",
       "      <td>-2.382625e+00</td>\n",
       "      <td>-2.236649e+00</td>\n",
       "      <td>-1.402716e+00</td>\n",
       "      <td>-1.082446e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.910121e-01</td>\n",
       "      <td>-7.480449e-01</td>\n",
       "      <td>-7.328068e-01</td>\n",
       "      <td>-4.796963e-01</td>\n",
       "      <td>-1.108198e-01</td>\n",
       "      <td>-6.992863e-01</td>\n",
       "      <td>-7.796693e-01</td>\n",
       "      <td>-7.999242e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.757722e-01</td>\n",
       "      <td>-1.124075e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.412180e-16</td>\n",
       "      <td>-5.816991e-16</td>\n",
       "      <td>-2.669032e-02</td>\n",
       "      <td>-2.415838e-01</td>\n",
       "      <td>-3.290547e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.470876e-01</td>\n",
       "      <td>6.235938e-01</td>\n",
       "      <td>6.056510e-01</td>\n",
       "      <td>3.896465e-01</td>\n",
       "      <td>-5.816991e-16</td>\n",
       "      <td>6.138773e-01</td>\n",
       "      <td>6.484824e-01</td>\n",
       "      <td>6.126843e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.800427e+00</td>\n",
       "      <td>2.563961e+00</td>\n",
       "      <td>3.091358e+00</td>\n",
       "      <td>2.873483e+00</td>\n",
       "      <td>3.962057e+00</td>\n",
       "      <td>2.791807e+00</td>\n",
       "      <td>3.067844e+00</td>\n",
       "      <td>3.155380e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pregnancies       Glucose  BloodPressure  SkinThickness       Insulin  \\\n",
       "count  4.290000e+02  4.290000e+02   4.290000e+02   4.290000e+02  4.290000e+02   \n",
       "mean  -4.140692e-17  2.484415e-17  -3.498885e-16   3.685216e-16 -5.010237e-16   \n",
       "std    1.001168e+00  1.001168e+00   1.001168e+00   1.001168e+00  1.001168e+00   \n",
       "min   -1.198632e+00 -2.621503e+00  -3.027306e+00  -2.590957e+00 -2.382625e+00   \n",
       "25%   -8.910121e-01 -7.480449e-01  -7.328068e-01  -4.796963e-01 -1.108198e-01   \n",
       "50%   -2.757722e-01 -1.124075e-01   0.000000e+00   4.412180e-16 -5.816991e-16   \n",
       "75%    6.470876e-01  6.235938e-01   6.056510e-01   3.896465e-01 -5.816991e-16   \n",
       "max    2.800427e+00  2.563961e+00   3.091358e+00   2.873483e+00  3.962057e+00   \n",
       "\n",
       "                BMI  DiabetesPedigreeFunction           Age  \n",
       "count  4.290000e+02              4.290000e+02  4.290000e+02  \n",
       "mean   3.457478e-16              1.242208e-16  1.780498e-16  \n",
       "std    1.001168e+00              1.001168e+00  1.001168e+00  \n",
       "min   -2.236649e+00             -1.402716e+00 -1.082446e+00  \n",
       "25%   -6.992863e-01             -7.796693e-01 -7.999242e-01  \n",
       "50%   -2.669032e-02             -2.415838e-01 -3.290547e-01  \n",
       "75%    6.138773e-01              6.484824e-01  6.126843e-01  \n",
       "max    2.791807e+00              3.067844e+00  3.155380e+00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-preservation",
   "metadata": {},
   "source": [
    "# 6. Regresión Logística - Test #1\n",
    "Primera prueba de regresión logística. Se usa SGD y AUC como métrica principal. Se emplea la Binary Cross-Entropy como loss subrogada, dado que **la AUC no es diferenciable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "empirical-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading TensorBoard for learning logging\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "biblical-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "governing-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.metrics import SensitivityAtSpecificity\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "diverse-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/logistic_regression_first_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "elect-temperature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Get model brief\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "municipal-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics definition\n",
    "metrics = ['AUC', 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hindu-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 14s 148ms/step - loss: 0.7195 - auc: 0.6092 - accuracy: 0.5716 - val_loss: 0.7297 - val_auc: 0.5880 - val_accuracy: 0.5838\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.7163 - auc: 0.6327 - accuracy: 0.5924 - val_loss: 0.7116 - val_auc: 0.6075 - val_accuracy: 0.6162\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6539 - auc: 0.6856 - accuracy: 0.6367 - val_loss: 0.6947 - val_auc: 0.6256 - val_accuracy: 0.6162\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6688 - auc: 0.6649 - accuracy: 0.6131 - val_loss: 0.6798 - val_auc: 0.6402 - val_accuracy: 0.6162\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6249 - auc: 0.7175 - accuracy: 0.6624 - val_loss: 0.6659 - val_auc: 0.6550 - val_accuracy: 0.6432\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6401 - auc: 0.6859 - accuracy: 0.6675 - val_loss: 0.6533 - val_auc: 0.6685 - val_accuracy: 0.6595\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6432 - auc: 0.7187 - accuracy: 0.6895 - val_loss: 0.6419 - val_auc: 0.6824 - val_accuracy: 0.6595\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6077 - auc: 0.7314 - accuracy: 0.6993 - val_loss: 0.6305 - val_auc: 0.6933 - val_accuracy: 0.6703\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6049 - auc: 0.7393 - accuracy: 0.6947 - val_loss: 0.6210 - val_auc: 0.7037 - val_accuracy: 0.6865\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.6066 - auc: 0.7336 - accuracy: 0.6955 - val_loss: 0.6121 - val_auc: 0.7165 - val_accuracy: 0.6919\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.5880 - auc: 0.7600 - accuracy: 0.7062 - val_loss: 0.6036 - val_auc: 0.7256 - val_accuracy: 0.6919\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5545 - auc: 0.7929 - accuracy: 0.7308 - val_loss: 0.5959 - val_auc: 0.7337 - val_accuracy: 0.6973\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5775 - auc: 0.7653 - accuracy: 0.7092 - val_loss: 0.5888 - val_auc: 0.7415 - val_accuracy: 0.6973\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.5757 - auc: 0.7690 - accuracy: 0.7145 - val_loss: 0.5821 - val_auc: 0.7471 - val_accuracy: 0.7081\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.5605 - auc: 0.7815 - accuracy: 0.7305 - val_loss: 0.5763 - val_auc: 0.7534 - val_accuracy: 0.7081\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 0.5357 - auc: 0.7961 - accuracy: 0.7363 - val_loss: 0.5704 - val_auc: 0.7592 - val_accuracy: 0.7081\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5228 - auc: 0.8219 - accuracy: 0.7601 - val_loss: 0.5653 - val_auc: 0.7657 - val_accuracy: 0.7351\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5312 - auc: 0.8126 - accuracy: 0.7292 - val_loss: 0.5607 - val_auc: 0.7704 - val_accuracy: 0.7351\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5310 - auc: 0.8107 - accuracy: 0.7256 - val_loss: 0.5560 - val_auc: 0.7732 - val_accuracy: 0.7351\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5449 - auc: 0.7877 - accuracy: 0.7034 - val_loss: 0.5519 - val_auc: 0.7769 - val_accuracy: 0.7351\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5432 - auc: 0.7911 - accuracy: 0.7100 - val_loss: 0.5483 - val_auc: 0.7817 - val_accuracy: 0.7297\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5274 - auc: 0.8121 - accuracy: 0.7277 - val_loss: 0.5446 - val_auc: 0.7857 - val_accuracy: 0.7297\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5111 - auc: 0.8195 - accuracy: 0.7373 - val_loss: 0.5409 - val_auc: 0.7889 - val_accuracy: 0.7297\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5268 - auc: 0.8035 - accuracy: 0.7325 - val_loss: 0.5377 - val_auc: 0.7928 - val_accuracy: 0.7243\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5147 - auc: 0.8211 - accuracy: 0.7490 - val_loss: 0.5348 - val_auc: 0.7944 - val_accuracy: 0.7243\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4909 - auc: 0.8469 - accuracy: 0.7518 - val_loss: 0.5322 - val_auc: 0.7964 - val_accuracy: 0.7243\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5194 - auc: 0.8084 - accuracy: 0.7183 - val_loss: 0.5299 - val_auc: 0.7985 - val_accuracy: 0.7351\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5406 - auc: 0.7968 - accuracy: 0.7264 - val_loss: 0.5274 - val_auc: 0.8001 - val_accuracy: 0.7351\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4897 - auc: 0.8424 - accuracy: 0.7653 - val_loss: 0.5251 - val_auc: 0.8019 - val_accuracy: 0.7351\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4986 - auc: 0.8267 - accuracy: 0.7631 - val_loss: 0.5228 - val_auc: 0.8038 - val_accuracy: 0.7405\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5058 - auc: 0.8274 - accuracy: 0.7408 - val_loss: 0.5208 - val_auc: 0.8043 - val_accuracy: 0.7405\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5042 - auc: 0.8378 - accuracy: 0.7518 - val_loss: 0.5190 - val_auc: 0.8055 - val_accuracy: 0.7459\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5048 - auc: 0.8311 - accuracy: 0.7261 - val_loss: 0.5170 - val_auc: 0.8060 - val_accuracy: 0.7459\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5270 - auc: 0.8003 - accuracy: 0.7230 - val_loss: 0.5154 - val_auc: 0.8072 - val_accuracy: 0.7459\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5063 - auc: 0.8242 - accuracy: 0.7316 - val_loss: 0.5137 - val_auc: 0.8083 - val_accuracy: 0.7459\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5037 - auc: 0.8242 - accuracy: 0.7239 - val_loss: 0.5121 - val_auc: 0.8094 - val_accuracy: 0.7459\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4918 - auc: 0.8335 - accuracy: 0.7338 - val_loss: 0.5104 - val_auc: 0.8109 - val_accuracy: 0.7459\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4898 - auc: 0.8371 - accuracy: 0.7245 - val_loss: 0.5091 - val_auc: 0.8110 - val_accuracy: 0.7459\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4977 - auc: 0.8285 - accuracy: 0.7255 - val_loss: 0.5078 - val_auc: 0.8116 - val_accuracy: 0.7459\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4773 - auc: 0.8405 - accuracy: 0.7480 - val_loss: 0.5065 - val_auc: 0.8126 - val_accuracy: 0.7568\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4876 - auc: 0.8288 - accuracy: 0.7375 - val_loss: 0.5055 - val_auc: 0.8134 - val_accuracy: 0.7568\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4911 - auc: 0.8372 - accuracy: 0.7387 - val_loss: 0.5043 - val_auc: 0.8141 - val_accuracy: 0.7459\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4829 - auc: 0.8532 - accuracy: 0.7288 - val_loss: 0.5033 - val_auc: 0.8151 - val_accuracy: 0.7405\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5028 - auc: 0.8278 - accuracy: 0.7155 - val_loss: 0.5023 - val_auc: 0.8163 - val_accuracy: 0.7405\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4810 - auc: 0.8392 - accuracy: 0.7300 - val_loss: 0.5013 - val_auc: 0.8169 - val_accuracy: 0.7459\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5118 - auc: 0.8060 - accuracy: 0.7065 - val_loss: 0.5003 - val_auc: 0.8177 - val_accuracy: 0.7351\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4672 - auc: 0.8603 - accuracy: 0.7567 - val_loss: 0.4995 - val_auc: 0.8185 - val_accuracy: 0.7405\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8546 - accuracy: 0.7701 - val_loss: 0.4986 - val_auc: 0.8186 - val_accuracy: 0.7405\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4619 - auc: 0.8583 - accuracy: 0.7762 - val_loss: 0.4978 - val_auc: 0.8195 - val_accuracy: 0.7568\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4641 - auc: 0.8559 - accuracy: 0.7633 - val_loss: 0.4971 - val_auc: 0.8195 - val_accuracy: 0.7568\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4918 - auc: 0.8321 - accuracy: 0.7449 - val_loss: 0.4964 - val_auc: 0.8206 - val_accuracy: 0.7568\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4795 - auc: 0.8400 - accuracy: 0.7642 - val_loss: 0.4957 - val_auc: 0.8214 - val_accuracy: 0.7622\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5186 - auc: 0.8050 - accuracy: 0.7129 - val_loss: 0.4951 - val_auc: 0.8216 - val_accuracy: 0.7622\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4618 - auc: 0.8529 - accuracy: 0.7647 - val_loss: 0.4945 - val_auc: 0.8224 - val_accuracy: 0.7622\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4595 - auc: 0.8515 - accuracy: 0.7679 - val_loss: 0.4939 - val_auc: 0.8237 - val_accuracy: 0.7622\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4828 - auc: 0.8440 - accuracy: 0.7709 - val_loss: 0.4934 - val_auc: 0.8241 - val_accuracy: 0.7622\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4379 - auc: 0.8709 - accuracy: 0.7917 - val_loss: 0.4929 - val_auc: 0.8242 - val_accuracy: 0.7622\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4819 - auc: 0.8435 - accuracy: 0.7566 - val_loss: 0.4924 - val_auc: 0.8243 - val_accuracy: 0.7622\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4661 - auc: 0.8561 - accuracy: 0.7656 - val_loss: 0.4916 - val_auc: 0.8249 - val_accuracy: 0.7568\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4845 - auc: 0.8387 - accuracy: 0.7488 - val_loss: 0.4909 - val_auc: 0.8254 - val_accuracy: 0.7622\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8434 - accuracy: 0.7725 - val_loss: 0.4904 - val_auc: 0.8255 - val_accuracy: 0.7568\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4773 - auc: 0.8401 - accuracy: 0.7634 - val_loss: 0.4900 - val_auc: 0.8260 - val_accuracy: 0.7568\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4855 - auc: 0.8257 - accuracy: 0.7518 - val_loss: 0.4895 - val_auc: 0.8266 - val_accuracy: 0.7568\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4556 - auc: 0.8584 - accuracy: 0.7694 - val_loss: 0.4891 - val_auc: 0.8270 - val_accuracy: 0.7568\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4890 - auc: 0.8304 - accuracy: 0.7382 - val_loss: 0.4888 - val_auc: 0.8275 - val_accuracy: 0.7568\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4776 - auc: 0.8379 - accuracy: 0.7408 - val_loss: 0.4883 - val_auc: 0.8275 - val_accuracy: 0.7568\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4838 - auc: 0.8337 - accuracy: 0.7389 - val_loss: 0.4878 - val_auc: 0.8278 - val_accuracy: 0.7568\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8464 - accuracy: 0.7581 - val_loss: 0.4877 - val_auc: 0.8277 - val_accuracy: 0.7568\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4484 - auc: 0.8552 - accuracy: 0.7883 - val_loss: 0.4874 - val_auc: 0.8282 - val_accuracy: 0.7568\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4555 - auc: 0.8560 - accuracy: 0.7719 - val_loss: 0.4870 - val_auc: 0.8283 - val_accuracy: 0.7568\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4688 - auc: 0.8525 - accuracy: 0.7687 - val_loss: 0.4866 - val_auc: 0.8287 - val_accuracy: 0.7568\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4554 - auc: 0.8562 - accuracy: 0.7678 - val_loss: 0.4862 - val_auc: 0.8290 - val_accuracy: 0.7568\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5034 - auc: 0.8220 - accuracy: 0.7364 - val_loss: 0.4857 - val_auc: 0.8289 - val_accuracy: 0.7568\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4757 - auc: 0.8420 - accuracy: 0.7610 - val_loss: 0.4855 - val_auc: 0.8293 - val_accuracy: 0.7568\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4522 - auc: 0.8631 - accuracy: 0.7873 - val_loss: 0.4853 - val_auc: 0.8292 - val_accuracy: 0.7568\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4698 - auc: 0.8530 - accuracy: 0.7569 - val_loss: 0.4851 - val_auc: 0.8293 - val_accuracy: 0.7568\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4888 - auc: 0.8390 - accuracy: 0.7334 - val_loss: 0.4849 - val_auc: 0.8297 - val_accuracy: 0.7568\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5005 - auc: 0.8157 - accuracy: 0.7395 - val_loss: 0.4846 - val_auc: 0.8298 - val_accuracy: 0.7622\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4803 - auc: 0.8343 - accuracy: 0.7558 - val_loss: 0.4844 - val_auc: 0.8300 - val_accuracy: 0.7622\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4357 - auc: 0.8738 - accuracy: 0.7884 - val_loss: 0.4841 - val_auc: 0.8302 - val_accuracy: 0.7622\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4700 - auc: 0.8456 - accuracy: 0.7666 - val_loss: 0.4838 - val_auc: 0.8301 - val_accuracy: 0.7622\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4595 - auc: 0.8497 - accuracy: 0.7682 - val_loss: 0.4835 - val_auc: 0.8301 - val_accuracy: 0.7622\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4825 - auc: 0.8337 - accuracy: 0.7433 - val_loss: 0.4834 - val_auc: 0.8303 - val_accuracy: 0.7622\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4933 - auc: 0.8293 - accuracy: 0.7457 - val_loss: 0.4832 - val_auc: 0.8305 - val_accuracy: 0.7622\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4796 - auc: 0.8379 - accuracy: 0.7494 - val_loss: 0.4829 - val_auc: 0.8305 - val_accuracy: 0.7622\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4962 - auc: 0.8270 - accuracy: 0.7246 - val_loss: 0.4828 - val_auc: 0.8308 - val_accuracy: 0.7622\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4897 - auc: 0.8275 - accuracy: 0.7531 - val_loss: 0.4827 - val_auc: 0.8313 - val_accuracy: 0.7676\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5236 - auc: 0.7945 - accuracy: 0.7215 - val_loss: 0.4824 - val_auc: 0.8318 - val_accuracy: 0.7676\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4639 - auc: 0.8542 - accuracy: 0.7673 - val_loss: 0.4823 - val_auc: 0.8320 - val_accuracy: 0.7676\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8492 - accuracy: 0.7731 - val_loss: 0.4821 - val_auc: 0.8319 - val_accuracy: 0.7676\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4648 - auc: 0.8491 - accuracy: 0.7976 - val_loss: 0.4818 - val_auc: 0.8321 - val_accuracy: 0.7676\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8324 - accuracy: 0.7759 - val_loss: 0.4817 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4657 - auc: 0.8471 - accuracy: 0.7634 - val_loss: 0.4815 - val_auc: 0.8331 - val_accuracy: 0.7676\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4471 - auc: 0.8581 - accuracy: 0.7865 - val_loss: 0.4815 - val_auc: 0.8325 - val_accuracy: 0.7676\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4490 - auc: 0.8650 - accuracy: 0.7850 - val_loss: 0.4812 - val_auc: 0.8319 - val_accuracy: 0.7676\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4566 - auc: 0.8527 - accuracy: 0.7639 - val_loss: 0.4811 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4721 - auc: 0.8437 - accuracy: 0.7561 - val_loss: 0.4811 - val_auc: 0.8324 - val_accuracy: 0.7676\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8307 - accuracy: 0.7615 - val_loss: 0.4810 - val_auc: 0.8328 - val_accuracy: 0.7676\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4515 - auc: 0.8603 - accuracy: 0.7718 - val_loss: 0.4809 - val_auc: 0.8328 - val_accuracy: 0.7676\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4519 - auc: 0.8445 - accuracy: 0.7856 - val_loss: 0.4808 - val_auc: 0.8328 - val_accuracy: 0.7676\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4801 - auc: 0.8430 - accuracy: 0.7675 - val_loss: 0.4805 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4604 - auc: 0.8382 - accuracy: 0.7663 - val_loss: 0.4805 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4517 - auc: 0.8635 - accuracy: 0.7846 - val_loss: 0.4804 - val_auc: 0.8324 - val_accuracy: 0.7676\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4544 - auc: 0.8491 - accuracy: 0.7739 - val_loss: 0.4804 - val_auc: 0.8324 - val_accuracy: 0.7676\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4526 - auc: 0.8664 - accuracy: 0.7677 - val_loss: 0.4802 - val_auc: 0.8326 - val_accuracy: 0.7676\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4825 - auc: 0.8286 - accuracy: 0.7461 - val_loss: 0.4803 - val_auc: 0.8328 - val_accuracy: 0.7676\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4359 - auc: 0.8668 - accuracy: 0.7955 - val_loss: 0.4801 - val_auc: 0.8329 - val_accuracy: 0.7676\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8493 - accuracy: 0.7726 - val_loss: 0.4800 - val_auc: 0.8329 - val_accuracy: 0.7622\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4549 - auc: 0.8573 - accuracy: 0.7674 - val_loss: 0.4800 - val_auc: 0.8328 - val_accuracy: 0.7622\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4620 - auc: 0.8510 - accuracy: 0.7468 - val_loss: 0.4799 - val_auc: 0.8329 - val_accuracy: 0.7622\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4661 - auc: 0.8456 - accuracy: 0.7554 - val_loss: 0.4799 - val_auc: 0.8329 - val_accuracy: 0.7622\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4674 - auc: 0.8445 - accuracy: 0.7669 - val_loss: 0.4797 - val_auc: 0.8327 - val_accuracy: 0.7622\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4811 - auc: 0.8357 - accuracy: 0.7666 - val_loss: 0.4799 - val_auc: 0.8328 - val_accuracy: 0.7622\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4786 - auc: 0.8281 - accuracy: 0.7570 - val_loss: 0.4797 - val_auc: 0.8327 - val_accuracy: 0.7622\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4585 - auc: 0.8559 - accuracy: 0.7671 - val_loss: 0.4797 - val_auc: 0.8327 - val_accuracy: 0.7622\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4463 - auc: 0.8582 - accuracy: 0.7783 - val_loss: 0.4796 - val_auc: 0.8327 - val_accuracy: 0.7622\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4635 - auc: 0.8499 - accuracy: 0.7607 - val_loss: 0.4795 - val_auc: 0.8327 - val_accuracy: 0.7622\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4576 - auc: 0.8557 - accuracy: 0.7613 - val_loss: 0.4795 - val_auc: 0.8326 - val_accuracy: 0.7622\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4736 - auc: 0.8387 - accuracy: 0.7577 - val_loss: 0.4795 - val_auc: 0.8326 - val_accuracy: 0.7622\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4760 - auc: 0.8280 - accuracy: 0.7562 - val_loss: 0.4795 - val_auc: 0.8324 - val_accuracy: 0.7622\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8481 - accuracy: 0.7645 - val_loss: 0.4795 - val_auc: 0.8323 - val_accuracy: 0.7622\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4682 - auc: 0.8414 - accuracy: 0.7652 - val_loss: 0.4796 - val_auc: 0.8318 - val_accuracy: 0.7622\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4649 - auc: 0.8431 - accuracy: 0.7597 - val_loss: 0.4795 - val_auc: 0.8320 - val_accuracy: 0.7622\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4601 - auc: 0.8524 - accuracy: 0.7708 - val_loss: 0.4795 - val_auc: 0.8320 - val_accuracy: 0.7622\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4582 - auc: 0.8522 - accuracy: 0.7665 - val_loss: 0.4795 - val_auc: 0.8325 - val_accuracy: 0.7622\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4429 - auc: 0.8646 - accuracy: 0.7773 - val_loss: 0.4795 - val_auc: 0.8326 - val_accuracy: 0.7622\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4941 - auc: 0.8268 - accuracy: 0.7469 - val_loss: 0.4795 - val_auc: 0.8326 - val_accuracy: 0.7622\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4642 - auc: 0.8420 - accuracy: 0.7678 - val_loss: 0.4796 - val_auc: 0.8324 - val_accuracy: 0.7622\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4577 - auc: 0.8474 - accuracy: 0.7748 - val_loss: 0.4796 - val_auc: 0.8326 - val_accuracy: 0.7676\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4672 - auc: 0.8381 - accuracy: 0.7662 - val_loss: 0.4796 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4457 - auc: 0.8615 - accuracy: 0.7774 - val_loss: 0.4794 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4473 - auc: 0.8541 - accuracy: 0.7793 - val_loss: 0.4793 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4705 - auc: 0.8475 - accuracy: 0.7532 - val_loss: 0.4794 - val_auc: 0.8325 - val_accuracy: 0.7676\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4806 - auc: 0.8339 - accuracy: 0.7487 - val_loss: 0.4794 - val_auc: 0.8327 - val_accuracy: 0.7676\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4588 - auc: 0.8487 - accuracy: 0.7816 - val_loss: 0.4794 - val_auc: 0.8328 - val_accuracy: 0.7676\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4729 - auc: 0.8298 - accuracy: 0.7488 - val_loss: 0.4795 - val_auc: 0.8329 - val_accuracy: 0.7676\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4894 - auc: 0.8329 - accuracy: 0.7384 - val_loss: 0.4794 - val_auc: 0.8330 - val_accuracy: 0.7676\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8314 - accuracy: 0.7447 - val_loss: 0.4794 - val_auc: 0.8333 - val_accuracy: 0.7676\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4810 - auc: 0.8397 - accuracy: 0.7555 - val_loss: 0.4793 - val_auc: 0.8334 - val_accuracy: 0.7676\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4650 - auc: 0.8487 - accuracy: 0.7654 - val_loss: 0.4792 - val_auc: 0.8335 - val_accuracy: 0.7676\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4824 - auc: 0.8425 - accuracy: 0.7683 - val_loss: 0.4792 - val_auc: 0.8332 - val_accuracy: 0.7676\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4576 - auc: 0.8533 - accuracy: 0.7673 - val_loss: 0.4793 - val_auc: 0.8332 - val_accuracy: 0.7676\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4618 - auc: 0.8447 - accuracy: 0.7675 - val_loss: 0.4794 - val_auc: 0.8333 - val_accuracy: 0.7676\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4578 - auc: 0.8538 - accuracy: 0.7729 - val_loss: 0.4793 - val_auc: 0.8331 - val_accuracy: 0.7676\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4445 - auc: 0.8596 - accuracy: 0.7858 - val_loss: 0.4792 - val_auc: 0.8331 - val_accuracy: 0.7622\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4443 - auc: 0.8542 - accuracy: 0.7740 - val_loss: 0.4792 - val_auc: 0.8335 - val_accuracy: 0.7676\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5069 - auc: 0.8130 - accuracy: 0.7426 - val_loss: 0.4792 - val_auc: 0.8333 - val_accuracy: 0.7730\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4936 - auc: 0.8182 - accuracy: 0.7525 - val_loss: 0.4791 - val_auc: 0.8335 - val_accuracy: 0.7730\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4485 - auc: 0.8615 - accuracy: 0.7720 - val_loss: 0.4790 - val_auc: 0.8333 - val_accuracy: 0.7730\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4326 - auc: 0.8675 - accuracy: 0.7984 - val_loss: 0.4790 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4538 - auc: 0.8567 - accuracy: 0.7737 - val_loss: 0.4789 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4552 - auc: 0.8532 - accuracy: 0.7776 - val_loss: 0.4789 - val_auc: 0.8336 - val_accuracy: 0.7730\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4529 - auc: 0.8480 - accuracy: 0.7819 - val_loss: 0.4790 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4766 - auc: 0.8351 - accuracy: 0.7685 - val_loss: 0.4790 - val_auc: 0.8337 - val_accuracy: 0.7730\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4573 - auc: 0.8417 - accuracy: 0.7747 - val_loss: 0.4789 - val_auc: 0.8340 - val_accuracy: 0.7730\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4685 - auc: 0.8507 - accuracy: 0.7694 - val_loss: 0.4790 - val_auc: 0.8340 - val_accuracy: 0.7784\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4481 - auc: 0.8523 - accuracy: 0.7933 - val_loss: 0.4788 - val_auc: 0.8340 - val_accuracy: 0.7784\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8494 - accuracy: 0.7577 - val_loss: 0.4788 - val_auc: 0.8337 - val_accuracy: 0.7784\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4597 - auc: 0.8507 - accuracy: 0.7681 - val_loss: 0.4788 - val_auc: 0.8339 - val_accuracy: 0.7730\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4677 - auc: 0.8436 - accuracy: 0.7702 - val_loss: 0.4788 - val_auc: 0.8339 - val_accuracy: 0.7784\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4597 - auc: 0.8462 - accuracy: 0.7626 - val_loss: 0.4788 - val_auc: 0.8339 - val_accuracy: 0.7784\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4457 - auc: 0.8660 - accuracy: 0.7764 - val_loss: 0.4789 - val_auc: 0.8338 - val_accuracy: 0.7784\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4427 - auc: 0.8523 - accuracy: 0.7938 - val_loss: 0.4789 - val_auc: 0.8340 - val_accuracy: 0.7784\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4402 - auc: 0.8585 - accuracy: 0.7826 - val_loss: 0.4789 - val_auc: 0.8337 - val_accuracy: 0.7784\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4427 - auc: 0.8631 - accuracy: 0.7949 - val_loss: 0.4788 - val_auc: 0.8341 - val_accuracy: 0.7784\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4562 - auc: 0.8595 - accuracy: 0.7712 - val_loss: 0.4787 - val_auc: 0.8343 - val_accuracy: 0.7730\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4844 - auc: 0.8302 - accuracy: 0.7607 - val_loss: 0.4787 - val_auc: 0.8346 - val_accuracy: 0.7730\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4984 - auc: 0.8339 - accuracy: 0.7377 - val_loss: 0.4787 - val_auc: 0.8344 - val_accuracy: 0.7730\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4423 - auc: 0.8611 - accuracy: 0.7884 - val_loss: 0.4787 - val_auc: 0.8344 - val_accuracy: 0.7730\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4578 - auc: 0.8445 - accuracy: 0.7685 - val_loss: 0.4786 - val_auc: 0.8344 - val_accuracy: 0.7730\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4447 - auc: 0.8670 - accuracy: 0.7856 - val_loss: 0.4786 - val_auc: 0.8346 - val_accuracy: 0.7676\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4515 - auc: 0.8559 - accuracy: 0.7612 - val_loss: 0.4787 - val_auc: 0.8346 - val_accuracy: 0.7730\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4732 - auc: 0.8412 - accuracy: 0.7533 - val_loss: 0.4786 - val_auc: 0.8346 - val_accuracy: 0.7676\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4456 - auc: 0.8610 - accuracy: 0.7859 - val_loss: 0.4786 - val_auc: 0.8346 - val_accuracy: 0.7676\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4471 - auc: 0.8519 - accuracy: 0.7682 - val_loss: 0.4785 - val_auc: 0.8346 - val_accuracy: 0.7676\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4717 - auc: 0.8397 - accuracy: 0.7722 - val_loss: 0.4786 - val_auc: 0.8350 - val_accuracy: 0.7676\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4488 - auc: 0.8588 - accuracy: 0.7877 - val_loss: 0.4786 - val_auc: 0.8347 - val_accuracy: 0.7676\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4695 - auc: 0.8463 - accuracy: 0.7725 - val_loss: 0.4785 - val_auc: 0.8348 - val_accuracy: 0.7676\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4503 - auc: 0.8453 - accuracy: 0.7785 - val_loss: 0.4786 - val_auc: 0.8346 - val_accuracy: 0.7676\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4380 - auc: 0.8632 - accuracy: 0.7860 - val_loss: 0.4785 - val_auc: 0.8348 - val_accuracy: 0.7676\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4472 - auc: 0.8610 - accuracy: 0.7711 - val_loss: 0.4784 - val_auc: 0.8347 - val_accuracy: 0.7676\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4649 - auc: 0.8519 - accuracy: 0.7875 - val_loss: 0.4783 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4302 - auc: 0.8796 - accuracy: 0.7983 - val_loss: 0.4783 - val_auc: 0.8350 - val_accuracy: 0.7676\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4293 - auc: 0.8703 - accuracy: 0.8122 - val_loss: 0.4783 - val_auc: 0.8351 - val_accuracy: 0.7676\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4575 - auc: 0.8486 - accuracy: 0.7643 - val_loss: 0.4783 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4716 - auc: 0.8483 - accuracy: 0.7445 - val_loss: 0.4783 - val_auc: 0.8351 - val_accuracy: 0.7676\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4693 - auc: 0.8415 - accuracy: 0.7798 - val_loss: 0.4783 - val_auc: 0.8351 - val_accuracy: 0.7676\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4928 - auc: 0.8284 - accuracy: 0.7435 - val_loss: 0.4783 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4562 - auc: 0.8460 - accuracy: 0.7741 - val_loss: 0.4783 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4439 - auc: 0.8601 - accuracy: 0.7730 - val_loss: 0.4783 - val_auc: 0.8353 - val_accuracy: 0.7676\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4666 - auc: 0.8439 - accuracy: 0.7607 - val_loss: 0.4782 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4344 - auc: 0.8582 - accuracy: 0.7870 - val_loss: 0.4782 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4413 - auc: 0.8670 - accuracy: 0.7830 - val_loss: 0.4782 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4340 - auc: 0.8757 - accuracy: 0.7808 - val_loss: 0.4782 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4887 - auc: 0.8350 - accuracy: 0.7596 - val_loss: 0.4782 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4750 - auc: 0.8424 - accuracy: 0.7585 - val_loss: 0.4781 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5006 - auc: 0.8245 - accuracy: 0.7316 - val_loss: 0.4782 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4393 - auc: 0.8333 - accuracy: 0.75 - 0s 4ms/step - loss: 0.4475 - auc: 0.8520 - accuracy: 0.7658 - val_loss: 0.4782 - val_auc: 0.8353 - val_accuracy: 0.7676\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4997 - auc: 0.8266 - accuracy: 0.7412 - val_loss: 0.4782 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4623 - auc: 0.8541 - accuracy: 0.7682 - val_loss: 0.4781 - val_auc: 0.8354 - val_accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295b7aa2460>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# Training model\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=1, callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cardiovascular-invitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dbb2bcca9c4c0534\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dbb2bcca9c4c0534\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TensorBoard launch\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "crucial-border",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5015 - auc: 0.8086 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "model = load_model(mc_path)\n",
    "eval = model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-valuable",
   "metadata": {},
   "source": [
    "# 7. Elección del umbral usando f2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-retreat",
   "metadata": {},
   "source": [
    "A la prueba anterior, se suma la selección del umbral (o **threshold**) con el cual el clasificador discrimina entre clases. El mejor umbral de clasificación se calcula para todos los modelos, después del correspondiente entrenamiento. Para esta elección se elije el mejor valor del f2-score sobre el subset de **valid**. También se muestra la evolución de esta métrica respecto al umbral en el subset de **train**. En teoría, este umbral **no modifica la mérica principal del modelo, que es el área bajo la curva ROC**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "waiting-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def round_threshold(vector, threshold=0.5):\n",
    "    rounded_vector = []\n",
    "    for element in vector:\n",
    "        if element >= threshold:\n",
    "            rounded_vector.append(1)\n",
    "        else:\n",
    "            rounded_vector.append(0)\n",
    "            \n",
    "    return np.array(rounded_vector)\n",
    "        \n",
    "def f2_threshold_selection(y_probs_valid, y_true_valid, y_probs_train, y_true_train, steps=100, plot=True):\n",
    "    # Thresholds and f2-score vectors\n",
    "    thresholds = np.linspace(0, 1, steps)\n",
    "    f2_score_valid = []\n",
    "    f2_score_train = []\n",
    "    \n",
    "    for thld in thresholds:\n",
    "        # Generate predictions with current threshold\n",
    "        y_pred_valid = round_threshold(vector=y_probs_valid, threshold=thld)\n",
    "        y_pred_train = round_threshold(vector=y_probs_train, threshold=thld)\n",
    "        # Compute f2 score for that threshold and append\n",
    "        score_valid = fbeta_score(y_true=y_true_valid, y_pred=y_pred_valid, beta=2)\n",
    "        score_train = fbeta_score(y_true=y_true_train, y_pred=y_pred_train, beta=2)\n",
    "        f2_score_valid.append(score_valid)\n",
    "        f2_score_train.append(score_train)\n",
    "    \n",
    "    idx = np.argmax(f2_score_valid)\n",
    "    if plot == True:\n",
    "        plt.plot(thresholds, f2_score_valid, label='valid')\n",
    "        plt.plot(thresholds, f2_score_train, label='train')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('F2 score')\n",
    "        plt.axvline(thresholds[idx], color='black', linestyle='--')\n",
    "        plt.xlim([0,1])\n",
    "        plt.ylim([0,1])\n",
    "        plt.grid(b=True)\n",
    "        plt.title('Selecting best threshold')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    return thresholds, f2_score_valid, idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "growing-public",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best f2-score for valid is 0.8082 @ threshold = 0.1919\n"
     ]
    }
   ],
   "source": [
    "# Get binary-class probability from model\n",
    "y_probs_valid = model.predict(x_valid)\n",
    "y_probs_train = model.predict(x_train)\n",
    "thresholds, f2_score, idx = f2_threshold_selection(y_probs_valid, y_valid, y_probs_train, y_train, steps=100)\n",
    "print(f'Best f2-score for valid is {f2_score[idx]:.4f} @ threshold = {thresholds[idx]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-sleep",
   "metadata": {},
   "source": [
    "# 8. Early Stopping\n",
    "Habiendo concluido el test #1, se cree necesario agregar un callback de early stopping al modelo. Este callback deberá detener el proceso de aprendizaje en el momento en el que la **métrica principal** del modelo **deje de aumentar**. Posteriormente, se recupera el modelo con mejor performance en cuanto a esta métrica (AUC). Cabe aclarar que esta técnica es especialmente útil cuando la métrica principal no es diferenciable, y por ende se debe emplear una **loss subrogada** (en este caso, la binary cross entropy). De esta forma, el número de epochs que recorra el proceso de entrenamiento se verá limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "distinct-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Early Stopping callback from keras.\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "expressed-disney",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "14/14 [==============================] - 3s 168ms/step - loss: 0.7647 - auc: 0.5472 - accuracy: 0.5198 - val_loss: 0.7100 - val_auc: 0.5922 - val_accuracy: 0.5892\n",
      "Epoch 2/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7725 - auc: 0.5375 - accuracy: 0.5166 - val_loss: 0.6865 - val_auc: 0.6169 - val_accuracy: 0.5946\n",
      "Epoch 3/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7160 - auc: 0.5899 - accuracy: 0.5469 - val_loss: 0.6651 - val_auc: 0.6421 - val_accuracy: 0.6270\n",
      "Epoch 4/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.7085 - auc: 0.5927 - accuracy: 0.5706 - val_loss: 0.6467 - val_auc: 0.6664 - val_accuracy: 0.6324\n",
      "Epoch 5/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6794 - auc: 0.6340 - accuracy: 0.6073 - val_loss: 0.6301 - val_auc: 0.6888 - val_accuracy: 0.6378\n",
      "Epoch 6/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6954 - auc: 0.6386 - accuracy: 0.6023 - val_loss: 0.6151 - val_auc: 0.7073 - val_accuracy: 0.6541\n",
      "Epoch 7/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6823 - auc: 0.6412 - accuracy: 0.5965 - val_loss: 0.6023 - val_auc: 0.7250 - val_accuracy: 0.6919\n",
      "Epoch 8/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6304 - auc: 0.6988 - accuracy: 0.6417 - val_loss: 0.5912 - val_auc: 0.7397 - val_accuracy: 0.7081\n",
      "Epoch 9/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6069 - auc: 0.7068 - accuracy: 0.6592 - val_loss: 0.5814 - val_auc: 0.7515 - val_accuracy: 0.7189\n",
      "Epoch 10/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.6096 - auc: 0.7285 - accuracy: 0.6716 - val_loss: 0.5722 - val_auc: 0.7643 - val_accuracy: 0.7243\n",
      "Epoch 11/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5987 - auc: 0.7290 - accuracy: 0.6523 - val_loss: 0.5641 - val_auc: 0.7746 - val_accuracy: 0.7459\n",
      "Epoch 12/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5902 - auc: 0.7400 - accuracy: 0.6767 - val_loss: 0.5569 - val_auc: 0.7799 - val_accuracy: 0.7568\n",
      "Epoch 13/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5819 - auc: 0.7471 - accuracy: 0.6807 - val_loss: 0.5508 - val_auc: 0.7850 - val_accuracy: 0.7568\n",
      "Epoch 14/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5714 - auc: 0.7690 - accuracy: 0.6993 - val_loss: 0.5451 - val_auc: 0.7918 - val_accuracy: 0.7568\n",
      "Epoch 15/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5659 - auc: 0.7692 - accuracy: 0.7108 - val_loss: 0.5400 - val_auc: 0.7948 - val_accuracy: 0.7622\n",
      "Epoch 16/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5498 - auc: 0.7912 - accuracy: 0.7080 - val_loss: 0.5353 - val_auc: 0.7992 - val_accuracy: 0.7568\n",
      "Epoch 17/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5649 - auc: 0.7766 - accuracy: 0.7247 - val_loss: 0.5310 - val_auc: 0.8031 - val_accuracy: 0.7622\n",
      "Epoch 18/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5478 - auc: 0.7885 - accuracy: 0.7416 - val_loss: 0.5271 - val_auc: 0.8060 - val_accuracy: 0.7622\n",
      "Epoch 19/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5359 - auc: 0.7918 - accuracy: 0.7153 - val_loss: 0.5237 - val_auc: 0.8091 - val_accuracy: 0.7568\n",
      "Epoch 20/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5515 - auc: 0.7809 - accuracy: 0.7018 - val_loss: 0.5207 - val_auc: 0.8114 - val_accuracy: 0.7568\n",
      "Epoch 21/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5381 - auc: 0.7923 - accuracy: 0.6985 - val_loss: 0.5179 - val_auc: 0.8129 - val_accuracy: 0.7622\n",
      "Epoch 22/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5542 - auc: 0.7869 - accuracy: 0.6987 - val_loss: 0.5153 - val_auc: 0.8150 - val_accuracy: 0.7676\n",
      "Epoch 23/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5344 - auc: 0.7955 - accuracy: 0.7100 - val_loss: 0.5128 - val_auc: 0.8169 - val_accuracy: 0.7676\n",
      "Epoch 24/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5216 - auc: 0.8078 - accuracy: 0.7466 - val_loss: 0.5106 - val_auc: 0.8182 - val_accuracy: 0.7784\n",
      "Epoch 25/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5101 - auc: 0.8230 - accuracy: 0.7485 - val_loss: 0.5087 - val_auc: 0.8192 - val_accuracy: 0.7730\n",
      "Epoch 26/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5357 - auc: 0.7932 - accuracy: 0.7281 - val_loss: 0.5068 - val_auc: 0.8185 - val_accuracy: 0.7730\n",
      "Epoch 27/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5454 - auc: 0.7875 - accuracy: 0.7176 - val_loss: 0.5052 - val_auc: 0.8197 - val_accuracy: 0.7730\n",
      "Epoch 28/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4981 - auc: 0.8356 - accuracy: 0.7464 - val_loss: 0.5037 - val_auc: 0.8205 - val_accuracy: 0.7730\n",
      "Epoch 29/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4907 - auc: 0.8381 - accuracy: 0.7621 - val_loss: 0.5022 - val_auc: 0.8216 - val_accuracy: 0.7730\n",
      "Epoch 30/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5065 - auc: 0.8311 - accuracy: 0.7263 - val_loss: 0.5008 - val_auc: 0.8234 - val_accuracy: 0.7622\n",
      "Epoch 31/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5153 - auc: 0.8087 - accuracy: 0.7173 - val_loss: 0.4995 - val_auc: 0.8240 - val_accuracy: 0.7676\n",
      "Epoch 32/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5104 - auc: 0.8203 - accuracy: 0.7351 - val_loss: 0.4984 - val_auc: 0.8251 - val_accuracy: 0.7676\n",
      "Epoch 33/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5066 - auc: 0.8285 - accuracy: 0.7504 - val_loss: 0.4973 - val_auc: 0.8260 - val_accuracy: 0.7676\n",
      "Epoch 34/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5231 - auc: 0.8115 - accuracy: 0.7179 - val_loss: 0.4962 - val_auc: 0.8261 - val_accuracy: 0.7676\n",
      "Epoch 35/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5121 - auc: 0.8039 - accuracy: 0.7422 - val_loss: 0.4952 - val_auc: 0.8264 - val_accuracy: 0.7676\n",
      "Epoch 36/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5059 - auc: 0.8256 - accuracy: 0.7459 - val_loss: 0.4943 - val_auc: 0.8266 - val_accuracy: 0.7676\n",
      "Epoch 37/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5049 - auc: 0.8226 - accuracy: 0.7419 - val_loss: 0.4934 - val_auc: 0.8270 - val_accuracy: 0.7676\n",
      "Epoch 38/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4711 - auc: 0.8451 - accuracy: 0.7675 - val_loss: 0.4926 - val_auc: 0.8270 - val_accuracy: 0.7676\n",
      "Epoch 39/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4996 - auc: 0.8286 - accuracy: 0.7593 - val_loss: 0.4919 - val_auc: 0.8277 - val_accuracy: 0.7730\n",
      "Epoch 40/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4979 - auc: 0.8261 - accuracy: 0.7260 - val_loss: 0.4912 - val_auc: 0.8277 - val_accuracy: 0.7730\n",
      "Epoch 41/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5061 - auc: 0.8136 - accuracy: 0.7194 - val_loss: 0.4905 - val_auc: 0.8283 - val_accuracy: 0.7676\n",
      "Epoch 42/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5054 - auc: 0.8148 - accuracy: 0.7514 - val_loss: 0.4898 - val_auc: 0.8283 - val_accuracy: 0.7676\n",
      "Epoch 43/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4856 - auc: 0.8404 - accuracy: 0.7626 - val_loss: 0.4892 - val_auc: 0.8290 - val_accuracy: 0.7676\n",
      "Epoch 44/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4974 - auc: 0.8229 - accuracy: 0.7363 - val_loss: 0.4886 - val_auc: 0.8299 - val_accuracy: 0.7730\n",
      "Epoch 45/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4727 - auc: 0.8456 - accuracy: 0.7683 - val_loss: 0.4881 - val_auc: 0.8303 - val_accuracy: 0.7730\n",
      "Epoch 46/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4636 - auc: 0.8486 - accuracy: 0.7669 - val_loss: 0.4876 - val_auc: 0.8303 - val_accuracy: 0.7784\n",
      "Epoch 47/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4882 - auc: 0.8369 - accuracy: 0.7545 - val_loss: 0.4871 - val_auc: 0.8309 - val_accuracy: 0.7784\n",
      "Epoch 48/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4791 - auc: 0.8400 - accuracy: 0.7461 - val_loss: 0.4867 - val_auc: 0.8307 - val_accuracy: 0.7784\n",
      "Epoch 49/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4754 - auc: 0.8439 - accuracy: 0.7585 - val_loss: 0.4862 - val_auc: 0.8316 - val_accuracy: 0.7784\n",
      "Epoch 50/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4988 - auc: 0.8273 - accuracy: 0.7422 - val_loss: 0.4858 - val_auc: 0.8313 - val_accuracy: 0.7730\n",
      "Epoch 51/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4624 - auc: 0.8529 - accuracy: 0.7719 - val_loss: 0.4852 - val_auc: 0.8320 - val_accuracy: 0.7730\n",
      "Epoch 52/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4764 - auc: 0.8349 - accuracy: 0.7654 - val_loss: 0.4849 - val_auc: 0.8322 - val_accuracy: 0.7730\n",
      "Epoch 53/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4649 - auc: 0.8552 - accuracy: 0.7877 - val_loss: 0.4846 - val_auc: 0.8323 - val_accuracy: 0.7730\n",
      "Epoch 54/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4546 - auc: 0.8613 - accuracy: 0.7786 - val_loss: 0.4843 - val_auc: 0.8320 - val_accuracy: 0.7784\n",
      "Epoch 55/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4433 - auc: 0.8741 - accuracy: 0.7993 - val_loss: 0.4840 - val_auc: 0.8326 - val_accuracy: 0.7784\n",
      "Epoch 56/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4826 - auc: 0.8384 - accuracy: 0.7751 - val_loss: 0.4837 - val_auc: 0.8331 - val_accuracy: 0.7784\n",
      "Epoch 57/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4563 - auc: 0.8541 - accuracy: 0.7753 - val_loss: 0.4834 - val_auc: 0.8331 - val_accuracy: 0.7784\n",
      "Epoch 58/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4899 - auc: 0.8355 - accuracy: 0.7479 - val_loss: 0.4831 - val_auc: 0.8326 - val_accuracy: 0.7730\n",
      "Epoch 59/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4726 - auc: 0.8408 - accuracy: 0.7560 - val_loss: 0.4829 - val_auc: 0.8328 - val_accuracy: 0.7730\n",
      "Epoch 60/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4821 - auc: 0.8382 - accuracy: 0.7613 - val_loss: 0.4826 - val_auc: 0.8326 - val_accuracy: 0.7730\n",
      "Epoch 61/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4576 - auc: 0.8572 - accuracy: 0.7695 - val_loss: 0.4824 - val_auc: 0.8329 - val_accuracy: 0.7730\n",
      "Epoch 62/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4439 - auc: 0.8592 - accuracy: 0.7773 - val_loss: 0.4821 - val_auc: 0.8325 - val_accuracy: 0.7730\n",
      "Epoch 63/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4419 - auc: 0.8683 - accuracy: 0.7899 - val_loss: 0.4819 - val_auc: 0.8324 - val_accuracy: 0.7730\n",
      "Epoch 64/1000\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4830 - auc: 0.8358 - accuracy: 0.7374 - val_loss: 0.4817 - val_auc: 0.8328 - val_accuracy: 0.7730\n",
      "Epoch 65/1000\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4759 - auc: 0.8380 - accuracy: 0.7400 - val_loss: 0.4816 - val_auc: 0.8325 - val_accuracy: 0.7730\n",
      "Epoch 66/1000\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4623 - auc: 0.8578 - accuracy: 0.7626 - val_loss: 0.4814 - val_auc: 0.8326 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295baa3d8e0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure Early Stopping callback\n",
    "es_callback = EarlyStopping(monitor='val_auc', mode='max', min_delta=0.001, patience=10, restore_best_weights=True)\n",
    "\n",
    "# Define Model Checkpoint callback\n",
    "mc_path = 'model_checkpoints/early_stopping_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Creating new model\n",
    "es_model = Sequential()\n",
    "es_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Compiling model\n",
    "es_model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/ES/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Training model\n",
    "es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "practical-outside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5164 - auc: 0.8025 - accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "# Evaluate test subset and predict.\n",
    "es_model = load_model(mc_path)\n",
    "eval = es_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-alarm",
   "metadata": {},
   "source": [
    "# 9. Learning Rate Scheduling\n",
    "En este apartado se prueba la opción de Learning Rate Scheduling. Esta se encarga de aplicarle una función al Learning Rate entre epochs, de forma tal de encontrar el mínimo de la loss de forma más rápida, y apuntando a evitar mínimos locales y, por ende, overfitting. Se sigue aplicando el concepto de **early stopping** para la AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "primary-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.optimizers.schedules import ExponentialDecay, PolynomialDecay # API in https://keras.io/api/optimizers/learning_rate_schedules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "nervous-competition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 163ms/step - loss: 0.6224 - auc: 0.7829 - accuracy: 0.6984 - val_loss: 0.6120 - val_auc: 0.7671 - val_accuracy: 0.6703\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5980 - auc: 0.7967 - accuracy: 0.7095 - val_loss: 0.5726 - val_auc: 0.7831 - val_accuracy: 0.6973\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5401 - auc: 0.8065 - accuracy: 0.7139 - val_loss: 0.5437 - val_auc: 0.7937 - val_accuracy: 0.7297\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.5004 - auc: 0.8331 - accuracy: 0.7408 - val_loss: 0.5218 - val_auc: 0.8028 - val_accuracy: 0.7351\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4917 - auc: 0.8315 - accuracy: 0.7564 - val_loss: 0.5097 - val_auc: 0.8097 - val_accuracy: 0.7351\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4913 - auc: 0.8281 - accuracy: 0.7281 - val_loss: 0.5005 - val_auc: 0.8141 - val_accuracy: 0.7405\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4949 - auc: 0.8355 - accuracy: 0.7601 - val_loss: 0.4931 - val_auc: 0.8193 - val_accuracy: 0.7459\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4992 - auc: 0.8245 - accuracy: 0.7518 - val_loss: 0.4871 - val_auc: 0.8240 - val_accuracy: 0.7514\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4777 - auc: 0.8412 - accuracy: 0.7688 - val_loss: 0.4848 - val_auc: 0.8254 - val_accuracy: 0.7622\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4740 - auc: 0.8449 - accuracy: 0.7796 - val_loss: 0.4826 - val_auc: 0.8274 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4512 - auc: 0.8603 - accuracy: 0.7738 - val_loss: 0.4804 - val_auc: 0.8302 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4665 - auc: 0.8537 - accuracy: 0.7656 - val_loss: 0.4797 - val_auc: 0.8313 - val_accuracy: 0.7676\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4671 - auc: 0.8505 - accuracy: 0.7590 - val_loss: 0.4787 - val_auc: 0.8324 - val_accuracy: 0.7676\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4942 - auc: 0.8262 - accuracy: 0.7541 - val_loss: 0.4769 - val_auc: 0.8346 - val_accuracy: 0.7622\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4740 - auc: 0.8458 - accuracy: 0.7638 - val_loss: 0.4782 - val_auc: 0.8337 - val_accuracy: 0.7568\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4421 - auc: 0.8666 - accuracy: 0.7719 - val_loss: 0.4771 - val_auc: 0.8348 - val_accuracy: 0.7568\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4587 - auc: 0.8541 - accuracy: 0.7665 - val_loss: 0.4774 - val_auc: 0.8336 - val_accuracy: 0.7568\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4464 - auc: 0.8670 - accuracy: 0.7687 - val_loss: 0.4767 - val_auc: 0.8355 - val_accuracy: 0.7622\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4475 - auc: 0.8548 - accuracy: 0.7764 - val_loss: 0.4777 - val_auc: 0.8351 - val_accuracy: 0.7568\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4717 - auc: 0.8393 - accuracy: 0.7756 - val_loss: 0.4775 - val_auc: 0.8358 - val_accuracy: 0.7622\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4675 - auc: 0.8471 - accuracy: 0.7622 - val_loss: 0.4774 - val_auc: 0.8367 - val_accuracy: 0.7622\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4528 - auc: 0.8543 - accuracy: 0.8048 - val_loss: 0.4783 - val_auc: 0.8362 - val_accuracy: 0.7568\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4738 - auc: 0.8352 - accuracy: 0.7791 - val_loss: 0.4779 - val_auc: 0.8366 - val_accuracy: 0.7676\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4484 - auc: 0.8590 - accuracy: 0.7858 - val_loss: 0.4786 - val_auc: 0.8355 - val_accuracy: 0.7676\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4732 - auc: 0.8325 - accuracy: 0.7726 - val_loss: 0.4786 - val_auc: 0.8354 - val_accuracy: 0.7676\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4324 - auc: 0.8652 - accuracy: 0.7899 - val_loss: 0.4769 - val_auc: 0.8355 - val_accuracy: 0.7730\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4117 - auc: 0.8760 - accuracy: 0.8066 - val_loss: 0.4763 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4289 - auc: 0.8696 - accuracy: 0.7929 - val_loss: 0.4759 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4868 - auc: 0.8281 - accuracy: 0.7652 - val_loss: 0.4769 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4646 - auc: 0.8391 - accuracy: 0.7695 - val_loss: 0.4769 - val_auc: 0.8371 - val_accuracy: 0.7676\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4493 - auc: 0.8575 - accuracy: 0.7853 - val_loss: 0.4772 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4426 - auc: 0.8547 - accuracy: 0.7918 - val_loss: 0.4780 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4598 - auc: 0.8493 - accuracy: 0.7836 - val_loss: 0.4788 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4447 - auc: 0.8557 - accuracy: 0.7945 - val_loss: 0.4792 - val_auc: 0.8365 - val_accuracy: 0.7784\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4238 - auc: 0.8693 - accuracy: 0.7910 - val_loss: 0.4785 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4945 - auc: 0.8238 - accuracy: 0.7439 - val_loss: 0.4792 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4455 - auc: 0.8572 - accuracy: 0.7881 - val_loss: 0.4798 - val_auc: 0.8361 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295bbf88c40>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/lrs_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/LRS/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model\n",
    "lrs_model = Sequential()\n",
    "lrs_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "\n",
    "# Define learning rate at start\n",
    "ilr = 0.1\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=100000, decay_rate=0.96, staircase=False) # Decay every (decay_steps) steps with a base of (decay_rate).\n",
    "\n",
    "# Compiling model\n",
    "lrs_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rough-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5011 - auc: 0.8077 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-payday",
   "metadata": {},
   "source": [
    "**PREGUNTA**: ¿Exponential Decay se lleva bien con Early Stopping?, ya que si reduzco el learning rate \"me muevo menos\", con lo cual el callback de Early Stopping cortaría prematuramente. Ahora probamos sin Early Stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "decreased-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 139ms/step - loss: 0.4608 - auc: 0.8489 - accuracy: 0.7809 - val_loss: 0.4769 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4771 - val_auc: 0.8368 - val_accuracy: 0.7622\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4765 - val_auc: 0.8375 - val_accuracy: 0.7676\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8502 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8378 - val_accuracy: 0.7676\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8489 - accuracy: 0.7786 - val_loss: 0.4766 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7879 - val_loss: 0.4758 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4755 - val_auc: 0.8389 - val_accuracy: 0.7838\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4760 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4783 - val_auc: 0.8369 - val_accuracy: 0.7838\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8493 - accuracy: 0.7739 - val_loss: 0.4781 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8499 - accuracy: 0.7786 - val_loss: 0.4797 - val_auc: 0.8361 - val_accuracy: 0.7784\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8488 - accuracy: 0.7692 - val_loss: 0.4796 - val_auc: 0.8365 - val_accuracy: 0.7784\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8502 - accuracy: 0.7692 - val_loss: 0.4781 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4780 - val_auc: 0.8376 - val_accuracy: 0.7838\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4772 - val_auc: 0.8376 - val_accuracy: 0.7838\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4787 - val_auc: 0.8378 - val_accuracy: 0.7838\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4781 - val_auc: 0.8380 - val_accuracy: 0.7838\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4780 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4784 - val_auc: 0.8379 - val_accuracy: 0.7892\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7739 - val_loss: 0.4790 - val_auc: 0.8372 - val_accuracy: 0.7838\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8491 - accuracy: 0.7739 - val_loss: 0.4776 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7739 - val_loss: 0.4767 - val_auc: 0.8385 - val_accuracy: 0.7730\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4774 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4616 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4772 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4777 - val_auc: 0.8379 - val_accuracy: 0.7730\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4781 - val_auc: 0.8383 - val_accuracy: 0.7838\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8378 - val_accuracy: 0.7838\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4772 - val_auc: 0.8378 - val_accuracy: 0.7838\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7739 - val_loss: 0.4778 - val_auc: 0.8383 - val_accuracy: 0.7838\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8496 - accuracy: 0.7739 - val_loss: 0.4791 - val_auc: 0.8389 - val_accuracy: 0.7784\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4787 - val_auc: 0.8384 - val_accuracy: 0.7838\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8484 - accuracy: 0.7809 - val_loss: 0.4768 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4772 - val_auc: 0.8385 - val_accuracy: 0.7730\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8486 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8387 - val_accuracy: 0.7730\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4782 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8389 - val_accuracy: 0.7730\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4774 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8497 - accuracy: 0.7692 - val_loss: 0.4778 - val_auc: 0.8388 - val_accuracy: 0.7784\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4790 - val_auc: 0.8389 - val_accuracy: 0.7784\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4787 - val_auc: 0.8375 - val_accuracy: 0.7676\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4783 - val_auc: 0.8378 - val_accuracy: 0.7676\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8499 - accuracy: 0.7739 - val_loss: 0.4783 - val_auc: 0.8381 - val_accuracy: 0.7676\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4780 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4785 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4609 - auc: 0.8489 - accuracy: 0.7786 - val_loss: 0.4785 - val_auc: 0.8377 - val_accuracy: 0.7676\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8502 - accuracy: 0.7739 - val_loss: 0.4776 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4778 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4785 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8485 - accuracy: 0.7739 - val_loss: 0.4786 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8495 - accuracy: 0.7879 - val_loss: 0.4782 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4788 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4790 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7762 - val_loss: 0.4776 - val_auc: 0.8375 - val_accuracy: 0.7676\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4757 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8489 - accuracy: 0.7739 - val_loss: 0.4761 - val_auc: 0.8380 - val_accuracy: 0.7676\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4606 - auc: 0.8496 - accuracy: 0.7809 - val_loss: 0.4768 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4761 - val_auc: 0.8377 - val_accuracy: 0.7784\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4610 - auc: 0.8497 - accuracy: 0.7692 - val_loss: 0.4759 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4766 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4611 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4768 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4608 - auc: 0.8496 - accuracy: 0.7855 - val_loss: 0.4772 - val_auc: 0.8371 - val_accuracy: 0.7730\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4777 - val_auc: 0.8365 - val_accuracy: 0.7676\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4603 - auc: 0.8499 - accuracy: 0.7692 - val_loss: 0.4775 - val_auc: 0.8375 - val_accuracy: 0.7676\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7692 - val_loss: 0.4778 - val_auc: 0.8373 - val_accuracy: 0.7622\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4611 - auc: 0.8487 - accuracy: 0.7739 - val_loss: 0.4774 - val_auc: 0.8376 - val_accuracy: 0.7676\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4768 - val_auc: 0.8387 - val_accuracy: 0.7730\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8488 - accuracy: 0.7832 - val_loss: 0.4771 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4612 - auc: 0.8483 - accuracy: 0.7786 - val_loss: 0.4770 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8382 - val_accuracy: 0.7730\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4790 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4607 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4779 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4779 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7832 - val_loss: 0.4780 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8483 - accuracy: 0.7855 - val_loss: 0.4777 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4761 - val_auc: 0.8386 - val_accuracy: 0.7730\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8484 - accuracy: 0.7786 - val_loss: 0.4766 - val_auc: 0.8386 - val_accuracy: 0.7784\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4760 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4754 - val_auc: 0.8390 - val_accuracy: 0.7784\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4744 - val_auc: 0.8383 - val_accuracy: 0.7730\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4754 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8488 - accuracy: 0.7809 - val_loss: 0.4764 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4772 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4763 - val_auc: 0.8382 - val_accuracy: 0.7730\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4763 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4767 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8498 - accuracy: 0.7786 - val_loss: 0.4767 - val_auc: 0.8385 - val_accuracy: 0.7784\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4779 - val_auc: 0.8376 - val_accuracy: 0.7676\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4611 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4782 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8501 - accuracy: 0.7716 - val_loss: 0.4777 - val_auc: 0.8380 - val_accuracy: 0.7676\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8502 - accuracy: 0.7786 - val_loss: 0.4784 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8384 - val_accuracy: 0.7730\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8485 - accuracy: 0.7739 - val_loss: 0.4786 - val_auc: 0.8384 - val_accuracy: 0.7730\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8486 - accuracy: 0.7786 - val_loss: 0.4794 - val_auc: 0.8379 - val_accuracy: 0.7676\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4609 - auc: 0.8493 - accuracy: 0.7716 - val_loss: 0.4786 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4788 - val_auc: 0.8373 - val_accuracy: 0.7676\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8490 - accuracy: 0.7739 - val_loss: 0.4780 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4760 - val_auc: 0.8385 - val_accuracy: 0.7730\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4761 - val_auc: 0.8392 - val_accuracy: 0.7730\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4776 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8498 - accuracy: 0.7716 - val_loss: 0.4766 - val_auc: 0.8383 - val_accuracy: 0.7838\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4769 - val_auc: 0.8390 - val_accuracy: 0.7838\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4775 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4762 - val_auc: 0.8390 - val_accuracy: 0.7784\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4772 - val_auc: 0.8381 - val_accuracy: 0.7892\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4781 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7786 - val_loss: 0.4782 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4616 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4781 - val_auc: 0.8376 - val_accuracy: 0.7676\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8494 - accuracy: 0.7716 - val_loss: 0.4787 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4766 - val_auc: 0.8375 - val_accuracy: 0.7730\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8496 - accuracy: 0.7692 - val_loss: 0.4774 - val_auc: 0.8374 - val_accuracy: 0.7676\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8377 - val_accuracy: 0.7676\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8496 - accuracy: 0.7809 - val_loss: 0.4769 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7716 - val_loss: 0.4759 - val_auc: 0.8390 - val_accuracy: 0.7784\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8495 - accuracy: 0.7832 - val_loss: 0.4755 - val_auc: 0.8393 - val_accuracy: 0.7784\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8496 - accuracy: 0.7786 - val_loss: 0.4764 - val_auc: 0.8384 - val_accuracy: 0.7730\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4615 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4764 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4618 - auc: 0.8492 - accuracy: 0.7832 - val_loss: 0.4760 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7832 - val_loss: 0.4757 - val_auc: 0.8391 - val_accuracy: 0.7784\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7832 - val_loss: 0.4761 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4762 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8487 - accuracy: 0.7809 - val_loss: 0.4777 - val_auc: 0.8383 - val_accuracy: 0.7784\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4778 - val_auc: 0.8370 - val_accuracy: 0.7730\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8490 - accuracy: 0.7809 - val_loss: 0.4780 - val_auc: 0.8376 - val_accuracy: 0.7676\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8488 - accuracy: 0.7786 - val_loss: 0.4786 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8495 - accuracy: 0.7832 - val_loss: 0.4789 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8489 - accuracy: 0.7809 - val_loss: 0.4779 - val_auc: 0.8370 - val_accuracy: 0.7784\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4605 - auc: 0.8496 - accuracy: 0.7762 - val_loss: 0.4791 - val_auc: 0.8369 - val_accuracy: 0.7784\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4794 - val_auc: 0.8365 - val_accuracy: 0.7784\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8489 - accuracy: 0.7832 - val_loss: 0.4802 - val_auc: 0.8365 - val_accuracy: 0.7730\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8495 - accuracy: 0.7762 - val_loss: 0.4798 - val_auc: 0.8361 - val_accuracy: 0.7676\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4798 - val_auc: 0.8363 - val_accuracy: 0.7676\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4615 - auc: 0.8482 - accuracy: 0.7855 - val_loss: 0.4794 - val_auc: 0.8368 - val_accuracy: 0.7730\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4800 - val_auc: 0.8363 - val_accuracy: 0.7730\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4617 - auc: 0.8486 - accuracy: 0.7786 - val_loss: 0.4792 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4803 - val_auc: 0.8366 - val_accuracy: 0.7784\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8487 - accuracy: 0.7762 - val_loss: 0.4777 - val_auc: 0.8373 - val_accuracy: 0.7730\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4774 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8489 - accuracy: 0.7855 - val_loss: 0.4758 - val_auc: 0.8389 - val_accuracy: 0.7784\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8496 - accuracy: 0.7809 - val_loss: 0.4765 - val_auc: 0.8382 - val_accuracy: 0.7784\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4617 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4775 - val_auc: 0.8378 - val_accuracy: 0.7676\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8500 - accuracy: 0.7716 - val_loss: 0.4774 - val_auc: 0.8380 - val_accuracy: 0.7784\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4782 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7786 - val_loss: 0.4779 - val_auc: 0.8380 - val_accuracy: 0.7838\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8487 - accuracy: 0.7786 - val_loss: 0.4767 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8495 - accuracy: 0.7716 - val_loss: 0.4769 - val_auc: 0.8374 - val_accuracy: 0.7784\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8499 - accuracy: 0.7669 - val_loss: 0.4778 - val_auc: 0.8372 - val_accuracy: 0.7676\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4780 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4780 - val_auc: 0.8378 - val_accuracy: 0.7784\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4790 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7669 - val_loss: 0.4782 - val_auc: 0.8375 - val_accuracy: 0.7784\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8490 - accuracy: 0.7832 - val_loss: 0.4783 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8489 - accuracy: 0.7832 - val_loss: 0.4789 - val_auc: 0.8376 - val_accuracy: 0.7784\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8499 - accuracy: 0.7692 - val_loss: 0.4784 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8491 - accuracy: 0.7762 - val_loss: 0.4791 - val_auc: 0.8373 - val_accuracy: 0.7784\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8484 - accuracy: 0.7809 - val_loss: 0.4793 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8494 - accuracy: 0.7855 - val_loss: 0.4790 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8489 - accuracy: 0.7809 - val_loss: 0.4779 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7855 - val_loss: 0.4781 - val_auc: 0.8381 - val_accuracy: 0.7784\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8487 - accuracy: 0.7762 - val_loss: 0.4777 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8490 - accuracy: 0.7786 - val_loss: 0.4781 - val_auc: 0.8367 - val_accuracy: 0.7730\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8492 - accuracy: 0.7809 - val_loss: 0.4788 - val_auc: 0.8361 - val_accuracy: 0.7730\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4795 - val_auc: 0.8364 - val_accuracy: 0.7730\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8491 - accuracy: 0.7739 - val_loss: 0.4790 - val_auc: 0.8363 - val_accuracy: 0.7784\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8499 - accuracy: 0.7786 - val_loss: 0.4785 - val_auc: 0.8363 - val_accuracy: 0.7784\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7786 - val_loss: 0.4784 - val_auc: 0.8367 - val_accuracy: 0.7838\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7832 - val_loss: 0.4786 - val_auc: 0.8379 - val_accuracy: 0.7784\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8492 - accuracy: 0.7716 - val_loss: 0.4789 - val_auc: 0.8368 - val_accuracy: 0.7784\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7832 - val_loss: 0.4789 - val_auc: 0.8369 - val_accuracy: 0.7730\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4790 - val_auc: 0.8359 - val_accuracy: 0.7784\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8494 - accuracy: 0.7809 - val_loss: 0.4790 - val_auc: 0.8357 - val_accuracy: 0.7730\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4795 - val_auc: 0.8359 - val_accuracy: 0.7730\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8497 - accuracy: 0.7762 - val_loss: 0.4804 - val_auc: 0.8357 - val_accuracy: 0.7784\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8493 - accuracy: 0.7809 - val_loss: 0.4805 - val_auc: 0.8354 - val_accuracy: 0.7730\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8499 - accuracy: 0.7739 - val_loss: 0.4800 - val_auc: 0.8358 - val_accuracy: 0.7730\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4779 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8494 - accuracy: 0.7762 - val_loss: 0.4781 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8498 - accuracy: 0.7809 - val_loss: 0.4764 - val_auc: 0.8381 - val_accuracy: 0.7730\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4766 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7786 - val_loss: 0.4770 - val_auc: 0.8373 - val_accuracy: 0.7730\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8488 - accuracy: 0.7762 - val_loss: 0.4768 - val_auc: 0.8376 - val_accuracy: 0.7730\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4776 - val_auc: 0.8374 - val_accuracy: 0.7730\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4606 - auc: 0.8496 - accuracy: 0.7832 - val_loss: 0.4772 - val_auc: 0.8380 - val_accuracy: 0.7730\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8492 - accuracy: 0.7762 - val_loss: 0.4763 - val_auc: 0.8387 - val_accuracy: 0.7784\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8491 - accuracy: 0.7716 - val_loss: 0.4765 - val_auc: 0.8379 - val_accuracy: 0.7730\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4608 - auc: 0.8495 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8373 - val_accuracy: 0.7676\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8492 - accuracy: 0.7692 - val_loss: 0.4786 - val_auc: 0.8372 - val_accuracy: 0.7730\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8491 - accuracy: 0.7809 - val_loss: 0.4783 - val_auc: 0.8372 - val_accuracy: 0.7784\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4611 - auc: 0.8495 - accuracy: 0.7692 - val_loss: 0.4779 - val_auc: 0.8370 - val_accuracy: 0.7784\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4610 - auc: 0.8498 - accuracy: 0.7692 - val_loss: 0.4784 - val_auc: 0.8365 - val_accuracy: 0.7784\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8495 - accuracy: 0.7716 - val_loss: 0.4789 - val_auc: 0.8373 - val_accuracy: 0.7730\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4614 - auc: 0.8490 - accuracy: 0.7739 - val_loss: 0.4796 - val_auc: 0.8363 - val_accuracy: 0.7784\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4609 - auc: 0.8494 - accuracy: 0.7786 - val_loss: 0.4785 - val_auc: 0.8364 - val_accuracy: 0.7784\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4617 - auc: 0.8483 - accuracy: 0.7762 - val_loss: 0.4780 - val_auc: 0.8371 - val_accuracy: 0.7676\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4613 - auc: 0.8493 - accuracy: 0.7786 - val_loss: 0.4776 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4612 - auc: 0.8493 - accuracy: 0.7739 - val_loss: 0.4780 - val_auc: 0.8370 - val_accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295bc45e130>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/LRS/noES\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Training model\n",
    "lrs_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "driving-afternoon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5034 - auc: 0.8052 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model with test subset.\n",
    "lrs_model = load_model(mc_path)\n",
    "eval = lrs_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-advertiser",
   "metadata": {},
   "source": [
    "# 10. Regularización\n",
    "La idea de la regularización es la de limitar aquellos pesos que son altos. De esta forma, se agrega una capa previa a la capa densa que contiene la capa de regularización. Se probarán dos regularizaciones distintas: L1 y L2 (donde el número significa el grado del término adicional que se suma a la función de costo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-three",
   "metadata": {},
   "source": [
    "# 10.1. Regularización L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "described-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1, l2, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "floating-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 175ms/step - loss: 0.8347 - auc: 0.5452 - accuracy: 0.5279 - val_loss: 0.6615 - val_auc: 0.6982 - val_accuracy: 0.6865\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6762 - auc: 0.6890 - accuracy: 0.6443 - val_loss: 0.5788 - val_auc: 0.7695 - val_accuracy: 0.7676\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5726 - auc: 0.7813 - accuracy: 0.6945 - val_loss: 0.5410 - val_auc: 0.8028 - val_accuracy: 0.7622\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5334 - auc: 0.8179 - accuracy: 0.7258 - val_loss: 0.5220 - val_auc: 0.8197 - val_accuracy: 0.7676\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5224 - auc: 0.8191 - accuracy: 0.7337 - val_loss: 0.5129 - val_auc: 0.8255 - val_accuracy: 0.7676\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4972 - auc: 0.8427 - accuracy: 0.7501 - val_loss: 0.5083 - val_auc: 0.8294 - val_accuracy: 0.7676\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4918 - auc: 0.8395 - accuracy: 0.7785 - val_loss: 0.5046 - val_auc: 0.8342 - val_accuracy: 0.7784\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4876 - auc: 0.8502 - accuracy: 0.7656 - val_loss: 0.5026 - val_auc: 0.8355 - val_accuracy: 0.7892\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4941 - auc: 0.8394 - accuracy: 0.7820 - val_loss: 0.5012 - val_auc: 0.8357 - val_accuracy: 0.7838\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4867 - auc: 0.8542 - accuracy: 0.7754 - val_loss: 0.4998 - val_auc: 0.8368 - val_accuracy: 0.7838\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5051 - auc: 0.8332 - accuracy: 0.7486 - val_loss: 0.4989 - val_auc: 0.8364 - val_accuracy: 0.7838\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4850 - auc: 0.8498 - accuracy: 0.7619 - val_loss: 0.4994 - val_auc: 0.8367 - val_accuracy: 0.7784\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5041 - auc: 0.8370 - accuracy: 0.7777 - val_loss: 0.4977 - val_auc: 0.8382 - val_accuracy: 0.7892\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4946 - auc: 0.8501 - accuracy: 0.7714 - val_loss: 0.4968 - val_auc: 0.8389 - val_accuracy: 0.7838\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4840 - auc: 0.8518 - accuracy: 0.7904 - val_loss: 0.4964 - val_auc: 0.8391 - val_accuracy: 0.7838\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4843 - auc: 0.8431 - accuracy: 0.7687 - val_loss: 0.4961 - val_auc: 0.8385 - val_accuracy: 0.7838\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5069 - auc: 0.8287 - accuracy: 0.7577 - val_loss: 0.4957 - val_auc: 0.8391 - val_accuracy: 0.7838\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4796 - auc: 0.8466 - accuracy: 0.7739 - val_loss: 0.4963 - val_auc: 0.8393 - val_accuracy: 0.7784\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4663 - auc: 0.8567 - accuracy: 0.7798 - val_loss: 0.4974 - val_auc: 0.8386 - val_accuracy: 0.7838\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4862 - auc: 0.8524 - accuracy: 0.7759 - val_loss: 0.4976 - val_auc: 0.8399 - val_accuracy: 0.7838\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4977 - auc: 0.8344 - accuracy: 0.7655 - val_loss: 0.4983 - val_auc: 0.8390 - val_accuracy: 0.7892\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4983 - auc: 0.8548 - accuracy: 0.7775 - val_loss: 0.4977 - val_auc: 0.8389 - val_accuracy: 0.7838\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5185 - auc: 0.8257 - accuracy: 0.7599 - val_loss: 0.4980 - val_auc: 0.8383 - val_accuracy: 0.7838\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4639 - auc: 0.8633 - accuracy: 0.7955 - val_loss: 0.4976 - val_auc: 0.8402 - val_accuracy: 0.7730\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4684 - auc: 0.8610 - accuracy: 0.7849 - val_loss: 0.4986 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4834 - auc: 0.8442 - accuracy: 0.7787 - val_loss: 0.4990 - val_auc: 0.8377 - val_accuracy: 0.7730\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4748 - auc: 0.8553 - accuracy: 0.7697 - val_loss: 0.4996 - val_auc: 0.8378 - val_accuracy: 0.7730\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4836 - auc: 0.8466 - accuracy: 0.7861 - val_loss: 0.5001 - val_auc: 0.8374 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295be45ae50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/L1/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model for L1 Regularization\n",
    "l1_model = Sequential()\n",
    "\n",
    "# Adding dense layer to model\n",
    "l1_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, kernel_regularizer=l1(0.01)))\n",
    "\n",
    "# Compiling model\n",
    "l1_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "l1_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-tyler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "coupled-municipality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5194 - auc: 0.8086 - accuracy: 0.7403\n"
     ]
    }
   ],
   "source": [
    "l1_model = load_model(mc_path)\n",
    "eval = l1_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-causing",
   "metadata": {},
   "source": [
    "# 10.2. Regularización L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "specified-probability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 3s 136ms/step - loss: 0.6484 - auc: 0.7321 - accuracy: 0.6538 - val_loss: 0.6432 - val_auc: 0.7348 - val_accuracy: 0.6865\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5523 - auc: 0.7964 - accuracy: 0.7234 - val_loss: 0.5861 - val_auc: 0.7672 - val_accuracy: 0.6811\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5190 - auc: 0.8249 - accuracy: 0.7460 - val_loss: 0.5492 - val_auc: 0.7885 - val_accuracy: 0.7189\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.5009 - auc: 0.8363 - accuracy: 0.7408 - val_loss: 0.5316 - val_auc: 0.8032 - val_accuracy: 0.7351\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4687 - auc: 0.8654 - accuracy: 0.7725 - val_loss: 0.5198 - val_auc: 0.8108 - val_accuracy: 0.7351\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4804 - auc: 0.8552 - accuracy: 0.7687 - val_loss: 0.5139 - val_auc: 0.8154 - val_accuracy: 0.7351\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4775 - auc: 0.8469 - accuracy: 0.7647 - val_loss: 0.5100 - val_auc: 0.8192 - val_accuracy: 0.7351\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4894 - auc: 0.8356 - accuracy: 0.7447 - val_loss: 0.5066 - val_auc: 0.8222 - val_accuracy: 0.7405\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4597 - auc: 0.8661 - accuracy: 0.7889 - val_loss: 0.5037 - val_auc: 0.8248 - val_accuracy: 0.7568\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4811 - auc: 0.8372 - accuracy: 0.7688 - val_loss: 0.5020 - val_auc: 0.8262 - val_accuracy: 0.7622\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4959 - auc: 0.8183 - accuracy: 0.7661 - val_loss: 0.5009 - val_auc: 0.8275 - val_accuracy: 0.7730\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4654 - auc: 0.8555 - accuracy: 0.7837 - val_loss: 0.5000 - val_auc: 0.8284 - val_accuracy: 0.7622\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4714 - auc: 0.8546 - accuracy: 0.7770 - val_loss: 0.4987 - val_auc: 0.8288 - val_accuracy: 0.7622\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4740 - auc: 0.8474 - accuracy: 0.7749 - val_loss: 0.4974 - val_auc: 0.8306 - val_accuracy: 0.7514\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4759 - auc: 0.8498 - accuracy: 0.7819 - val_loss: 0.4962 - val_auc: 0.8313 - val_accuracy: 0.7514\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4701 - auc: 0.8612 - accuracy: 0.7814 - val_loss: 0.4951 - val_auc: 0.8333 - val_accuracy: 0.7568\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4975 - auc: 0.8240 - accuracy: 0.7678 - val_loss: 0.4959 - val_auc: 0.8331 - val_accuracy: 0.7568\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4607 - auc: 0.8530 - accuracy: 0.7892 - val_loss: 0.4958 - val_auc: 0.8333 - val_accuracy: 0.7568\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4793 - auc: 0.8466 - accuracy: 0.7609 - val_loss: 0.4958 - val_auc: 0.8324 - val_accuracy: 0.7514\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4790 - auc: 0.8447 - accuracy: 0.7613 - val_loss: 0.4958 - val_auc: 0.8329 - val_accuracy: 0.7514\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4638 - auc: 0.8699 - accuracy: 0.7662 - val_loss: 0.4957 - val_auc: 0.8325 - val_accuracy: 0.7514\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4783 - auc: 0.8484 - accuracy: 0.7568 - val_loss: 0.4959 - val_auc: 0.8324 - val_accuracy: 0.7514\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4777 - auc: 0.8494 - accuracy: 0.7551 - val_loss: 0.4964 - val_auc: 0.8326 - val_accuracy: 0.7514\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4836 - auc: 0.8409 - accuracy: 0.7530 - val_loss: 0.4955 - val_auc: 0.8330 - val_accuracy: 0.7514\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4736 - auc: 0.8521 - accuracy: 0.7762 - val_loss: 0.4956 - val_auc: 0.8327 - val_accuracy: 0.7514\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - ETA: 0s - loss: 0.4390 - auc: 0.8545 - accuracy: 0.75 - 0s 3ms/step - loss: 0.4675 - auc: 0.8507 - accuracy: 0.7601 - val_loss: 0.4955 - val_auc: 0.8329 - val_accuracy: 0.7514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295c0faddf0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/L2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model for L2 Regularization\n",
    "l2_model = Sequential()\n",
    "# Adding dense layer to model\n",
    "l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(0.01)))\n",
    "# Compiling model\n",
    "l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "minor-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5074 - auc: 0.8105 - accuracy: 0.7468\n"
     ]
    }
   ],
   "source": [
    "l2_model = load_model(mc_path)\n",
    "eval = l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-seafood",
   "metadata": {},
   "source": [
    "# 10.3. Regularización L1+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "upset-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 130ms/step - loss: 0.7768 - auc: 0.6111 - accuracy: 0.5464 - val_loss: 0.6366 - val_auc: 0.7490 - val_accuracy: 0.7081\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5900 - auc: 0.7663 - accuracy: 0.6684 - val_loss: 0.5678 - val_auc: 0.7964 - val_accuracy: 0.7351\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5455 - auc: 0.8040 - accuracy: 0.7276 - val_loss: 0.5345 - val_auc: 0.8119 - val_accuracy: 0.7297\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5537 - auc: 0.7938 - accuracy: 0.7266 - val_loss: 0.5176 - val_auc: 0.8250 - val_accuracy: 0.7568\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5072 - auc: 0.8346 - accuracy: 0.7581 - val_loss: 0.5062 - val_auc: 0.8303 - val_accuracy: 0.7622\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5062 - auc: 0.8352 - accuracy: 0.7646 - val_loss: 0.4996 - val_auc: 0.8312 - val_accuracy: 0.7514\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4599 - auc: 0.8611 - accuracy: 0.7872 - val_loss: 0.4950 - val_auc: 0.8339 - val_accuracy: 0.7568\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4989 - auc: 0.8376 - accuracy: 0.7621 - val_loss: 0.4929 - val_auc: 0.8360 - val_accuracy: 0.7676\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4620 - auc: 0.8667 - accuracy: 0.7785 - val_loss: 0.4910 - val_auc: 0.8368 - val_accuracy: 0.7676\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4921 - auc: 0.8402 - accuracy: 0.7663 - val_loss: 0.4914 - val_auc: 0.8358 - val_accuracy: 0.7676\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4474 - auc: 0.8689 - accuracy: 0.7938 - val_loss: 0.4918 - val_auc: 0.8357 - val_accuracy: 0.7676\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4966 - auc: 0.8356 - accuracy: 0.7592 - val_loss: 0.4913 - val_auc: 0.8367 - val_accuracy: 0.7676\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4738 - auc: 0.8560 - accuracy: 0.7718 - val_loss: 0.4914 - val_auc: 0.8362 - val_accuracy: 0.7730\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 0.4677 - auc: 0.8563 - accuracy: 0.7886 - val_loss: 0.4926 - val_auc: 0.8350 - val_accuracy: 0.7676\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 0.4673 - auc: 0.8617 - accuracy: 0.7853 - val_loss: 0.4923 - val_auc: 0.8350 - val_accuracy: 0.7784\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4785 - auc: 0.8498 - accuracy: 0.7709 - val_loss: 0.4921 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4919 - auc: 0.8438 - accuracy: 0.7566 - val_loss: 0.4916 - val_auc: 0.8352 - val_accuracy: 0.7676\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.4955 - auc: 0.8322 - accuracy: 0.7508 - val_loss: 0.4920 - val_auc: 0.8341 - val_accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x295be544670>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/L1+L2_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/L1L2/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating new model for L1 and L2 Regularization\n",
    "l1l2_model = Sequential()\n",
    "\n",
    "# Adding dense layer to model\n",
    "l1l2_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(0.01)))\n",
    "\n",
    "# Compiling model\n",
    "l1l2_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "l1l2_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose='auto', callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "proud-toronto",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step - loss: 0.5156 - auc: 0.8034 - accuracy: 0.7532\n"
     ]
    }
   ],
   "source": [
    "l1l2_model = load_model(mc_path)\n",
    "eval = l1l2_model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-manhattan",
   "metadata": {},
   "source": [
    "En este caso, se nota una leve mejora en la métrica empleando regularización L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-reference",
   "metadata": {},
   "source": [
    "# 11. Dropout\n",
    "Se emplea una capa extra de dropout para minimizar el overfitting. Este regularizador funciona ignorando a neuronas de forma aleatoria. Se realiza dropout **solo en la etapa de entrenamiento**. **En teoría, no se lleva muy bien con la normalización por capas**. No tiene sentido probar esto en una red de una sola capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "tested-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "returning-giving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4790 - auc: 0.8365 - accuracy: 0.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47897306084632874, 0.8364559412002563, 0.7621621489524841]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model checkpoint callback\n",
    "mc_path = 'model_checkpoints/dropout_test.h5'\n",
    "mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "# Configuring TensorBoard to log learning process\n",
    "log_dir = \"logs/fit/REG/Dropout/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Creating model\n",
    "do_model = Sequential()\n",
    "\n",
    "# Adding dropout layer to network\n",
    "do_model.add(Dropout(0))\n",
    "\n",
    "# Adding Dense layer\n",
    "do_model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "\n",
    "# Compiling model\n",
    "do_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "\n",
    "# Training model\n",
    "do_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "do_model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "coral-champagne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4776 - auc: 0.8371 - accuracy: 0.7676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4776022434234619, 0.8371064066886902, 0.7675675749778748]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True, activity_regularizer=l2(1e-4)))\n",
    "model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "model.evaluate(x=x_valid, y=y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-exhaust",
   "metadata": {},
   "source": [
    "# 12. Feature Engineering. Features Polinomiales\n",
    "El objertivo de esta sección es el de agregar variables de entrada al modelo, que surgen de combinar las variables originales. El grado del polinomio determina la cantidad de nuevas variables que se suman al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-cliff",
   "metadata": {},
   "source": [
    "A continuación se observa la progresión de la métrica en **train** y **valid** en función del grado del polinomio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "increasing-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import History, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-might",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: Realizar la normalización de los datos **después** de aplicar el feature polinomial, sino se rompe todo :(."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "instructional-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LRS callback\n",
    "# Define learning rate at start\n",
    "ilr = 0.2 # ilr=0.5, ds = 100000, dr=0.8, stc=False\n",
    "lr_schedule = ExponentialDecay(ilr, decay_steps=1000, decay_rate=0.8, staircase=True) # Decay every (decay_steps) steps with a base of (decay_rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-enough",
   "metadata": {},
   "source": [
    "En este punto cabe aclarar que se probó el parámetro *interaction_only* del preprocesador de polinomios y se llegó a la conclusión de que el desempeño mejora con este valor en *True*. Esto es así dado que, al activarlo, se logra un número mucho menor de variables en cada orden. Esto contribuye ampliamente a **reducir el overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "increased-foundation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Polynomial order = 1 ---\n",
      "Input count = 8\n",
      "AUC for TRAIN subset is 0.8501\n",
      "AUC for VALID subset is 0.8369\n",
      "--- Polynomial order = 2 ---\n",
      "Input count = 36\n",
      "AUC for TRAIN subset is 0.8738\n",
      "AUC for VALID subset is 0.8372\n",
      "--- Polynomial order = 3 ---\n",
      "Input count = 92\n",
      "AUC for TRAIN subset is 0.9090\n",
      "AUC for VALID subset is 0.8145\n",
      "--- Polynomial order = 4 ---\n",
      "Input count = 162\n",
      "AUC for TRAIN subset is 0.9041\n",
      "AUC for VALID subset is 0.8199\n",
      "--- Polynomial order = 5 ---\n",
      "Input count = 218\n",
      "AUC for TRAIN subset is 0.9285\n",
      "AUC for VALID subset is 0.8122\n",
      "--- Polynomial order = 6 ---\n",
      "Input count = 246\n",
      "AUC for TRAIN subset is 0.9169\n",
      "AUC for VALID subset is 0.8105\n",
      "--- Polynomial order = 7 ---\n",
      "Input count = 254\n",
      "AUC for TRAIN subset is 0.9170\n",
      "AUC for VALID subset is 0.8359\n",
      "--- Polynomial order = 8 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9289\n",
      "AUC for VALID subset is 0.8053\n",
      "--- Polynomial order = 9 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9194\n",
      "AUC for VALID subset is 0.8364\n",
      "--- Polynomial order = 10 ---\n",
      "Input count = 255\n",
      "AUC for TRAIN subset is 0.9225\n",
      "AUC for VALID subset is 0.8325\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "# Define polynomial degrees to train and compute metrics\n",
    "poly_degrees = np.arange(1, 11, 1)\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=True)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    # Configuring TensorBoard to log learning process\n",
    "    log_dir = \"logs/fit/POLY/ioenabled\" + str(deg)\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Define the Model Checkpoint callback.\n",
    "    mc_path = 'model_checkpoints/get_best_poly_' + str(deg) + '_ioenabled_checkpoint.h5'\n",
    "    mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "    \n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(1e-4)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback, tensorboard_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "public-belly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-history",
   "metadata": {},
   "source": [
    "Ahora volvemos a probar lo mismo, esta vez desactivando el parámetro *interactions_only*. Aumento el grado de regularización dado que la cantidad de variables escala rápidamente con el orden del polinimio, produciendo overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "elegant-substitute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "valid_auc_scores = []\n",
    "train_auc_scores = []\n",
    "\n",
    "for deg in poly_degrees:\n",
    "    \n",
    "    # Create and initialize polynomial preprocessor\n",
    "    poly = preprocessing.PolynomialFeatures(degree=deg, include_bias=False, interaction_only=False)\n",
    "    poly.fit(x_train_un)\n",
    "    \n",
    "    # Get poly subsets, but unnormalized\n",
    "    x_train_poly = poly.transform(x_train_un)\n",
    "    x_valid_poly = poly.transform(x_valid_un)\n",
    "    x_test_poly = poly.transform(x_test_un)\n",
    "    \n",
    "    # Apply z-score to normalize poly subsets\n",
    "\n",
    "    # Create an instance of the StandardScaler for each variable\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "\n",
    "    # Fit the distribution\n",
    "    scaler.fit(x_train_poly)\n",
    "\n",
    "    # Transform and normalize all variables\n",
    "    x_train_poly = scaler.transform(x_train_poly)\n",
    "    x_test_poly = scaler.transform(x_test_poly)\n",
    "    x_valid_poly = scaler.transform(x_valid_poly)\n",
    "    \n",
    "    # Configuring TensorBoard to log learning process\n",
    "    log_dir = \"logs/fit/POLY/iodisabled\" + str(deg)\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Define the Model Checkpoint callback.\n",
    "    mc_path = 'model_checkpoints/get_best_poly_' + str(deg) + '_iodisabled_checkpoint.h5'\n",
    "    mc_callback = ModelCheckpoint(filepath=mc_path, monitor='val_auc', save_best_only=True, mode='max')\n",
    "\n",
    "    # Creating model\n",
    "    p_model  = Sequential()\n",
    "    p_model.add(Dense(1, input_shape=(poly.n_output_features_,), activation='sigmoid', use_bias=True, kernel_regularizer=l2(1e-2)))\n",
    "    # Compiling model\n",
    "    p_model.compile(optimizer=SGD(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=metrics)\n",
    "    # Fitting model\n",
    "    p_model.fit(x_train_poly, y_train, validation_data=(x_valid_poly, y_valid), shuffle=True, epochs=200, batch_size=32, verbose=0, callbacks=[es_callback, mc_callback, tensorboard_callback])\n",
    "    \n",
    "    # Load best model\n",
    "    p_model = load_model(mc_path)\n",
    "    \n",
    "    # Inform number of variables in model\n",
    "    input_n = x_train_poly.shape[1]\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(f'--- Polynomial order = {deg} ---')\n",
    "    print(f'Input count = {input_n}') \n",
    "    eval_valid = p_model.evaluate(x=x_valid_poly, y=y_valid, return_dict=True, verbose=0)\n",
    "    eval_train = p_model.evaluate(x=x_train_poly, y=y_train, return_dict=True, verbose=0)\n",
    "    \n",
    "    # Append scores to result\n",
    "    auc_t = eval_train['auc']\n",
    "    auc_v = eval_valid['auc']\n",
    "    \n",
    "    valid_auc_scores.append(auc_v)\n",
    "    train_auc_scores.append(auc_t)\n",
    "    print(f'AUC for TRAIN subset is {auc_t:.4f}')\n",
    "    print(f'AUC for VALID subset is {auc_v:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "lightweight-clerk",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-3e47d637fa12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_degrees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_auc_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_degrees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_auc_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_degrees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_auc_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoly_degrees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_auc_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2888\u001b[0m         \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deprecated_parameter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2889\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[1;32m-> 2890\u001b[1;33m     __ret = gca().scatter(\n\u001b[0m\u001b[0;32m   2891\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 **kwargs)\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\joaco\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4439\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4441\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must be the same size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot data\n",
    "plt.scatter(poly_degrees, valid_auc_scores, label='valid')\n",
    "plt.plot(poly_degrees, valid_auc_scores)\n",
    "plt.scatter(poly_degrees, train_auc_scores, label='train')\n",
    "plt.plot(poly_degrees, train_auc_scores)\n",
    "\n",
    "# Plot best poly degree, based on AUC calculation over VALID subset\n",
    "best_deg = poly_degrees[np.argmax(valid_auc_scores)]\n",
    "plt.axvline(best_deg, color='black', linestyle='--')\n",
    "\n",
    "# Make the plot nice\n",
    "plt.xlabel('Poly degree')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(poly_degrees)\n",
    "plt.grid(b=True)\n",
    "plt.legend()\n",
    "plt.title('Polynomial feature analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-boards",
   "metadata": {},
   "source": [
    "# 13. Agregando K-Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KFold from sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kfold_crossval(model, x, y, metrics, n_splits=2, shuffle=False, random_state=None, epochs=1, verbose='auto', callbacks=None, batch_size=32):\n",
    "    x = x.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    # Initialize kfold splitter\n",
    "    kf = KFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    fold_num = 1\n",
    "    \n",
    "    # Create arrays for metrics\n",
    "    auc_valid = []\n",
    "    auc_train = []\n",
    "    \n",
    "    for train_index, valid_index in kf.split(x):\n",
    "        print(f'----- Fold N = {fold_num} -----')\n",
    "        # Get train and valid splits\n",
    "        x_train, y_train = x[train_index], y[train_index]\n",
    "        x_valid, y_valid = x[valid_index], y[valid_index]\n",
    "        \n",
    "        # Fit model\n",
    "        history = model.fit(x=x_train, \n",
    "                  y=y_train, \n",
    "                  validation_data=(x_valid, y_valid), \n",
    "                  shuffle=shuffle, \n",
    "                  epochs=epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  verbose=verbose, \n",
    "                  callbacks=callbacks\n",
    "                 )\n",
    "        \n",
    "        # Get metrics\n",
    "        eval_train = model.evaluate(x=x_train, y=y_train, return_dict=True, verbose=0)\n",
    "        eval_valid = model.evaluate(x=x_valid, y=y_valid, return_dict=True, verbose=0)\n",
    "        \n",
    "        tscore = eval_train['auc']\n",
    "        vscore = eval_valid['auc']\n",
    "        \n",
    "        tsize = x_train.shape[0]\n",
    "        vsize = x_valid.shape[0]\n",
    "        \n",
    "        # Append metrics\n",
    "        auc_train.append(tscore)\n",
    "        auc_valid.append(vscore)\n",
    "        \n",
    "        print(f'Size for TRAIN is {tsize}')\n",
    "        print(f'Size for VALID is {vsize}')\n",
    "        \n",
    "        print(f'AUC for TRAIN is {tscore:.4f}')\n",
    "        print(f'AUC for VALID is {vscore:.4f}')\n",
    "        \n",
    "        # Increase fold number\n",
    "        fold_num = fold_num + 1\n",
    "        \n",
    "    return auc_train, auc_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(8,), activation='sigmoid', use_bias=True))\n",
    "# Compiling model\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=metrics)\n",
    "# Training model\n",
    "auct, aucv = kfold_crossval(model=model, x=x_train, y=y_train, metrics=metrics,\n",
    "                            n_splits=5, shuffle=False, random_state=None, epochs=1000, verbose=0, \n",
    "                            callbacks=[tensorboard_callback, es_callback, mc_callback])\n",
    "#es_model.fit(x=x_train, y=y_train, validation_data=(x_valid, y_valid), shuffle=True, epochs=1000, batch_size=32, verbose=1, callbacks=[tensorboard_callback, es_callback, mc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucv_mean = np.mean(aucv)\n",
    "auct_mean = np.mean(auct)\n",
    "print(f'Average AUC for TRAIN is {auct_mean:.4f}')\n",
    "print(f'Average AUC for VALID is {aucv_mean:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
